\documentclass[10pt,a4paper]{article}


\usepackage[a4paper, total={8in, 10in}]{geometry}


\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\begin{document}

\section{Slide \textit{'hardware-insights'}}

\subsection{OOO Pipeline execution and imprecise exceptions}

When processor executes instructions in an order governed by the availability of input data and execution units, rather than by their original order in a program, we are adopting an \textbf{out-of-order execution paradigm}; \textbf{in other words different instructions can surpass each other depending on data or micro-controller availability.} We distinguish two events when we use this kind of paradigm: the \textbf{emission}, that is the action of injecting instructions into the pipeline; the \textbf{retire}, that is the action of committing instructions and making their side effects visible in terms of ISA exposed architectural resources

\textbf{Is important to recall that out-of-order completion must preserve exception behaviour in the sense that exactly those exceptions that would arise if the program were executed in strict program order actually do arise}. However, when we use OOO execution paradigm a processor may generate the so called \textbf{imprecise exceptions}. \textbf{An exception is imprecise if the processor state  when  an  exception  is  raised  does  not  look  exactly  as  if  the instructions were executed sequentially in strict program order}. In other words imprecise exceptions can occur because when:

\begin{itemize}
\item The pipeline may have already completed instructions that are later in program order than the instruction causing the exception.
\item The pipeline may have not yet completed some instructions that are earlier in program order than the instruction causing the exception.
\end{itemize}

\textbf{Recall that any instruction may change the micro-architectural state, although finally not committing its actions onto ISA exposed resources.} Since the the pipeline may have not yet completed the execution of instructions preceding the offending one, hardware status can been already changed an this fact can be exploited by several attacks (like \textbf{Meltdown}).

\subsection{Tomasulo algorithm}

Tomasulo’s algorithm is a computer architecture hardware algorithm for dynamic scheduling of instructions that allows out-of-order execution and enables more efficient use of multiple execution units. Suppose two operation A and B such that A precedes B in program order, that algorithm permit to resolve three hazard:

\begin{description}
\item[RAW (Read After Write)] B reads a datum before A writes it.
\item[WAW (Write After Write)] B writes a datum before A writes the same datum.

\item[WAR (Write After Read)] B writes a datum before A reads the same datum.
\end{description}

\textbf{RAW hazards are avoided by executing an instruction only when its operands are available, while WAR and WAW hazards are eliminated by \textit{register renaming}}

\subsection{UMA}

When we have a \textbf{single main memory} that has a \textit{symmetric relationship to all processors and a uniform access time from any processor}, these multiprocessors are most often called \textit{symmetric shared-memory multiprocessors} (\textbf{SMPs}), and this style of architecture is sometimes called \textit{uniform memory access} (\textbf{UMA}): in fact, \textbf{all processors have a uniform latency from memory}. \textbf{The term shared memory refers to the fact that the address space is shared; that is, the same physical address on two processors refers to the same location in memory.} In this architecture all CPUs can have one or more level of cache. However this architecture is obliviously \textbf{not} scalable when the number of CPUs grows.

\subsection{NUMA}

When we have a distributed memory, a multiprocessor architecture is usually called \textit{distributed shared-memory} (\textbf{DSM}). When we use this kind of system, we have two benefits:
\begin{itemize}
\item A cost-effective way to scale the memory bandwidth if most of the accesses are to the local memory in the node.
\item Reduces the latency for accesses to the local memory by a CPU.
\end{itemize}

The key disadvantages for that architecture is that communicating data between processors becomes more complex, \textbf{and that it requires more effort in the software to take advantage of the increased memory bandwidth afforded by distributed memories.}

The DSM multiprocessors are also called \textbf{NUMAs} (\textit{non-uniform memory access}), \textbf{since the access time depends on the location of a data word in memory.} In fact, when a CPU wants to access to an item stored into his node, performing a \textit{local access} involving inner private/shared caches and controllers, access latency is very low. However when a CPU wants to access to an item stored on another node, performing a \textit{remote accesses} involving remote controllers and caches, latency can be very high respect to previous case.

\subsection{The problem of cache coherence}

Unfortunately, \textbf{the view of memory held by different processors is through their individual caches}, which, without any additional precautions, could end up seeing different values of a same shared data (\textbf{cache coherence} problem).

By definition, \textbf{coherence defines what values can be returned by a read} (a \textbf{cache coherence protocols} defines how to maintain coherence) while \textbf{consistency determines when a written value will be returned by a read} (a \textbf{memory consistency protocol} defines when written value must be seen by a reader).

A memory system is coherent if:
\begin{enumerate}
\item A read from location $X$, previously written by a processor, returns the last written value if no other processor carried out writes on $X$ in the meanwhile. \textbf{This property preserve program order that is the causal consistency along program order.}

\item A read by a processor to location $X$ that follows a write by another processor to $X$ returns the written value if the read and write are sufficiently separated in time and no other writes to $X$ occur between the two accesses. \textbf{This property assure that a processor couldn't continuously
read an old data value (Avoidance of staleness).}

\item \textbf{Writes to the same location are serialized}; that is, two writes to the same location by any two processors are seen in the same order by all processors.
\end{enumerate}

The choice and the design of a coherence protocol depends on many factors including: overhead, latency, cache policies, interconnection topology and so on.
\textbf{However the Key to implementing a cache coherence protocol is tracking the state of any copy of a data block}. There are two classes of protocol which define when update aforementioned copies:
\begin{description}
\item[Update protocol] When we use this type of protocol, also called \textit{write update} or \textit{write broadcast}, when a core writes to a block, it updates all other copies (\textbf{it consumes considerably more bandwidth}).

\item[Invalidate protocol] When we use this type of protocol, a processor has \textbf{exclusive access} to a data item before it writes that item; moreover that CPU invalidates other copies on a write that is no other readable or writeable copies of an item exist when the write occurs. \textbf{It is the most common protocol, but suffer of some latency.}

\end{description}

\subsection{Snooping protocol}

The key to implementing an invalidate protocol is the use of the bus, or another broadcast medium, called \textit{network} to perform invalidates and to issue "transactions" on the state of cache blocks.

To perform any operation, the processor simply \textbf{acquires} bus access and broadcasts the address to be invalidated on the bus. All processors continuously \textbf{snoop} on the bus, watching the addresses. The processors check whether the address on the bus is in their cache. If so, the corresponding data in the cache are invalidated. \textbf{A state transition cannot occur unless the broadcast medium is acquired by the source controller and are carried out atomically with a distribute fashions thanks to \textit{serialization} over the broadcast medium}.

When we perform a read, we also need to locate a data item when a cache miss occurs. In a \textbf{write-through cache}, it is easy to find the recent value of a data item, \textit{since all written data are always sent to the memory, from which the most recent value of a data item can always be fetched} (using write through simplifies the implementation of cache coherence). For a \textbf{write-back cache}, the problem of finding the most recent data value is
harder, since the most recent value of a data item can be in a cache rather than in memory (the CPU must get data from another cache)

\newpage
\section{Slide \textit{'kernel-programming-basics'}}

\subsection{Segmentation}

Intel microprocessors perform address translation in \textbf{three} different ways:
\begin{description}
\item[Real Mode] This mode exists mostly to \textbf{maintain processor compatibility with older models and to allow the operating system to bootstrap}. In this modality, a logical address is composed of a \texttt{seg} segment (hold by a 16 bit \textit{segment register}) and an \texttt{off} offset (hold by a 16 bit \textit{general register}) while the corresponding physical address is simply computed using \texttt{seg*16 + off}: \textit{as a result, no Global Descriptor Table, Local Descriptor Table, or paging table is needed by the CPU addressing circuit to translate a logical address into a physical one}. \textbf{Observer that in real mode no segment specific protection information are provided, therefore this modality is unsuitable for modern operating system}. Around 1MB ($2^20$ B) of memory is allowed
\item[Protected Mode] Similarly to real mode, a logical address consists of two parts: a \textbf{segment identifier} and an \textbf{offset} that specifies the relative address within the segment. However the target segment \textit{\textbf{identifier}} is a \textbf{13-bit field} present into a \textbf{16 bit field} called the \textbf{segment selector} keep by 16 bit segment register (\textbf{remaining 3 bit are used for protection purposes}) while a 32 bit general register holds the offset. The corresponding physical address is computed using the linear address of the first byte of the segment (which is hold by a special table) using following equation: \texttt{TABLE[segment].base + offset}. In this modality Up to 4 GB of memory is allowed.
\item[Long Mode] Identical to protected mode but the offset is hold by 64-bit general registers (\textit{although up to 48-bit are used in canonical form}). In this way is possible to address up to $2^{48}$ B (256 TB) of linear memory.
\end{description}

\textbf{From now we describe protected mode}. To make it easy to retrieve segment selectors quickly, the processor provides \textbf{segmentation registers} whose only purpose is to hold segment selectors; these registers are called \texttt{cs} (\textbf{code segment register, which points to a segment containing program instructions}), \texttt{ss} (\textbf{stack segment register, which points to a segment containing the current program stack}), \texttt{ds} (\textbf{data segment register, which points to a segment containing global and static data}), \texttt{es}, \texttt{fs}, and \texttt{gs} (\textbf{these three segmentation registers are general purpose and may refer to arbitrary data segments}.)

To be more precise, segment selectors have three fields:
\begin{description}
\item[index (13-bit)] identifies the Segment Descriptor entry contained in the GDT or in the LDT (see below).
\item[Table Indicator (1-bit)] specifies whether the Segment Descriptor is included in the GDT (TI = 0) or in the LDT (TI = 1).
\item[Requestor Privilege Level (RPL) (2-bit)] specifies the \textbf{Current Privilege Leve}l of the CPU when the corresponding Segment Selector is loaded into the \texttt{cs} register. The value 0 denotes the highest privilege level, while the value 3 denotes the lowest one. Linux uses only levels 0 and 3, which are respectively called \textbf{Kernel Mode} and \textbf{User Mode.}

\end{description}

 the 13-bit field which identifies the Segment Descriptor entry contained in the GDT or in the LDT (see below), 1-bit field which is te Table Indicator that specifies whether the Segment Descriptor is included in the GDT (TI = 0) or in the LDT (TI = 1). Requestor Privilege Level: specifies the Current Privilege Level of the CPU when the corresponding Seg-
ment Selector is loaded into the cs register; it also may be used to selectively weaken the processor priv-
ilege level when accessing data segments (see Intel documentation for details).

\subsubsection{GDT and LDT}

Each segment is represented by an \textbf{8-byte Segment Descriptor} that describes the segment characteristics. Segment Descriptors are stored either in the \textbf{Global Descriptor Table} (\textbf{GDT}) or in the \textbf{Local Descriptor Table} (\textbf{LDT}) \textbf{which are kept in main memory}.

\textbf{The address (32 bit in protected mode, 64 bit in long mode) and size (always 16 bit) of the GDT in main memory are contained in the \texttt{gdtr} control register, while the address and size of the currently used LDT are contained in the \texttt{ldtr} control register.}

\textbf{GDT is used for the mapping of linear addresses \textit{at least} for kernel mode} (that is to manage kernel level segments) while \textbf{LDT is used for user mode} (each process is permitted to have its own LDT). \textbf{However GDT is the unique used segment table in most operating systems}.

\subsubsection{Segment descriptor}

A Segment Descriptor, that is an entry of the GDT/LDT, has \textbf{many} fields including:
\begin{description}
\item[Base] Contains the linear address of the first byte of the segment.
\item[G (Granularity flag)] if equal to 0, the segment size is expressed in bytes; otherwise, it is expressed in multiples of 4096 bytes.
\item[Limit] Holds the offset of the last memory cell in the segment, \textit{thus binding the segment length}. When G is set to 0, the size of a segment may vary between 1 byte and 1 MB; otherwise, it may vary between 4 KB and 4 GB.
\item[DPL (Descriptor Privilege Level)(2 bit)]: \textit{used to restrict accesses to the segment}. It represents the minimal CPU privilege level requested for accessing the segment. Therefore, a segment with its DPL set to 0 is accessible only when the CPL is 0 (Kernel Mode) while a segment with its DPL set to 3 is accessible with every CPL value.
\item[P (Segment-Present)] is equal to 0 if the segment is not stored currently in main memory. Linux \textbf{always} sets this flag to 1, because it never swaps out whole segments to disk.
\end{description}

Now be careful. Because a Segment Descriptor is 8 bytes long, \textbf{its relative address inside the GDT/LDT is obtained by multiplying the 13-bit index field of the Segment Selector by 8}. For instance, if the GDT is at \texttt{0x00020000} (the value stored in the gdtr register) and the index specified by the Segment Selector is 2, the address of the corresponding Segment Descriptor is \texttt{0x00020000 + (2 x 8)}, or 0x00020010 (\textbf{all this in protected mode!!})

The first entry of the GDT is always set to 0. This ensures that logical addresses with a \texttt{null} Segment Selector will be considered invalid, thus causing a processor exception. 

\textbf{There a sort of cache to speed up the access to segment descriptor}. In fact for each of the six segmentation registers \textbf{there are additional non-programmable register which are used to contains the 8-byte Segment Descriptor}.  Every time a Segment Selector is loaded in a segmentation register, the corresponding Segment Descriptor is loaded from memory into the matching nonprogrammable CPU register. From then on, translations of logical addresses referring to that segment can be performed without accessing the GDT.

\textbf{After to have computed the address of a Segment Descriptor, to obtain the linear address, we adds the offset of the logical address to the Base field of the Segment Descriptor.}

\subsubsection{Segmentation in Linux}

Linux uses segmentation in a very limited way. \textbf{All Linux processes running in User Mode use the same pair of segments to address instructions and data} which are called \textbf{user code segment} and \textbf{user data segment}, respectively. Similarly, \textbf{all Linux processes running in Kernel Mode use the same pair of segments to address instructions and data}: they are called \textbf{kernel code segment} and \textbf{kernel data segment}. Now there are some differences about values stored in the fields of their segmentation descriptors: for instance user data/code segments have a DPL equal to 3, while kernel data/code segments have a DPL equal to 0. All these segment descriptors have their base set to \texttt{0x00000000}. 

The corresponding Segment Selectors are defined by the macros \texttt{\_\_USER\_CS}, \texttt{\_\_USER\_DS}, \texttt{\_\_KERNEL\_CS}, and \texttt{\_\_KERNEL\_DS}, respectively. To address the kernel code segment, for instance, the kernel just loads the value yielded by the \texttt{\_\_KERNEL\_CS} macro into the \texttt{cs} segmentation register. It is sufficient to load \texttt{\_\_KERNEL\_CS} into cs whenever the CPU switches to Kernel Mode (CPL equal to 0).






\newpage
\section{Slide \textit{'kernel-level-memory-management'}}

\subsection{Page Descriptor}

In Linux, \textbf{state information of a page frame is kept in a page descriptor} of type \texttt{struct page} (or struct \texttt{mem\_map\_t}), and all page descriptors, which are 32 byte long, are stored in an array called \texttt{mem\_map} (the space required by it is slightly less than 1\% of the whole RAM). These data structures are defined into \texttt{include/linux/mm.h}. 

The \texttt{virt\_to\_page(addr)} macro yields the address of the page descriptor associated with the linear address \texttt{addr}. 

\texttt{struct page} has many fields but the most important are:
\begin{description}

\item[\texttt{atomic\_t \_count}] It represent a usage reference counter for the page. If it is set to -1, the corresponding page frame is free and can be assigned to any process or to the kernel itself. If it is set to a value greater than or equal to 0, the page frame is assigned to one or more processes or is used to store some kernel data structures. The \texttt{page\_count()} function returns the value of the \texttt{\_count} field increased by one, that is, the number of users of the page. This field is managed via atomic updates, such as with \texttt{LOCK} directives.
\item[\texttt{struct list\_head lru}] Contains pointers to the least recently used doubly linked list of pages. 
\item[\texttt{unsigned long flags}] Array of flags used to describe the status of current page frame (but also encodes the zone number to which the page frame belongs). There are up to 32 flags and Linux kernel defines many macros to manipulate them. Some flags are:
\begin{description}
\item[\texttt{PG\_locked}] The page is locked; for instance, it is involved in a disk I/O operation.
\item[\texttt{PG\_dirty}] The page has been modified.
\item[\texttt{PG\_reserved}] The page frame is reserved for kernel code or is unusable.
\end{description}

\end{description}

\subsection{Free list}

Linux uses \textbf{free list} to manage memory allocation. \textbf{It operates by connecting unallocated regions of memory together in a linked list, using the first word of each unallocated region as a pointer to the next.} 

Free lists make the allocation and deallocation operations very simple. \textbf{To free a region, one would just link it to the free list. To allocate a region, one would simply remove a single region from the end of the free list and use it}. 

\subsection{NUMA}

Is extremely important to remember that Linux 2.6 supports the \textit{Non-Uniform Memory Access} (\textbf{NUMA}) model, \textbf{in which the access times for different memory locations from a given CPU may vary} and, according to that architecture, physical memory is partitioned in several \textbf{nodes}. The time needed by a given CPU to access pages within a single node is the same. However, this time might not be the same for two different CPUs. 

\subsection{NUMA Node Descriptor}

\textbf{Be careful that Linux splits physical memory inside each node into several zones. We have 3 free lists of frames, depending on the frame positioning within available zones (defined in \texttt{include/linux/mmzone.h}) which are:}

\begin{description}
\item[\texttt{ZONE\_DMA}] Contains page frames of memory below 16 MB, that is page frames that can be used by old ISA-based devices (\textit{Direct Memory Access} (DMA) processors).
\item[\texttt{ZONE\_NORNMAL}] Contains page frames of memory at and above 16 MB and below 896 MB (direct mapped by the kernel).
\item[\texttt{ZONE\_HIGHMEM}] Contains page frames of memory at and above 896 MB (only page cache and user).
\end{description}

To represent a NUMA node, Linux uses a descriptor of type \texttt{struct pg\_data\_t}. All node descriptors are stored in a singly linked list, whose first
element is pointed to by the \texttt{pgdat\_list} variable. Be careful to the fact that this data structure is used by Linux kernel even if the architecture is based on \textit{Uniform Memory Access} (\textbf{UMA}): in fact Linux makes use of a single node that includes all system physical memory. Thus, the \texttt{pgdat\_list} variable points to a list consisting of a single element (node 0) stored in the \texttt{contig\_page\_data} variable.

Remember that free lists information is kept within the \texttt{struct pg\_data\_t} data structure. In fact the most important fields of \texttt{struct pg\_data\_t} are:

\begin{description}
\item[\texttt{struct page *node\_mem\_map}] Array of page descriptors of the node
\item[\texttt{struct zone [] node\_zones}] Array of zone descriptors of the node
\end{description}

\subsection{Zone Descriptor}

Obliviously each memory zone has its own descriptor of type \texttt{struct zone} and many fields of this data structure are used for page frame reclaiming. However, most important fields are:

\begin{description}
\item[\texttt{struct page * zone\_mem\_map}] Pointer to first page descriptor of the zone.
\item[\texttt{spinlock\_t lock}] Spin lock protecting the descriptor.
\item[\texttt{struct free\_area [] free\_area}] Identifies the blocks of free page frames in the zone
\end{description}

In summary, Linux has links to the memory node and to the zone inside the node that includes the corresponding page frame of type \texttt{struct page}.

\subsection{Buddy allocator}

The technique adopted by Linux to solve the external fragmentation problem is based on the well-known \textbf{buddy system} algorithm. All free page frames are grouped into 11 lists of blocks that contain groups of 1, 2, 4, 8, 16, 32, 64, 128, 256, 512, and 1024 contiguous page frames, respectively. The largest request of 1024 page frames corresponds to a chunk of 4 MB of contiguous RAM. We use the term \texttt{order} \textbf{to indicate the logarithmic size of a block}.

Assume there is a request for a group of 256 contiguous page frames (i.e., one megabyte). The algorithm checks first to see whether a free block in the 256-page-frame list exists. If there is no such block, the algorithm looks for the next larger block — a free block in the 512-page-frame list. If such a block exists, the kernel allocates 256 of the 512 page frames to satisfy the request and inserts the remaining 256 page frames into the list of free 256-page-frame blocks. If there is no free 512-page block, the kernel then looks for the next larger block (i.e., a free 1024-page-frame block). If
such a block exists, it allocates 256 of the 1024 page frames to satisfy the request, inserts the first 512 of the remaining 768 page frames into the list of free 512-page-frame blocks, and inserts the last 256 page frames into the list of free 256-page-frame blocks. If the list of 1024-page-frame blocks is empty, the algorithm gives up and signals an error condition.

\textbf{Linux 2.6 uses a different buddy system for each zone}. Thus, in the x86 architecture, there are \textbf{3 buddy systems}: the first handles the page frames suitable for ISA DMA, the second handles the "normal" page frames, and the third handles the high memory page frames. Each buddy system relies on the following main data structures:
\begin{itemize}
\item \textbf{The \texttt{mem\_map} array where all page descriptors are stored}. Actually, each zone is concerned with a subset of the \texttt{mem\_map} elements. The first element in the subset and its number of elements are specified, respectively, by the \texttt{zone\_mem\_map} and \texttt{size} fields of the zone descriptor.

\item The array consisting of eleven elements of type \texttt{struct free\_area}, one element for each group size. As we said the array is stored in the \texttt{free\_area} field of the zone descriptor.
\end{itemize}

Let us consider the $k^{th}$ element of the \texttt{struct free\_area} array in the zone descriptor, which identifies all the free blocks of size $2^k$. In this data structure there is a pointer of type struct \texttt{list\_head}  which is is the head of a doubly linked circular list that collects the page descriptors associated with the free blocks of $2^k$ pages. Besides the head of the list, the $k^{th}$ element of the \texttt{struct free\_area} array includes also the field \texttt{nr\_free}, which specifies the number of free blocks of size $2^k$ pages, and a pointer to a bitmap that keeps fragmentation information.  

Recall that spin locks are used to manage \texttt{mem\_map} AND \texttt{struct free\_area} array.

\textbf{To achieve better performance a little number of page frames are kept in cache to quickly satisfy the allocation requests for single page frames}.

\subsection{API}

Page frames can be requested by using some different functions and macros (APIs) (they return \texttt{NULL} in case of failure, a linear address of the first allocated page in case of success) which prototype are stored into \texttt{\#include <linux/malloc.h>}. The most important are:

\begin{description}
\item[\texttt{get\_zeroed\_page(gfp\_mask)}] Function used to obtain a page frame filled with zeros.
\item[\texttt{\_\_get\_free\_page(gfp\_mask)}] Macro used to get a single page frame.
\item[\texttt{\_\_get\_free\_pages(gfp\_mask, order)}] Macro used to request $2^{order}$ contiguous page frames returning the linear address of the
first allocated page.
\item[\texttt{free\_page(addr)}] This macro releases the page frame having the linear address \texttt{addr}.

The parameter \texttt{gfp\_mask} is a group of flags that specify \textbf{how to look for free page frames} and they are extremely important when we require page frame allocation in different contexts including: 
\begin{description}
\item[Interrupt context] allocation is requested by an \textbf{interrupt handler} which uses above function with \texttt{GFP\_ATOMIC} flag (equivalent to \texttt{\_\_GFP\_HIGH}) \textbf{which means that the kernel is allowed to access the pool of reserved page frames: therefore the call cannot lead to sleep (that is no wait)}  An atomic request never blocks: if there are not enough free pages the allocation simply fails.

\item[Process context] allocation is caused by a system call using \texttt{GFP\_KERNEL} or \texttt{GFP\_USER} (both equivalent to \texttt{\_\_GFP\_WAIT | \_\_GFP\_IO | \_\_GFP\_FS}) according to which kernel is allowed to block the current process waiting for free page frames (\texttt{\_\_GFP\_WAIT}) and to perform I/O transfers on low memory pages in order to free page frames (\texttt{\_\_GFP\_IO}): therefore the call can lead to sleep.
\end{description}

\end{description}



\subsection{TLB operation}

Besides general-purpose hardware caches, x86 processors include a cache called \textit{Translation Lookaside Buffers} (\textbf{TLB}) to speed up linear address translation.

When a linear address is used for the first time, the corresponding physical address is computed through slow accesses to the Page Tables in RAM. The physical address is then stored in a TLB entry so that further references to the same linear address can be quickly translated.

In a multiprocessor system, \textbf{each CPU has its own TLB, called the \textbf{local TLB} of the CPU}. Contrary to the hardware cache, the corresponding entries of the TLB need \textbf{not} be synchronized, because processes running on the existing CPUs may associate the same linear address with different physical ones.

\textbf{When the \texttt{cr3} control register of a CPU is modified, the hardware automatically invalidates all entries of the local TLB, because a new set of page tables is in use (page table changes).} \textbf{However changes inside the current page table are not automatically reflected within the TLB.}

Fortunately, Linux offers several TLB flush methods that should be applied appropriately, depending on the type of page table change:
\begin{description}
\item[\texttt{flush\_tlb\_all}] This flushes the \textbf{entire TLB on all processors} running in the system, which makes it the most expensive TLB flush operation. It is used when we have made changes into the kernel page table entries. \textbf{After it completes, all modifications to the page tables will be visible globally to all processors.}

\item[\texttt{flush\_tlb\_mm(struct mm\_struct *mm)}] Flushes all TLB entries of the non-global pages owned by a given process that is all entries related to the userspace portion for the requested \texttt{mm} context. Is used when forking a new process.

\item[\texttt{flush\_tlb\_range}] Flushes the TLB entries corresponding to a linear address interval of a given process and is used when releasing a linear address interval of a process (when \texttt{mremap()} or \texttt{mprotect()} is used).

\item[\texttt{flush\_tlb\_page}] Flushes the TLB of a single Page Table entry of a given process and is used when handling a page fault.

\item[\texttt{flush\_tlb\_pgtables}] Flushes the TLB entries of a given contiguous subset of page tables of a given process and is called when a region is being unmapped and the page directory entries are being reclaimed 
\end{description}

Despite the rich set of TLB methods offered by the generic Linux kernel, every microprocessor usually offers a far more restricted set of TLB-invalidating assembly language instructions. \textbf{Intel microprocessors offers only two TLB-invalidating techniques: the automatic flush of all TLB entries when a value is loaded into the cr3 register and the \texttt{invlpg} assembly language instruction which invalidates a single TLB entry mapping a given linear address.}

The architecture-independent TLB-invalidating methods are extended quite simply to multiprocessor systems. \textbf{The function running on a CPU sends an Interprocessor Interrupt to the other CPUs that forces them to execute the proper TLB-invalidating function} (\textbf{expensive} operation (\textit{direct cost}) due to latency for cross-CPU coordination in case of global TLB flushes).

Remember that flush a TLB has the direct cost of the latency of the firmware level protocol for TLB entries invalidation (selective vs non-selective). Recall that flush TLB lead to \textbf{indirect cost} of refilling TLB entries and the latency experimented by MMU firmware upon misses in the translation process of virtual to physical addresses.

\subsubsection{When flush TLB?}

As a general rule,\textbf{ any process switch implies changing the set of active page tables and therefore local TLB entries relative to the old page tables must be flushed}; this is done automatically when the kernel writes the address of the new Page Global Directory into the \texttt{cr3} control register.

Besides \textbf{process switches}, there are other cases in which the kernel needs to flush some entries in a TLB. For instance, when the kernel assigns a page frame to a User Mode process and stores its physical address into a Page Table entry, it must flush any local TLB entry that refers to the corresponding linear address (virtual addresses accessible \textbf{locally} in time-sharing concurrency). On multiprocessor systems, the kernel also must flush the same TLB entry on the CPUs that are using the same set of page tables, if any (virtual addresses accessible \textbf{globally}  by every CPU/core in real-time-concurrency).

Kernel-page mapping has a \textit{global} nature, therefore when we use \texttt{vmalloc()} / \texttt{vfree()} on a specific CPU, all the other must observer mapping updates and TLB flush is necessary. 

\newpage
\section{Slide \textit{'kernel-level-task-management'}}

\subsection{Interrupt handling}

Under Linux, hardware interrupts are called \textbf{IRQ}'s (\textit{Interrupt Requests}) and their management  typically occurs via a \textbf{two-level logic}:
\begin{description}
\item[Top Half] A routine that actually responds to the interrupt and do a minimal amount of work to schedule its bottom half (this operation is very fast).
\item[Bottom Half] A routine scheduled by top half which execute whatever other work is required to handle the interrupt (such as awakening processes, starting up another I/O operation, and so on)
\end{description}

For instance, when a network interface reports the arrival of a new packet, the top half routine just retrieves the data and pushes it up to the protocol layer; actual processing of the packet is performed in a bottom half.

The most important aspect of this setup it that it permits the \textit{top half to service a new interrupt while the bottom half is still working}; \textbf{in fact all interrupts are enabled during execution of the bottom half}. Generally the execution of top half code is handled according to a \textit{non-interruptible scheme} (\textbf{but isn't mandatory}). 

This scheme permit to \textbf{avoid to keep locked resources when an interrupt occurs} (we may incur the risk of delaying critical actions as a spin-lock release) \textbf{avoiding possible deadlocks} when a slow interrupt management is hit by the activation of another one that needs the same resources. \textbf{Moreover this scheme keep
kernel response time small which is a very important property for many time-critical applications that expect their interrupt requests to be serviced in a few milliseconds.}

\subsection{Softirqs, Tasklets and work queues}

Form Linux 2.6, two different mechanisms are used to implement top/bottom-half processing:
\begin{itemize}
\item The so-called \textit{deferrable functions}, which we will call as \textbf{softirqs} and \textbf{tasklets}: they are very fast, but all tasklet code must be atomic.
\item The \textbf{Workqueues}, which may have a higher latency but that are allowed to sleep.
\end{itemize}

\subsubsection{Softirqs}

\textbf{Softirqs are statically allocated}, that is they are defined at compile time. The main data structure used to represent softirqs is the \texttt{softirq\_vec} array, which includes \texttt{NR\_SOFTIRQS} (32 entries) elements of type \texttt{softirq\_action}. \textbf{Observer that the priority of a softirq is the index of the corresponding \texttt{softirq\_action} element inside the array}. Some of the softirqs used in Linux are:

\begin{description}
\item[\texttt{HI\_SOFTIRQ}] With priority equal to 0 (first element of array) and it handles high priority tasklets.
\item[\texttt{TIMER\_SOFTIRQ}] With priority equal to 1 and it is used for timer related interrupts.
\end{description}

Another crucial data structure for implementing the softirqs is a \textbf{per-CPU 32-bit mask describing the pending softirqs}; it is stored in the \texttt{\_\_softirq\_pending} field of the \texttt{irq\_cpustat\_t} data structure (which is one of the data structure used per each CPU in the system). To get and set the value of the bit mask, the kernel makes use of the \texttt{local\_softirq\_pending()}. This is way softirqs can run concurrently on several CPUs, even if they are of the same type.

During interrupt acceptance, top half routine set properly the bit mask in the \texttt{\_\_softirq\_pending} field and then exit. 

Checks for active (pending) softirqs should be perfomed periodically, but without inducing too much overhead. They are performed in a few points of the kernel code. 

For this purpose, Linux, \textbf{for each CPU}, uses the so called \texttt{ksoftirqd/n} kernel thread (where n is the logical number of the CPU) to manage softirqs array executing bottom halves asynchronously. Once awaken, that thread, running the \texttt{ksoftirqd()} function, checks softirq bit mask for pending softirqs inspecting the per-CPU field \texttt{\_\_softirq\_pending}. If there are no softirqs pending, the function puts the current thread in the \texttt{TASK\_INTERRUPTIBLE} state and invokes then the \texttt{cond\_resched()} function to perform a process switch; otherwise, the thread runs the softIRQ handler, running \texttt{do\_softirq()}.

Be careful that the top half routine can set the bit mask telling that a \texttt{ksoftirqd/x} awaken on a CPU-core x will not process the handler associated with a given softIRQ; in this way we can \textbf{create affinity between SoftIRQs and CPU-cores in order to exploit NUMA machines}. Is also possible to set bit mask  in order to build affinity on group of CPU for load balancing; \textbf{in other word is possible a multithread execution of bottom half tasks}.

\subsubsection{tasklet}

When we use softirqs not necessarily we queue bottom half task, so this setup can be even more responsive. However the queuing concept is still there for on demand usage, if required. 

Tasklets are built on top of two softirqs named \texttt{HI\_SOFTIRQ} and \texttt{TASKLET\_SOFTIRQ}. Several tasklets may be associated with the same softirq, each tasklet carrying its own function. There is no real difference between the two softirqs, except that \texttt{do\_softirq()} executes \texttt{HI\_SOFTIRQ}’s tasklets before \texttt{TASKLET\_SOFTIRQ}’s tasklets.

Tasklets and high-priority tasklets are stored in the \texttt{tasklet\_vec} and \texttt{tasklet\_hi\_vec}
arrays respectively and both of them include \texttt{NR\_CPUS} elements which are \textbf{list} of \textbf{tasklet descriptors} which are a data structure of type \texttt{tasklet\_struct}

Each \texttt{tasklet\_struct} has many fields including the \texttt{state} field which represents the status of current tasklet and can assume two value: \texttt{TASKLET\_STATE\_SCHED} (tasklet pending), \texttt{TASKLET\_STATE\_RUN} (tasklet is running). Using this field is possible to keep track of a specific bottom half task, related to the execution of a specific function internal to the kernel.

Linux offers many APIs to manage tasklets: for instance to allocate a new \texttt{tasklet\_struct} data structure and
initialize is need to invoke \texttt{tasklet\_init()}; this function receives as its parameters the address of the tasklet descriptor , the address of your tasklet function (\texttt{void (*func)}), and its optional integer argument (\texttt{unsigned long}) for data.

The tasklet may be selectively disabled by invoking either \texttt{tasklet\_disable\_nosync()} or \texttt{tasklet\_disable()}. Both functions increase the count field of the tasklet descriptor, but the latter function does not return until an already running instance of the tasklet function has terminated. To reenable the tasklet, use \texttt{tasklet\_enable()}. To activate the tasklet, you should invoke either the \texttt{tasklet\_schedule()} function or the \texttt{tasklet\_hi\_schedule()} function, according to the priority that you require for the tasklet. When a tasklet is enabled its descriptor is added at the beginning of the list pointed to by \texttt{tasklet\_vec[n]} or \texttt{tasklet\_hi\_vec[n]}, where n denotes the logical number of the local CPU; then \texttt{HI\_SOFTIRQ} and \texttt{TASKLET\_SOFTIRQ} softirq are enabled (\textbf{all these operation are executed with local interrupts disabled})

Remember that tasklets can be instantiated by exploiting also the following macros defined 
in include \texttt{include/linux/interrupt.h}: 
\begin{itemize}
\item \texttt{DECLARE\_TASKLET(tasklet, function, data)}
\item \texttt{DECLARE\_TASKLET\_DISABLED(tasklet, function, data)}
\end{itemize}

Finally to execute tasklet associated with the \texttt{HI\_SOFTIRQ} softirq we run \texttt{tasklet\_hi\_action()}, while for those associated with \texttt{TASKLET\_SOFTIRQ} we use \texttt{tasklet\_action()}. 

\textbf{Observer that if the tasklet has already been scheduled on a different CPU-core, it will not be moved to another CPU-core if it's still pending (generic softirqs can instead be processed by different CPU-cores)}

\textbf{Tasklets run in interrupt context (see below)}

\subsubsection{Work queue}

The work queues have been introduced in Linux 2.6 and replace a similar construct called "task queue" used in Linux 2.4. Also the work queues are used to allow kernel functions to be activated and later executed by special kernel threads called \textit{worker threads}. 

However there is one important difference with softirq and tasklet: \textbf{deferrable functions (that is softirqs and tasklet) run in interrupt context while functions in work queues run in process context}. \textit{Running in process context is the only way to execute functions that can block (for instance, functions that need to access some block of data on disk) because no process switch can take place in interrupt context.}

\textbf{Observer that interrupts are enabled while the work queues are being run (except if the same work to be done disables them)}

The main data structure associated with a work queue is a descriptor called \texttt{workqueue\_struct}, which contains \textbf{many} fields including an array of \texttt{NR\_CPUS} elements (the maximum number of CPUs in the system.) Each element is a descriptor of type \texttt{cpu\_workqueue\_struct}, which contains a \texttt{worklist} field which is the head of a doubly linked list collecting the pending functions of the work queue. Every pending function is represented by a \texttt{work\_struct} data structure.

The \texttt{create\_workqueue("foo")} function receives as its parameter a string of characters and returns the address of a \texttt{workqueue\_struct} descriptor. The function also creates n worker threads (where n is the number of CPUs effectively present in the system), named after the string passed to the function: \texttt{foo/0, foo/1}, and so on. The \texttt{create\_singlethread\_workqueue()} function is similar, but it creates just one worker thread, no matter what the number of CPUs in the system is. To destroy a work queue the kernel invokes the \texttt{destroy\_workqueue()} function, which receives as its parameter a pointer to a \texttt{workqueue\_struct} array.

Another very important API is \texttt{queue\_work()} which inserts a function (already packaged inside a \texttt{work\_struct} descriptor) in a work queue; it receives a pointer \texttt{wq} to the \texttt{workqueue\_struct} descriptor and a pointer work to the \texttt{work\_struct} descriptor.

The \texttt{queue\_delayed\_work()} function is nearly identical to \texttt{queue\_work()}, except that it
receives a third parameter representing a time delay in system ticks and it is used to ensure a minimum delay before the execution of the pending function.

\texttt{cancel\_delayed\_work()} cancels a previously scheduled work queue function.
The \texttt{flush\_workqueue()} function receives a \texttt{workqueue\_struct} descriptor address and blocks the calling process until all functions that are pending in the work queue terminate. 


\subsubsection{The predefined work queue} 

The kernel offers a predefined work queue called \textit{events}, which can be freely used by every kernel developer. The predefined work queue is nothing more than a standard work queue that may include functions of different kernel layers and I/O drivers.

To make use of the predefined work queue, the kernel offers some APIs including \texttt{schedule\_work(struct work\_struct *work)} and \texttt{schedule\_work\_on(int cpu, struct work\_struct *work)}.

\subsection{\texttt{container\_of}}

The macro \texttt{container\_of(ptr, type, member)} takes, as you can see, three arguments: a pointer to the member of a data structure, the name of the type of the data structure, and the name of the member the pointer refers to. The macro yields the address of the container structure which accommodates the specified member.

\subsection{Timers}

On the x86 architecture, the kernel must explicitly interact with several kinds of clock circuits which are used both to keep track of the current time of day and to make precise time measurements. \textbf{The timer circuits are programmed by the kernel, so that they issue interrupts at a fixed, predefined frequency; such periodic interrupts are crucial for implementing the software timers used by the kernel and the user programs}. 

\begin{description}
\item[Time Stamp Counter (TSC)] It is a counter accessible through the 64-bit \textit{Time Stamp Counter} (\textbf{TSC}) register, which can be read using \texttt{rdtsc} assembly language instruction. \textbf{It represents a counter that is increased at each clock signal.} It is used by Linux to determine the clock signal frequency while initializing the system; that task is accomplished using \texttt{calibrate\_tsc()}.
\item[High Precision Event Timer (HPET)] The HPET represents a very powerful chip which provides up to \textbf{eight 32-bit or 64-bit independent counters exploitable by kernel}. Each counter is driven by its own clock signal, whose frequency must be at least 10 MHz and, therefore, the counter is increased at least once in \textbf{100 nanoseconds}. Any counter is associated with at most 32 timers, each of which is composed by a \textit{comparator} and a \textit{match register}. \textbf{The comparator is a circuit that checks the value in the counter against the value in the match register, and raises a hardware interrupt if a match is found. Some of the timers can be enabled to generate a periodic interrupt.}
\item[LAPIC] The Local APIC Timer (LAPIC-T) represents another time-measuring device. This timer has a counter of \textbf{32 bits long} used to store the number of of ticks that must elapse before the interrupt is issued; therefore, the local timer can be programmed to issue interrupts at very low frequencies. \textbf{Observe that local APIC timer sends an interrupt only to its processor}. The APIC’s timer is based on the bus clock signal and can be can be programmed in such a way to decrease the timer counter every 1, 2, 4, 8, 16, 32, 64, or 128 bus clock signals.
\end{description}

\subsubsection{The timer interrupt handler}

As said these timer circuits issues special interrupts called \textbf{timer interrupt}, which notifies the kernel
that one more time interval has elapsed. Interrupts can both involve a specific CPU (CPU local timer interrupt signals timekeeping activities related to the local CPU, such as monitoring how long the current process has been running and updating the resource usage statistics) or signal activities not related to a specific CPU, such as handling of software timers and keeping the system time up-to-date.

\subsection{Preemption}

As a general definition a kernel is \textit{preemptive} \textbf{if a process switch may occur while the replaced process is executing a kernel function, that is, while it runs in Kernel Mode}. 

\textbf{Kernel pre-emption is disabled when the \texttt{preempt\_count} field in the \texttt{thread\_info} descriptor referenced by the \texttt{current\_thread\_info()} macro \textbf{is greater than zero}}. Linux provides several APIs to manage kernel pre-emption:
\begin{description}
\item[\texttt{preempt\_count()}] Return the \texttt{preempt\_count} field in the \texttt{thread\_info} descriptor.
\item[\texttt{preempt\_disable()}] Increases by one the value of the preemption counter.
\item[\texttt{preempt\_enable\_no\_resched()}] Decreases by one the value of the preemption counter.
\item[\texttt{preempt\_enable()}] Decreases by one the value of the preemption counter, and invokes \texttt{preempt\_schedule()} if the \texttt{TIF\_NEED\_RESCHED} flag in the \texttt{thread\_info} descriptor is set
\end{description}

But there are other two very important API and they are related to \textbf{per-CPU variables}. 

Remember that a \textbf{\textit{per-CPU variables} is an array of data structures, one element per each CPU in the system}. A CPU should not access the elements of the array corresponding to the other CPUs; on the other hand, it can freely read and modify its own element without fear of race conditions, because it is the only CPU entitled to do so. \textbf{While per-CPU variables provide protection against concurrent accesses from several CPUs}, they do \textbf{not provide protection against accesses from asynchronous functions} (interrupt handlers and deferrable functions like tasklet and softirqs). \textbf{Therefore per-CPU variables are prone to race conditions caused by kernel pre-emption, both in uniprocessor and multiprocessor systems}. \textbf{As a general rule, a kernel control path should access a per-CPU variable with kernel preemption disabled.}. We have some API:

\begin{description}
\item[\texttt{get\_cpu\_var(name)}] Disables kernel preemption, then selects the local CPU’s element of the per-CPU array name
\item[\texttt{put\_cpu\_var(name)}] Enables kernel preemption.
\end{description}


\end{document}