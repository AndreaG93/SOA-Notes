\documentclass[10pt,a4paper]{article}


\usepackage[a4paper, total={8in, 10in}]{geometry}


\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\begin{document}

\section{Slide \textit{'hardware-insights'}}

\subsection{OOO Pipeline execution and imprecise exceptions}

When processor executes instructions in an order governed by the availability of input data and execution units, rather than by their original order in a program, we are adopting an \textbf{out-of-order execution paradigm}; \textbf{in other words different instructions can surpass each other depending on data or micro-controller availability.} We distinguish two events when we use this kind of paradigm: the \textbf{emission}, that is the action of injecting instructions into the pipeline; the \textbf{retire}, that is the action of committing instructions and making their side effects visible in terms of ISA exposed architectural resources

\textbf{Is important to recall that out-of-order completion must preserve exception behaviour in the sense that exactly those exceptions that would arise if the program were executed in strict program order actually do arise}. However, when we use OOO execution paradigm a processor may generate the so called \textbf{imprecise exceptions}. \textbf{An exception is imprecise if the processor state  when  an  exception  is  raised  does  not  look  exactly  as  if  the instructions were executed sequentially in strict program order}. In other words imprecise exceptions can occur because when:

\begin{itemize}
\item The pipeline may have already completed instructions that are later in program order than the instruction causing the exception.
\item The pipeline may have not yet completed some instructions that are earlier in program order than the instruction causing the exception.
\end{itemize}

\textbf{Recall that any instruction may change the micro-architectural state, although finally not committing its actions onto ISA exposed resources.} Since the the pipeline may have not yet completed the execution of instructions preceding the offending one, hardware status can been already changed an this fact can be exploited by several attacks (like \textbf{Meltdown}).

\subsection{Tomasulo algorithm}

Tomasuloâ€™s algorithm is a computer architecture hardware algorithm for dynamic scheduling of instructions that allows out-of-order execution and enables more efficient use of multiple execution units. Suppose two operation A and B such that A precedes B in program order, that algorithm permit to resolve three hazard:

\begin{description}
\item[RAW (Read After Write)] B reads a datum before A writes it.
\item[WAW (Write After Write)] B writes a datum before A writes the same datum.
\item[WAR (Write After Read)] B writes a datum before A reads the same datum.
\end{description}

\textbf{RAW hazards are avoided by executing an instruction only when its operands are available, while WAR and WAW hazards are eliminated by \textit{register renaming}}

\subsection{UMA}

When we have a \textbf{single main memory} that has a \textit{symmetric relationship to all processors and a uniform access time from any processor}, these multiprocessors are most often called \textit{symmetric shared-memory multiprocessors} (\textbf{SMPs}), and this style of architecture is sometimes called \textit{uniform memory access} (\textbf{UMA}): in fact, \textbf{all processors have a uniform latency from memory}. \textbf{The term shared memory refers to the fact that the address space is shared; that is, the same physical address on two processors refers to the same location in memory.} In this architecture all CPUs can have one or more level of cache. However this architecture is obliviously \textbf{not} scalable when the number of CPUs grows.

\subsection{NUMA}

When we have a distributed memory, a multiprocessor architecture is usually called \textit{distributed shared-memory} (\textbf{DSM}). When we use this kind of system, we have two benefits:
\begin{itemize}
\item A cost-effective way to scale the memory bandwidth if most of the accesses are to the local memory in the node.
\item Reduces the latency for accesses to the local memory by a CPU.
\end{itemize}

The key disadvantages for that architecture is that communicating data between processors becomes more complex, \textbf{and that it requires more effort in the software to take advantage of the increased memory bandwidth afforded by distributed memories.}

The DSM multiprocessors are also called \textbf{NUMAs} (\textit{non-uniform memory access}), \textbf{since the access time depends on the location of a data word in memory.} In fact, when a CPU wants to access to an item stored into his node, performing a \textit{local access} involving inner private/shared caches and controllers, access latency is very low. However when a CPU wants to access to an item stored on another node, performing a \textit{remote accesses} involving remote controllers and caches, latency can be very high respect to previous case.

\subsection{The problem of cache coherence}

Unfortunately, \textbf{the view of memory held by different processors is through their individual caches}, which, without any additional precautions, could end up seeing different values of a same shared data (\textbf{cache coherence} problem).

By definition, \textbf{coherence defines what values can be returned by a read} (a \textbf{cache coherence protocols} defines how to maintain coherence) while \textbf{consistency determines when a written value will be returned by a read} (a \textbf{memory consistency protocol} defines when written value must be seen by a reader).

A memory system is coherent if:
\begin{enumerate}
\item A read from location $X$, previously written by a processor, returns the last written value if no other processor carried out writes on $X$ in the meanwhile. \textbf{This property preserve program order that is the causal consistency along program order.}

\item A read by a processor to location $X$ that follows a write by another processor to $X$ returns the written value if the read and write are sufficiently separated in time and no other writes to $X$ occur between the two accesses. \textbf{This property assure that a processor couldn't continuously
read an old data value (Avoidance of staleness).}

\item \textbf{Writes to the same location are serialized}; that is, two writes to the same location by any two processors are seen in the same order by all processors.
\end{enumerate}

The choice and the design of a coherence protocol depends on many factors including: overhead, latency, cache policies, interconnection topology and so on.
\textbf{However the Key to implementing a cache coherence protocol is tracking the state of any copy of a data block}. There are two classes of protocol which define when update aforementioned copies:
\begin{description}
\item[Update protocol] When we use this type of protocol, also called \textit{write update} or \textit{write broadcast}, when a core writes to a block, it updates all other copies (\textbf{it consumes considerably more bandwidth}).

\item[Invalidate protocol] When we use this type of protocol, a processor has \textbf{exclusive access} to a data item before it writes that item; moreover that CPU invalidates other copies on a write that is no other readable or writeable copies of an item exist when the write occurs. \textbf{It is the most common protocol, but suffer of some latency.}

\end{description}

\subsection{Snooping protocol}

The key to implementing an invalidate protocol is the use of the bus, or another broadcast medium, called \textit{network} to perform invalidates and to issue "transactions" on the state of cache blocks.

To perform any operation, the processor simply \textbf{acquires} bus access and broadcasts the address to be invalidated on the bus. All processors continuously \textbf{snoop} on the bus, watching the addresses. The processors check whether the address on the bus is in their cache. If so, the corresponding data in the cache are invalidated. \textbf{A state transition cannot occur unless the broadcast medium is acquired by the source controller and are carried out atomically with a distribute fashions thanks to \textit{serialization} over the broadcast medium}.

When we perform a read, we also need to locate a data item when a cache miss occurs. In a \textbf{write-through cache}, it is easy to find the recent value of a data item, \textit{since all written data are always sent to the memory, from which the most recent value of a data item can always be fetched} (using write through simplifies the implementation of cache coherence). For a \textbf{write-back cache}, the problem of finding the most recent data value is
harder, since the most recent value of a data item can be in a cache rather than in memory (the CPU must get data from another cache)

\newpage
\section{Slide \textit{'kernel-programming-basics'}}

\subsection{Segmentation}

Intel microprocessors perform address translation in \textbf{three} different ways:
\begin{description}
\item[Real Mode] This mode exists mostly to \textbf{maintain processor compatibility with older models and to allow the operating system to bootstrap}. In this modality, a logical address is composed of a \texttt{seg} segment (hold by a 16 bit \textit{segment register}) and an \texttt{off} offset (hold by a 16 bit \textit{general register}) while the corresponding physical address is simply computed using \texttt{seg*16 + off}: \textit{as a result, no Global Descriptor Table, Local Descriptor Table, or paging table is needed by the CPU addressing circuit to translate a logical address into a physical one}. \textbf{Observer that in real mode no segment specific protection information are provided, therefore this modality is unsuitable for modern operating system}. Around 1MB ($2^20$ B) of memory is allowed
\item[Protected Mode] Similarly to real mode, a logical address consists of two parts: a \textbf{segment identifier} and an \textbf{offset} that specifies the relative address within the segment. However the target segment \textit{\textbf{identifier}} is a \textbf{13-bit field} present into a \textbf{16 bit field} called the \textbf{segment selector} keep by 16 bit segment register (\textbf{remaining 3 bit are used for protection purposes}) while a 32 bit general register holds the offset. The corresponding physical address is computed using the linear address of the first byte of the segment (which is hold by a special table) using following equation: \texttt{TABLE[segment].base + offset}. In this modality Up to 4 GB of memory is allowed.
\item[Long Mode] Identical to protected mode but the offset is hold by 64-bit general registers (\textit{although up to 48-bit are used in canonical form}). In this way is possible to address up to $2^{48}$ B (256 TB) of linear memory.
\end{description}

\textbf{From now we describe protected mode}. To make it easy to retrieve segment selectors quickly, the processor provides \textbf{segmentation registers} whose only purpose is to hold segment selectors; these registers are called \texttt{cs} (\textbf{code segment register, which points to a segment containing program instructions}), \texttt{ss} (\textbf{stack segment register, which points to a segment containing the current program stack}), \texttt{ds} (\textbf{data segment register, which points to a segment containing global and static data}), \texttt{es}, \texttt{fs}, and \texttt{gs} (\textbf{these three segmentation registers are general purpose and may refer to arbitrary data segments}.)

To be more precise, segment selectors have three fields:
\begin{description}
\item[index (13-bit)] identifies the Segment Descriptor entry contained in the GDT or in the LDT (see below).
\item[Table Indicator (1-bit)] specifies whether the Segment Descriptor is included in the GDT (TI = 0) or in the LDT (TI = 1).
\item[Requestor Privilege Level (RPL) (2-bit)] specifies the \textbf{Current Privilege Leve}l of the CPU when the corresponding Segment Selector is loaded into the \texttt{cs} register. The value 0 denotes the highest privilege level, while the value 3 denotes the lowest one. Linux uses only levels 0 and 3, which are respectively called \textbf{Kernel Mode} and \textbf{User Mode.}

\end{description}

 the 13-bit field which identifies the Segment Descriptor entry contained in the GDT or in the LDT (see below), 1-bit field which is te Table Indicator that specifies whether the Segment Descriptor is included in the GDT (TI = 0) or in the LDT (TI = 1). Requestor Privilege Level: specifies the Current Privilege Level of the CPU when the corresponding Seg-
ment Selector is loaded into the cs register; it also may be used to selectively weaken the processor priv-
ilege level when accessing data segments (see Intel documentation for details).

\subsubsection{GDT and LDT}

Each segment is represented by an \textbf{8-byte Segment Descriptor} that describes the segment characteristics. Segment Descriptors are stored either in the \textbf{Global Descriptor Table} (\textbf{GDT}) or in the \textbf{Local Descriptor Table} (\textbf{LDT}) \textbf{which are kept in main memory}.

\textbf{The address (32 bit in protected mode, 64 bit in long mode) and size (always 16 bit) of the GDT in main memory are contained in the \texttt{gdtr} control register, while the address and size of the currently used LDT are contained in the \texttt{ldtr} control register.}

\textbf{GDT is used for the mapping of linear addresses \textit{at least} for kernel mode} (that is to manage kernel level segments) while \textbf{LDT is used for user mode} (each process is permitted to have its own LDT). \textbf{However GDT is the unique used segment table in most operating systems}.

\subsubsection{Segment descriptor}

A Segment Descriptor, that is an entry of the GDT/LDT, has \textbf{many} fields including:
\begin{description}
\item[Base] Contains the linear address of the first byte of the segment.
\item[G (Granularity flag)] if equal to 0, the segment size is expressed in bytes; otherwise, it is expressed in multiples of 4096 bytes.
\item[Limit] Holds the offset of the last memory cell in the segment, \textit{thus binding the segment length}. When G is set to 0, the size of a segment may vary between 1 byte and 1 MB; otherwise, it may vary between 4 KB and 4 GB.
\item[DPL (Descriptor Privilege Level)(2 bit)]: \textit{used to restrict accesses to the segment}. It represents the minimal CPU privilege level requested for accessing the segment. Therefore, a segment with its DPL set to 0 is accessible only when the CPL is 0 (Kernel Mode) while a segment with its DPL set to 3 is accessible with every CPL value.
\item[P (Segment-Present)] is equal to 0 if the segment is not stored currently in main memory. Linux \textbf{always} sets this flag to 1, because it never swaps out whole segments to disk.
\end{description}

Now be careful. Because a Segment Descriptor is 8 bytes long, \textbf{its relative address inside the GDT/LDT is obtained by multiplying the 13-bit index field of the Segment Selector by 8}. For instance, if the GDT is at \texttt{0x00020000} (the value stored in the gdtr register) and the index specified by the Segment Selector is 2, the address of the corresponding Segment Descriptor is \texttt{0x00020000 + (2 x 8)}, or 0x00020010 (\textbf{all this in protected mode!!})

The first entry of the GDT is always set to 0. This ensures that logical addresses with a \texttt{null} Segment Selector will be considered invalid, thus causing a processor exception. 

\textbf{There a sort of cache to speed up the access to segment descriptor}. In fact for each of the six segmentation registers \textbf{there are additional non-programmable register which are used to contains the 8-byte Segment Descriptor}.  Every time a Segment Selector is loaded in a segmentation register, the corresponding Segment Descriptor is loaded from memory into the matching nonprogrammable CPU register. From then on, translations of logical addresses referring to that segment can be performed without accessing the GDT.

\textbf{After to have computed the address of a Segment Descriptor, to obtain the linear address, we adds the offset of the logical address to the Base field of the Segment Descriptor.}

\subsubsection{Segmentation in Linux}

Linux uses segmentation in a very limited way. Linux prefers paging to segmentation for the following reasons:
\begin{enumerate}
\item Memory management is simpler when all processes use the same segment register values, that is, when they share the same set of linear addresses.
\item \textbf{One of the design objectives of Linux is portability to a wide range of architectures} (RISC architectures in particular have limited support for segmentation).
\end{enumerate}



 \textbf{All Linux processes running in User Mode use the same pair of segments to address instructions and data} which are called \textbf{user code segment} and \textbf{user data segment}, respectively. Similarly, \textbf{all Linux processes running in Kernel Mode use the same pair of segments to address instructions and data}: they are called \textbf{kernel code segment} and \textbf{kernel data segment}. Now there are some differences about values stored in the fields of their segmentation descriptors: for instance user data/code segments have a DPL equal to 3, while kernel data/code segments have a DPL equal to 0. All these segment descriptors have their base set to \texttt{0x00000000}. 

The corresponding Segment Selectors are defined by the macros \texttt{\_\_USER\_CS}, \texttt{\_\_USER\_DS}, \texttt{\_\_KERNEL\_CS}, and \texttt{\_\_KERNEL\_DS}, respectively. To address the kernel code segment, for instance, the kernel just loads the value yielded by the \texttt{\_\_KERNEL\_CS} macro into the \texttt{cs} segmentation register. It is sufficient to load \texttt{\_\_KERNEL\_CS} into cs whenever the CPU switches to Kernel Mode (CPL equal to 0).

\subsubsection{GDT}

\textbf{There is one GDT for every CPU in the system} that is we have some replicated GDT which differs by some entries (this is important for performance motivation and for transparency of data access separation (same segment = to access different linear addresses by more CPU)). All GDTs are stored in the \texttt{cpu\_gdt\_table} array.  \textbf{Each GDT includes 18 segment descriptors and 14 unused, or reserved entries including the null one}. The most important segment descriptors are:
\begin{itemize}
\item Four segment descriptors corresponding to user and kernel code and data segments (see previous section).
\item A \textbf{Task State Segment} (TSS), different for each processor in the system.
\item A segment including the default Local Descriptor Table (LDT).
\item Three \textbf{Thread-Local Storage} (TLS) segments: \textbf{this is a mechanism that allows multithreaded applications to make use of up to three segments containing data local to each thread}. The \texttt{set\_thread\_area()} and \texttt{get\_thread\_area()} system calls, respectively, create and release a TLS segment for the executing process.
\end{itemize}

\subsection{TSS}

The 80x86 architecture includes a specific segment type called the \textbf{Task State Segment} (\textbf{TSS}). Linux uses the tss\_struct structure to describe the format of the TSS. As already mentioned in
\textbf{The Task State Segments are sequentially stored into \texttt{init\_tss} array which stores one TSS for each CPU on the system}: in particular, the Base field of the TSS descriptor for the nth CPU points to the nth component of the \texttt{init\_tss} array. The DPL is set to 0, because processes in User Mode are not allowed to
access TSS segments.

\textbf{According to the original Intel design, TSS is normally used to store hardware contexts in case of process switch.} However, \textbf{Linux doesn't use hardware context switches because it uses a single TSS for each processor, instead of one for every process}. Linux use the TSS in only two cases:

\begin{itemize}
\item when the CPU switches from User Mode to Kernel Mode, \textbf{it fetches the address of the Kernel Mode stack from the TSS} 

Note that each process descriptor includes a field called \texttt{thread} of type \texttt{thread\_struct}, in
which the kernel saves the hardware context. This data structure includes fields for most of the CPU registers, except the general-purpose registers such as eax, ebx, etc., which are stored in the Kernel Mode stack.
\item When a User Mode process attempts to access an I/O port, the CPU may need to access an I/O Permission Bitmap stored in the TSS to verify whether the process is allowed to address the port.
\end{itemize}

80x86 architecture provides a segment register called the task register (\texttt{TR}) to hold a segment selector that points to a valid TSS segment descriptor which resides in the GDT. To initialize TR, 80x86 ISA provides the assembly instruction \texttt{ltr} which loads the source operand into the task register, where the source operand (a general-purpose register or a memory location) contains a segment selector that points to the TSS contained into GDT. After that, the CPU uses the segment selector into \texttt{tr} to locate the segment descriptor for the TSS in GDT.

\subsection{Control register}

A \textbf{control registers} are a set of registers which changes or controls the general behavior of a CPU like its addressing mode, paging control, and coprocessor control and so on. In modern x86 machines we have:
\begin{description}
\item[CR0] On x86-64 processors in long mode, it is 64 bits long register which have many control bits. The most important are:
\begin{itemize}
\item Bit 0 - PE Protected Mode Enable -If 1, system is in protected mode, else system is in real mode.
\item Bit 31 - PG Paging If 1, enable paging and use the CR3 register, else disable paging. 
\item Bit 16 - WP Write protect When set, the CPU can't write to read-only pages when privilege level is 0 
\end{itemize}
\item[CR1] Reserved.
\item[CR2] Contains a value called \textit{Page Fault Linear Address} (\textbf{PFLA}). \textbf{When a page fault occurs, the address the program attempted to access is stored in the CR2 register.}
\item[CR3] Used when virtual addressing is enabled, hence when the PG bit is set in CR0. CR3 holds a pointer to the \textit{page directory} for the current task
\end{description}

\subsection{Management of interrupts}

Linux keeps a system table called \textbf{Interrupt Descriptor Table} (\textbf{IDT}) to \textbf{associate to each interrupt or exception vector with the address of the corresponding handler}. 

On each CPU, the \texttt{idtr} (\textit{interrupt descriptor table register}) CPU register allows the IDT to be located in memory. In fact, in protected mode it holds:
\begin{enumerate}
\item a 48 bit field (\textit{64 bit in long mode}) which specifies the IDT base linear address.
\item a 16 bit field which provides the IDT max length, that is the number of entries currently present in it. 
\end{enumerate}

There are two assembly instruction capable to manipulate the IDT:
\begin{description}
\item[\texttt{lidt} (Load IDT)] It is used to Load the values in the source operand into IDT. 
\item[\texttt{sidt} (Store IDT)] It is used to store the content the interrupt descriptor table register in the destination operand.
\end{description}

\textbf{Remember that the IDT must be initialized by Linux before enabling interrupts by using the \texttt{lidt} assembly language instruction.} IDT is initialized with an excep-
tion handler function for each recognized exception by \texttt{trap\_init()} function (\texttt{/usr/src/linux/kernel/traps.c}). For example the interrupt 0x80 is associated to \texttt{\_system\_call} entry point using \texttt{set\_system\_gate (0x80, \&system\_call)}.


\textbf{In protected mode, each IDT's entry is an 8-byte (64 bit) descriptor} of type struct \texttt{desc\_struct} (which is defined in \texttt{include/asm-i386/desc.h}). In long mode each entry in the IDT grows by 64-bits. The IDT contains some descriptor's type and the value of the \texttt{Type} field encoded in the bits 40â€“43 identifies the descriptor type:
\begin{description}
\item[Interrupt Gate Descriptor] \textbf{They are used to handle interrupts}. \textit{Includes the Segment Selector and the offset inside the segment of an interrupt
or exception handler}. While transferring control to the proper segment, the processor clears the \texttt{IF} (\textbf{Interrupt flag}) to disable further maskable interrupts (bit 40 set to 0, 41-43 to 1). 
\item[Trap Gate Descriptor] \textbf{They are used to handle exceptions}. Similar to an interrupt gate, except that while transferring control to the proper
segment, the processor does not modify the IF flag (bit 40 set to 1, 41-43 to 1).
\end{description}

A fully populated IDT is 2 KB (256 entries of 8 bytes each) in length. \textbf{All entry from 0 to 31 are reserved by Intel for processor generated exceptions} (general protection fault, page fault, etc.) while all the other entries are available for system programming purposes.

The following architecture-dependent functions (defined in \texttt{arch/i386/kernel/traps.c}) are used to insert on entry in the IDT (note that \texttt{n} indicates the target entry of the IDT, while \texttt{addr} indicates the address of the software module to be invoked for handling the trap or the interrupt):
\begin{description} 
\item[\texttt{set\_trap\_gate(n,addr)}] Inserts a trap gate in the nth IDT entry. The Segment Selector inside the gate is set to the kernel codeâ€™s Segment Selector. The Offset field is set to addr. \textbf{The DPL field is set to 0} (\textit{we cannot rely on the \texttt{int} assembly instruction unless we are already executing in kernel mode}).
\item[\texttt{set\_intr\_gate(n,addr)}] Inserts an interrupt gate in the nth IDT entry. The Segment Selector inside the gate is set to the kernel code's Segment Selector. The Offset field is set to addr. The DPL field is set to 0.
\item[\texttt{set\_system\_gate(n,addr)}] Inserts a trap gate in the nth IDT entry. The Segment Selector inside the gate is set to the kernel code's Segment Selector. The Offset field is set to addr. \textbf{The DPL field is set to 3}.
\end{description}









\subsubsection{System calls}

Linux System Call Table

The following table lists the system calls for the Linux 2.2 kernel. It could also be thought of as an API for the interface between user space and kernel space. My motivation for making this table was to make programming in assembly language easier when using only system calls and not the C library (for more information on this topic, go to http://www.linuxassembly.org). On the left are the numbers of the system calls. This number will be put in register %eax. On the right of the table are the types of values to be put into the remaining registers before calling the software interrupt 'int 0x80'. After each syscall, an integer is returned in %eax. 



\newpage
\section{Slide \textit{'kernel-level-memory-management'}}

\subsection{Page Descriptor}

In Linux, \textbf{state information of a page frame is kept in a page descriptor} of type \texttt{struct page} (or struct \texttt{mem\_map\_t}), and all page descriptors, which are 32 byte long, are stored in an array called \texttt{mem\_map} (the space required by it is slightly less than 1\% of the whole RAM). These data structures are defined into \texttt{include/linux/mm.h}. 

The \texttt{virt\_to\_page(addr)} macro yields the address of the page descriptor associated with the linear address \texttt{addr}. 

\texttt{struct page} has many fields but the most important are:
\begin{description}

\item[\texttt{atomic\_t \_count}] It represent a usage reference counter for the page. If it is set to -1, the corresponding page frame is free and can be assigned to any process or to the kernel itself. If it is set to a value greater than or equal to 0, the page frame is assigned to one or more processes or is used to store some kernel data structures. The \texttt{page\_count()} function returns the value of the \texttt{\_count} field increased by one, that is, the number of users of the page. This field is managed via atomic updates, such as with \texttt{LOCK} directives.
\item[\texttt{struct list\_head lru}] Contains pointers to the least recently used doubly linked list of pages. 
\item[\texttt{unsigned long flags}] Array of flags used to describe the status of current page frame (but also encodes the zone number to which the page frame belongs). There are up to 32 flags and Linux kernel defines many macros to manipulate them. Some flags are:
\begin{description}
\item[\texttt{PG\_locked}] The page is locked; for instance, it is involved in a disk I/O operation.
\item[\texttt{PG\_dirty}] The page has been modified.
\item[\texttt{PG\_reserved}] The page frame is reserved for kernel code or is unusable.
\end{description}

\end{description}

\subsection{Free list}

Linux uses \textbf{free list} to manage memory allocation. \textbf{It operates by connecting unallocated regions of memory together in a linked list, using the first word of each unallocated region as a pointer to the next.} 

Free lists make the allocation and deallocation operations very simple. \textbf{To free a region, one would just link it to the free list. To allocate a region, one would simply remove a single region from the end of the free list and use it}. 

\subsection{NUMA}

Is extremely important to remember that Linux 2.6 supports the \textit{Non-Uniform Memory Access} (\textbf{NUMA}) model, \textbf{in which the access times for different memory locations from a given CPU may vary} and, according to that architecture, physical memory is partitioned in several \textbf{nodes}. The time needed by a given CPU to access pages within a single node is the same. However, this time might not be the same for two different CPUs. 

\subsection{NUMA Node Descriptor}

\textbf{Be careful that Linux splits physical memory inside each node into several zones. We have 3 free lists of frames, depending on the frame positioning within available zones (defined in \texttt{include/linux/mmzone.h}) which are:}

\begin{description}
\item[\texttt{ZONE\_DMA}] Contains page frames of memory below 16 MB, that is page frames that can be used by old ISA-based devices (\textit{Direct Memory Access} (DMA) processors).
\item[\texttt{ZONE\_NORNMAL}] Contains page frames of memory at and above 16 MB and below 896 MB (direct mapped by the kernel).
\item[\texttt{ZONE\_HIGHMEM}] Contains page frames of memory at and above 896 MB (only page cache and user).
\end{description}

To represent a NUMA node, Linux uses a descriptor of type \texttt{struct pg\_data\_t}. All node descriptors are stored in a singly linked list, whose first
element is pointed to by the \texttt{pgdat\_list} variable. Be careful to the fact that this data structure is used by Linux kernel even if the architecture is based on \textit{Uniform Memory Access} (\textbf{UMA}): in fact Linux makes use of a single node that includes all system physical memory. Thus, the \texttt{pgdat\_list} variable points to a list consisting of a single element (node 0) stored in the \texttt{contig\_page\_data} variable.

Remember that free lists information is kept within the \texttt{struct pg\_data\_t} data structure. In fact the most important fields of \texttt{struct pg\_data\_t} are:

\begin{description}
\item[\texttt{struct page *node\_mem\_map}] Array of page descriptors of the node
\item[\texttt{struct zone [] node\_zones}] Array of zone descriptors of the node
\end{description}

\subsection{Zone Descriptor}

Obliviously each memory zone has its own descriptor of type \texttt{struct zone} and many fields of this data structure are used for page frame reclaiming. However, most important fields are:

\begin{description}
\item[\texttt{struct page * zone\_mem\_map}] Pointer to first page descriptor of the zone.
\item[\texttt{spinlock\_t lock}] Spin lock protecting the descriptor.
\item[\texttt{struct free\_area [] free\_area}] Identifies the blocks of free page frames in the zone
\end{description}

In summary, Linux has links to the memory node and to the zone inside the node that includes the corresponding page frame of type \texttt{struct page}.

\subsection{Buddy allocator}

The technique adopted by Linux to solve the external fragmentation problem is based on the well-known \textbf{buddy system} algorithm. All free page frames are grouped into 11 lists of blocks that contain groups of 1, 2, 4, 8, 16, 32, 64, 128, 256, 512, and 1024 contiguous page frames, respectively. The largest request of 1024 page frames corresponds to a chunk of 4 MB of contiguous RAM. We use the term \texttt{order} \textbf{to indicate the logarithmic size of a block}.

Assume there is a request for a group of 256 contiguous page frames (i.e., one megabyte). The algorithm checks first to see whether a free block in the 256-page-frame list exists. If there is no such block, the algorithm looks for the next larger block â€” a free block in the 512-page-frame list. If such a block exists, the kernel allocates 256 of the 512 page frames to satisfy the request and inserts the remaining 256 page frames into the list of free 256-page-frame blocks. If there is no free 512-page block, the kernel then looks for the next larger block (i.e., a free 1024-page-frame block). If
such a block exists, it allocates 256 of the 1024 page frames to satisfy the request, inserts the first 512 of the remaining 768 page frames into the list of free 512-page-frame blocks, and inserts the last 256 page frames into the list of free 256-page-frame blocks. If the list of 1024-page-frame blocks is empty, the algorithm gives up and signals an error condition.

\textbf{Linux 2.6 uses a different buddy system for each zone}. Thus, in the x86 architecture, there are \textbf{3 buddy systems}: the first handles the page frames suitable for ISA DMA, the second handles the "normal" page frames, and the third handles the high memory page frames. Each buddy system relies on the following main data structures:
\begin{itemize}
\item \textbf{The \texttt{mem\_map} array where all page descriptors are stored}. Actually, each zone is concerned with a subset of the \texttt{mem\_map} elements. The first element in the subset and its number of elements are specified, respectively, by the \texttt{zone\_mem\_map} and \texttt{size} fields of the zone descriptor.

\item The array consisting of eleven elements of type \texttt{struct free\_area}, one element for each group size. As we said the array is stored in the \texttt{free\_area} field of the zone descriptor.
\end{itemize}

Let us consider the $k^{th}$ element of the \texttt{struct free\_area} array in the zone descriptor, which identifies all the free blocks of size $2^k$. In this data structure there is a pointer of type struct \texttt{list\_head}  which is is the head of a doubly linked circular list that collects the page descriptors associated with the free blocks of $2^k$ pages. Besides the head of the list, the $k^{th}$ element of the \texttt{struct free\_area} array includes also the field \texttt{nr\_free}, which specifies the number of free blocks of size $2^k$ pages, and a pointer to a bitmap that keeps fragmentation information.  

Recall that spin locks are used to manage \texttt{mem\_map} AND \texttt{struct free\_area} array.

\textbf{To achieve better performance a little number of page frames are kept in cache to quickly satisfy the allocation requests for single page frames}.

\subsection{API}

Page frames can be requested by using some different functions and macros (APIs) (they return \texttt{NULL} in case of failure, a linear address of the first allocated page in case of success) which prototype are stored into \texttt{\#include <linux/malloc.h>}. The most important are:

\begin{description}
\item[\texttt{get\_zeroed\_page(gfp\_mask)}] Function used to obtain a page frame filled with zeros.
\item[\texttt{\_\_get\_free\_page(gfp\_mask)}] Macro used to get a single page frame.
\item[\texttt{\_\_get\_free\_pages(gfp\_mask, order)}] Macro used to request $2^{order}$ contiguous page frames returning the linear address of the
first allocated page.
\item[\texttt{free\_page(addr)}] This macro releases the page frame having the linear address \texttt{addr}.

The parameter \texttt{gfp\_mask} is a group of flags that specify \textbf{how to look for free page frames} and they are extremely important when we require page frame allocation in different contexts including: 
\begin{description}
\item[Interrupt context] allocation is requested by an \textbf{interrupt handler} which uses above function with \texttt{GFP\_ATOMIC} flag (equivalent to \texttt{\_\_GFP\_HIGH}) \textbf{which means that the kernel is allowed to access the pool of reserved page frames: therefore the call cannot lead to sleep (that is no wait)}  An atomic request never blocks: if there are not enough free pages the allocation simply fails.

\item[Process context] allocation is caused by a system call using \texttt{GFP\_KERNEL} or \texttt{GFP\_USER} (both equivalent to \texttt{\_\_GFP\_WAIT | \_\_GFP\_IO | \_\_GFP\_FS}) according to which kernel is allowed to block the current process waiting for free page frames (\texttt{\_\_GFP\_WAIT}) and to perform I/O transfers on low memory pages in order to free page frames (\texttt{\_\_GFP\_IO}): therefore the call can lead to sleep.
\end{description}

\end{description}



\subsection{TLB operation}

Besides general-purpose hardware caches, x86 processors include a cache called \textit{Translation Lookaside Buffers} (\textbf{TLB}) to speed up linear address translation.

When a linear address is used for the first time, the corresponding physical address is computed through slow accesses to the Page Tables in RAM. The physical address is then stored in a TLB entry so that further references to the same linear address can be quickly translated.

In a multiprocessor system, \textbf{each CPU has its own TLB, called the \textbf{local TLB} of the CPU}. Contrary to the hardware cache, the corresponding entries of the TLB need \textbf{not} be synchronized, because processes running on the existing CPUs may associate the same linear address with different physical ones.

\textbf{When the \texttt{cr3} control register of a CPU is modified, the hardware automatically invalidates all entries of the local TLB, because a new set of page tables is in use (page table changes).} \textbf{However changes inside the current page table are not automatically reflected within the TLB.}

Fortunately, Linux offers several TLB flush methods that should be applied appropriately, depending on the type of page table change:
\begin{description}
\item[\texttt{flush\_tlb\_all}] This flushes the \textbf{entire TLB on all processors} running in the system, which makes it the most expensive TLB flush operation. It is used when we have made changes into the kernel page table entries. \textbf{After it completes, all modifications to the page tables will be visible globally to all processors.}

\item[\texttt{flush\_tlb\_mm(struct mm\_struct *mm)}] Flushes all TLB entries of the non-global pages owned by a given process that is all entries related to the userspace portion for the requested \texttt{mm} context. Is used when forking a new process.

\item[\texttt{flush\_tlb\_range}] Flushes the TLB entries corresponding to a linear address interval of a given process and is used when releasing a linear address interval of a process (when \texttt{mremap()} or \texttt{mprotect()} is used).

\item[\texttt{flush\_tlb\_page}] Flushes the TLB of a single Page Table entry of a given process and is used when handling a page fault.

\item[\texttt{flush\_tlb\_pgtables}] Flushes the TLB entries of a given contiguous subset of page tables of a given process and is called when a region is being unmapped and the page directory entries are being reclaimed 
\end{description}

Despite the rich set of TLB methods offered by the generic Linux kernel, every microprocessor usually offers a far more restricted set of TLB-invalidating assembly language instructions. \textbf{Intel microprocessors offers only two TLB-invalidating techniques: the automatic flush of all TLB entries when a value is loaded into the cr3 register and the \texttt{invlpg} assembly language instruction which invalidates a single TLB entry mapping a given linear address.}

The architecture-independent TLB-invalidating methods are extended quite simply to multiprocessor systems. \textbf{The function running on a CPU sends an Interprocessor Interrupt to the other CPUs that forces them to execute the proper TLB-invalidating function} (\textbf{expensive} operation (\textit{direct cost}) due to latency for cross-CPU coordination in case of global TLB flushes).

Remember that flush a TLB has the direct cost of the latency of the firmware level protocol for TLB entries invalidation (selective vs non-selective). Recall that flush TLB lead to \textbf{indirect cost} of refilling TLB entries and the latency experimented by MMU firmware upon misses in the translation process of virtual to physical addresses.

\subsubsection{When flush TLB?}

As a general rule,\textbf{ any process switch implies changing the set of active page tables and therefore local TLB entries relative to the old page tables must be flushed}; this is done automatically when the kernel writes the address of the new Page Global Directory into the \texttt{cr3} control register.

Besides \textbf{process switches}, there are other cases in which the kernel needs to flush some entries in a TLB. For instance, when the kernel assigns a page frame to a User Mode process and stores its physical address into a Page Table entry, it must flush any local TLB entry that refers to the corresponding linear address (virtual addresses accessible \textbf{locally} in time-sharing concurrency). On multiprocessor systems, the kernel also must flush the same TLB entry on the CPUs that are using the same set of page tables, if any (virtual addresses accessible \textbf{globally}  by every CPU/core in real-time-concurrency).

Kernel-page mapping has a \textit{global} nature, therefore when we use \texttt{vmalloc()} / \texttt{vfree()} on a specific CPU, all the other must observer mapping updates and TLB flush is necessary. 

\newpage
\section{Slide \textit{'kernel-level-task-management'}}

\subsection{Interrupt handling}

Under Linux, hardware interrupts are called \textbf{IRQ}'s (\textit{Interrupt Requests}) and their management  typically occurs via a \textbf{two-level logic}:
\begin{description}
\item[Top Half] A routine that actually responds to the interrupt and do a minimal amount of work to schedule its bottom half (this operation is very fast).
\item[Bottom Half] A routine scheduled by top half which execute whatever other work is required to handle the interrupt (such as awakening processes, starting up another I/O operation, and so on)
\end{description}

For instance, when a network interface reports the arrival of a new packet, the top half routine just retrieves the data and pushes it up to the protocol layer; actual processing of the packet is performed in a bottom half.

The most important aspect of this setup it that it permits the \textit{top half to service a new interrupt while the bottom half is still working}; \textbf{in fact all interrupts are enabled during execution of the bottom half}. Generally the execution of top half code is handled according to a \textit{non-interruptible scheme} (\textbf{but isn't mandatory}). 

This scheme permit to \textbf{avoid to keep locked resources when an interrupt occurs} (we may incur the risk of delaying critical actions as a spin-lock release) \textbf{avoiding possible deadlocks} when a slow interrupt management is hit by the activation of another one that needs the same resources. \textbf{Moreover this scheme keep
kernel response time small which is a very important property for many time-critical applications that expect their interrupt requests to be serviced in a few milliseconds.}

\subsection{Softirqs, Tasklets and work queues}

Form Linux 2.6, two different mechanisms are used to implement top/bottom-half processing:
\begin{itemize}
\item The so-called \textit{deferrable functions}, which we will call as \textbf{softirqs} and \textbf{tasklets}: they are very fast, but all tasklet code must be atomic.
\item The \textbf{Workqueues}, which may have a higher latency but that are allowed to sleep.
\end{itemize}

\subsubsection{Softirqs}

\textbf{Softirqs are statically allocated}, that is they are defined at compile time. The main data structure used to represent softirqs is the \texttt{softirq\_vec} array, which includes \texttt{NR\_SOFTIRQS} (32 entries) elements of type \texttt{softirq\_action}. \textbf{Observer that the priority of a softirq is the index of the corresponding \texttt{softirq\_action} element inside the array}. Some of the softirqs used in Linux are:

\begin{description}
\item[\texttt{HI\_SOFTIRQ}] With priority equal to 0 (first element of array) and it handles high priority tasklets.
\item[\texttt{TIMER\_SOFTIRQ}] With priority equal to 1 and it is used for timer related interrupts.
\end{description}

Another crucial data structure for implementing the softirqs is a \textbf{per-CPU 32-bit mask describing the pending softirqs}; it is stored in the \texttt{\_\_softirq\_pending} field of the \texttt{irq\_cpustat\_t} data structure (which is one of the data structure used per each CPU in the system). To get and set the value of the bit mask, the kernel makes use of the \texttt{local\_softirq\_pending()}. This is way softirqs can run concurrently on several CPUs, even if they are of the same type.

During interrupt acceptance, top half routine set properly the bit mask in the \texttt{\_\_softirq\_pending} field and then exit. 

Checks for active (pending) softirqs should be perfomed periodically, but without inducing too much overhead. They are performed in a few points of the kernel code. 

For this purpose, Linux, \textbf{for each CPU}, uses the so called \texttt{ksoftirqd/n} kernel thread (where n is the logical number of the CPU) to manage softirqs array executing bottom halves asynchronously. Once awaken, that thread, running the \texttt{ksoftirqd()} function, checks softirq bit mask for pending softirqs inspecting the per-CPU field \texttt{\_\_softirq\_pending}. If there are no softirqs pending, the function puts the current thread in the \texttt{TASK\_INTERRUPTIBLE} state and invokes then the \texttt{cond\_resched()} function to perform a process switch; otherwise, the thread runs the softIRQ handler, running \texttt{do\_softirq()}.

Be careful that the top half routine can set the bit mask telling that a \texttt{ksoftirqd/x} awaken on a CPU-core x will not process the handler associated with a given softIRQ; in this way we can \textbf{create affinity between SoftIRQs and CPU-cores in order to exploit NUMA machines}. Is also possible to set bit mask  in order to build affinity on group of CPU for load balancing; \textbf{in other word is possible a multithread execution of bottom half tasks}.

\subsubsection{tasklet}

When we use softirqs not necessarily we queue bottom half task, so this setup can be even more responsive. However the queuing concept is still there for on demand usage, if required. 

Tasklets are built on top of two softirqs named \texttt{HI\_SOFTIRQ} and \texttt{TASKLET\_SOFTIRQ}. Several tasklets may be associated with the same softirq, each tasklet carrying its own function. There is no real difference between the two softirqs, except that \texttt{do\_softirq()} executes \texttt{HI\_SOFTIRQ}â€™s tasklets before \texttt{TASKLET\_SOFTIRQ}â€™s tasklets.

Tasklets and high-priority tasklets are stored in the \texttt{tasklet\_vec} and \texttt{tasklet\_hi\_vec}
arrays respectively and both of them include \texttt{NR\_CPUS} elements which are \textbf{list} of \textbf{tasklet descriptors} which are a data structure of type \texttt{tasklet\_struct}

Each \texttt{tasklet\_struct} has many fields including the \texttt{state} field which represents the status of current tasklet and can assume two value: \texttt{TASKLET\_STATE\_SCHED} (tasklet pending), \texttt{TASKLET\_STATE\_RUN} (tasklet is running). Using this field is possible to keep track of a specific bottom half task, related to the execution of a specific function internal to the kernel.

Linux offers many APIs to manage tasklets: for instance to allocate a new \texttt{tasklet\_struct} data structure and
initialize is need to invoke \texttt{tasklet\_init()}; this function receives as its parameters the address of the tasklet descriptor , the address of your tasklet function (\texttt{void (*func)}), and its optional integer argument (\texttt{unsigned long}) for data.

The tasklet may be selectively disabled by invoking either \texttt{tasklet\_disable\_nosync()} or \texttt{tasklet\_disable()}. Both functions increase the count field of the tasklet descriptor, but the latter function does not return until an already running instance of the tasklet function has terminated. To reenable the tasklet, use \texttt{tasklet\_enable()}. To activate the tasklet, you should invoke either the \texttt{tasklet\_schedule()} function or the \texttt{tasklet\_hi\_schedule()} function, according to the priority that you require for the tasklet. When a tasklet is enabled its descriptor is added at the beginning of the list pointed to by \texttt{tasklet\_vec[n]} or \texttt{tasklet\_hi\_vec[n]}, where n denotes the logical number of the local CPU; then \texttt{HI\_SOFTIRQ} and \texttt{TASKLET\_SOFTIRQ} softirq are enabled (\textbf{all these operation are executed with local interrupts disabled})

Remember that tasklets can be instantiated by exploiting also the following macros defined 
in include \texttt{include/linux/interrupt.h}: 
\begin{itemize}
\item \texttt{DECLARE\_TASKLET(tasklet, function, data)}
\item \texttt{DECLARE\_TASKLET\_DISABLED(tasklet, function, data)}
\end{itemize}

Finally to execute tasklet associated with the \texttt{HI\_SOFTIRQ} softirq we run \texttt{tasklet\_hi\_action()}, while for those associated with \texttt{TASKLET\_SOFTIRQ} we use \texttt{tasklet\_action()}. 

\textbf{Observer that if the tasklet has already been scheduled on a different CPU-core, it will not be moved to another CPU-core if it's still pending (generic softirqs can instead be processed by different CPU-cores)}

\textbf{Tasklets run in interrupt context (see below)}

\subsubsection{Work queue}

The work queues have been introduced in Linux 2.6 and replace a similar construct called "task queue" used in Linux 2.4. Also the work queues are used to allow kernel functions to be activated and later executed by special kernel threads called \textit{worker threads}. 

However there is one important difference with softirq and tasklet: \textbf{deferrable functions (that is softirqs and tasklet) run in interrupt context while functions in work queues run in process context}. \textit{Running in process context is the only way to execute functions that can block (for instance, functions that need to access some block of data on disk) because no process switch can take place in interrupt context.}

\textbf{Observer that interrupts are enabled while the work queues are being run (except if the same work to be done disables them)}

The main data structure associated with a work queue is a descriptor called \texttt{workqueue\_struct}, which contains \textbf{many} fields including an array of \texttt{NR\_CPUS} elements (the maximum number of CPUs in the system.) Each element is a descriptor of type \texttt{cpu\_workqueue\_struct}, which contains a \texttt{worklist} field which is the head of a doubly linked list collecting the pending functions of the work queue. Every pending function is represented by a \texttt{work\_struct} data structure.

The \texttt{create\_workqueue("foo")} function receives as its parameter a string of characters and returns the address of a \texttt{workqueue\_struct} descriptor. The function also creates n worker threads (where n is the number of CPUs effectively present in the system), named after the string passed to the function: \texttt{foo/0, foo/1}, and so on. The \texttt{create\_singlethread\_workqueue()} function is similar, but it creates just one worker thread, no matter what the number of CPUs in the system is. To destroy a work queue the kernel invokes the \texttt{destroy\_workqueue()} function, which receives as its parameter a pointer to a \texttt{workqueue\_struct} array.

Another very important API is \texttt{queue\_work()} which inserts a function (already packaged inside a \texttt{work\_struct} descriptor) in a work queue; it receives a pointer \texttt{wq} to the \texttt{workqueue\_struct} descriptor and a pointer work to the \texttt{work\_struct} descriptor.

The \texttt{queue\_delayed\_work()} function is nearly identical to \texttt{queue\_work()}, except that it
receives a third parameter representing a time delay in system ticks and it is used to ensure a minimum delay before the execution of the pending function.

\texttt{cancel\_delayed\_work()} cancels a previously scheduled work queue function.
The \texttt{flush\_workqueue()} function receives a \texttt{workqueue\_struct} descriptor address and blocks the calling process until all functions that are pending in the work queue terminate. 


\subsubsection{The predefined work queue} 

The kernel offers a predefined work queue called \textit{events}, which can be freely used by every kernel developer. The predefined work queue is nothing more than a standard work queue that may include functions of different kernel layers and I/O drivers.

To make use of the predefined work queue, the kernel offers some APIs including \texttt{schedule\_work(struct work\_struct *work)} and \texttt{schedule\_work\_on(int cpu, struct work\_struct *work)}.

\subsection{\texttt{container\_of}}

The macro \texttt{container\_of(ptr, type, member)} takes, as you can see, three arguments: a pointer to the member of a data structure, the name of the type of the data structure, and the name of the member the pointer refers to. The macro yields the address of the container structure which accommodates the specified member.

\subsection{Timers}

On the x86 architecture, the kernel must explicitly interact with several kinds of clock circuits which are used both to keep track of the current time of day and to make precise time measurements. \textbf{The timer circuits are programmed by the kernel, so that they issue interrupts at a fixed, predefined frequency; such periodic interrupts are crucial for implementing the software timers used by the kernel and the user programs}. 

\begin{description}
\item[Time Stamp Counter (TSC)] It is a counter accessible through the 64-bit \textit{Time Stamp Counter} (\textbf{TSC}) register, which can be read using \texttt{rdtsc} assembly language instruction. \textbf{It represents a counter that is increased at each clock signal.} It is used by Linux to determine the clock signal frequency while initializing the system; that task is accomplished using \texttt{calibrate\_tsc()}.
\item[High Precision Event Timer (HPET)] The HPET represents a very powerful chip which provides up to \textbf{eight 32-bit or 64-bit independent counters exploitable by kernel}. Each counter is driven by its own clock signal, whose frequency must be at least 10 MHz and, therefore, the counter is increased at least once in \textbf{100 nanoseconds}. Any counter is associated with at most 32 timers, each of which is composed by a \textit{comparator} and a \textit{match register}. \textbf{The comparator is a circuit that checks the value in the counter against the value in the match register, and raises a hardware interrupt if a match is found. Some of the timers can be enabled to generate a periodic interrupt.}
\item[LAPIC] The Local APIC Timer (LAPIC-T) represents another time-measuring device. This timer has a counter of \textbf{32 bits long} used to store the number of of ticks that must elapse before the interrupt is issued; therefore, the local timer can be programmed to issue interrupts at very low frequencies. \textbf{Observe that local APIC timer sends an interrupt only to its processor}. The APICâ€™s timer is based on the bus clock signal and can be can be programmed in such a way to decrease the timer counter every 1, 2, 4, 8, 16, 32, 64, or 128 bus clock signals.
\end{description}

\subsubsection{The timer interrupt handler}

As said these timer circuits issues special interrupts called \textbf{timer interrupt}, which notifies the kernel
that one more time interval has elapsed. Interrupts can both involve a specific CPU (CPU local timer interrupt signals timekeeping activities related to the local CPU, such as monitoring how long the current process has been running and updating the resource usage statistics) or signal activities not related to a specific CPU, such as handling of software timers and keeping the system time up-to-date.

\subsection{Preemption}

As a general definition a kernel is \textit{preemptive} \textbf{if a process switch may occur while the replaced process is executing a kernel function, that is, while it runs in Kernel Mode}. 

\textbf{Kernel pre-emption is disabled when the \texttt{preempt\_count} field in the \texttt{thread\_info} descriptor referenced by the \texttt{current\_thread\_info()} macro \textbf{is greater than zero}}. Linux provides several APIs to manage kernel pre-emption:
\begin{description}
\item[\texttt{preempt\_count()}] Return the \texttt{preempt\_count} field in the \texttt{thread\_info} descriptor.
\item[\texttt{preempt\_disable()}] Increases by one the value of the preemption counter.
\item[\texttt{preempt\_enable\_no\_resched()}] Decreases by one the value of the preemption counter.
\item[\texttt{preempt\_enable()}] Decreases by one the value of the preemption counter, and invokes \texttt{preempt\_schedule()} if the \texttt{TIF\_NEED\_RESCHED} flag in the \texttt{thread\_info} descriptor is set
\end{description}

But there are other two very important API and they are related to \textbf{per-CPU variables}. 

Remember that a \textbf{\textit{per-CPU variables} is an array of data structures, one element per each CPU in the system}. A CPU should not access the elements of the array corresponding to the other CPUs; on the other hand, it can freely read and modify its own element without fear of race conditions, because it is the only CPU entitled to do so. \textbf{While per-CPU variables provide protection against concurrent accesses from several CPUs}, they do \textbf{not provide protection against accesses from asynchronous functions} (interrupt handlers and deferrable functions like tasklet and softirqs). \textbf{Therefore per-CPU variables are prone to race conditions caused by kernel pre-emption, both in uniprocessor and multiprocessor systems}. \textbf{As a general rule, a kernel control path should access a per-CPU variable with kernel preemption disabled.}. We have some API:

\begin{description}
\item[\texttt{get\_cpu\_var(name)}] Disables kernel preemption, then selects the local CPUâ€™s element of the per-CPU array name
\item[\texttt{put\_cpu\_var(name)}] Enables kernel preemption.
\end{description}


\newpage
\section{Slide \textit{'trap-interrupt-architecture'}}

\subsubsection{IPI}

An \textbf{Inter-processor Interrupt} (\textbf{IPI}) \textbf{allow a CPU to send interrupt signals to any other CPU in the system}. An interprocessor interrupt is delivered directly as a message on the bus that connects the \textbf{local APIC} (\textit{Advanced Programmable Interrupt Controller}) of all CPUs. \textbf{An IPI is a synchronous event from the sender CPU-core point of view while it is an asynchronous one from recipient CPU-core point of view.}
Classically, at least two priority levels are admitted:
\begin{description}
\item[High] Leads to immediate processing of the IPI at the recipient (\textit{a single IPI is accepted and stands out at any point in time})
\item[Low] Low priority generally leads to queue the requests and process them via sequentialization
\end{description}

Each local APIC of each CPU of the system has several 32-bit registers (which can be used for posting IPI requests in the system), an internal clock; a local timer device; and two additional IRQ lines, \texttt{LINT0} and \texttt{LINT1}, reserved for local APIC interrupts. All local APICs are connected to an external I/O APIC, giving rise to a multi-APIC system.

The I/O APIC consists of a set of 24 IRQ lines, a 24-entry Interrupt Redirection Table, programmable registers, and a message unit for sending and receiving APIC messages over the APIC bus.

The multi-APIC system allows CPUs to generate interprocessor interrupts. When a CPU wishes to send an interrupt to another CPU, it stores the interrupt vector and the identifier of the target's local APIC in the \textbf{Interrupt Command Register} (\textbf{ICR}) of its own local APIC. A message is then sent via the APIC bus to the target's local APIC, which therefore issues a corresponding interrupt to its own CPU. 

Interrupt requests coming from external hardware devices can be distributed among the available CPUs in two ways:
\begin{description}
\item[Static distribution] The interrupt is delivered to one specific CPU, to a subset of CPUs, or to all CPUs at once (broadcast mode).
\item[Dynamic distribution] The IRQ signal is delivered to the local APIC of the processor that is executing the process with the lowest priority. However interrupts are distributed in a round-robin fashion among CPUs with the same task priority, adopting a technique called \textit{arbitration}.
\end{description}



Linux makes use of three kinds of interprocessor interrupts:

\begin{description}
\item[\texttt{CALL\_FUNCTION\_VECTOR}] It is used to force all CPUs to run a function passed by the sender. The corresponding interrupt handler is named \texttt{call\_function\_interrupt()}. 
\item[\texttt{INVALIDATE\_TLB\_VECTOR}] It is used to force all CPUs to invalidate their TLB. The corresponding interrupt handler is named \texttt{invalidate\_interrupt()}. 
\item[\texttt{RESCHEDULE\_VECTOR}] When a CPU receives this type of interrupt, the corresponding handler, named \texttt{reschedule\_interrupt()}, limits itself to acknowledging the interrupt. Rescheduling is done automatically when returning from the interrupt.
\end{description}

Thanks to the following group of functions, issuing interprocessor interrupts (IPIs) becomes an easy task:

\begin{description}
\item[\texttt{send\_IPI\_all()}] Sends an IPI to all CPUs (including the sender).
\item[\texttt{send\_IPI\_allbutself()}] Sends an IPI to all CPUs except the sender.
\item[\texttt{send\_IPI\_self()}] Sends an IPI to the sender CPU.
\item[\texttt{send\_IPI\_mask()}] Sends an IPI to a group of CPUs specified by a bit mask.
\end{description}


\subsection{IST}

In long mode, each IDT's entry have a very important field (3 bit) called \textbf{IST} (\textit{Interrupt Stack Table}) which is completely absent on i386. \textbf{The IST is used to automatically switch to a new stack for events such trap or interrupts.} This mechanism unconditionally switches stacks when it is enabled and it can be enabled on an individual interrupt-vector basis using IST field in the IDT entry. This means that some interrupt vectors can use the legacy mechanism for stack-switch and others can use the IST mechanism.

\textbf{There can be up to 7 IST entries per CPU and they are located on the Task State Segment (TSS)}. The IST entries in the TSS point to dedicated stacks; each stack can be a different size. \textbf{When an interrupt occurs and the hardware loads such a descriptor, the hardware automatically sets the new stack pointer based on the IST value, then invokes the interrupt handler}.  

There are many stack provided by IST mechanism, one of them is the \texttt{DOUBLEFAULT\_STACK}, which is used for the \textbf{Double Fault Exception} (\#\texttt{DF}), invoked when handling one exception causes another exception. Using a separate stack allows the kernel to recover from it well enough in many cases.

\subsection{Exception handling}

Exception handlers have a standard structure consisting of three steps:
\begin{enumerate}
\item Save the contents of most registers (that is a CPU snapshot) in the Kernel Mode stack (this part is coded in assembly language);
\item Handle the exception by means of a high-level C function.
\item Exit from the handler by means of the \texttt{ret\_from\_exception()} function.
\end{enumerate}

When a CPU receives an interrupt, it starts executing the code at the address found in the corresponding entry of the IDT. Registers is the first task of the interrupt handler and a pointer to the \texttt{pt\_regs struct} (\texttt{include/asm-i386/ptrace.h}) is used to save register.

After the interrupt or exception is processed, \texttt{iret} assembly instruction is used to return program control from an exception or interrupt handler to a program or procedure that was interrupted by an exception. In Protected Mode, the action of the \texttt{iret} instruction depends on the settings of the \texttt{NT} (nested task) flag:
If the NT flag (EFLAGS register) is cleared, the \texttt{iret} instruction performs a far return from the interrupt procedure, without a task switch. Otherwise \texttt{iret} instruction performs a task switch (return) from a nested task


\end{document}