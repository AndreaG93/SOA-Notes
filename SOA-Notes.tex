\documentclass[10pt,a4paper]{article}


\usepackage[a4paper, total={8in, 10in}]{geometry}


\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\begin{document}

\section{Slide \textit{'hardware-insights'}}

\section{OOO Pipeline execution and imprecise exceptions}

When processor executes instructions in an order governed by the availability of input data and execution units, rather than by their original order in a program, we are adopting an \textbf{out-of-order execution paradigm}; \textbf{in other words different instructions can surpass each other depending on data or micro-controller availability.} We distinguish two events when we use this kind of paradigm: the \textbf{emission}, that is the action of injecting instructions into the pipeline; the \textbf{retire}, that is the action of committing instructions and making their side effects visible in terms of ISA exposed architectural resources

\textbf{Is important to recall that out-of-order completion must preserve exception behaviour in the sense that exactly those exceptions that would arise if the program were executed in strict program order actually do arise}. However, when we use OOO execution paradigm a processor may generate the so called \textbf{imprecise exceptions}. \textbf{An exception is imprecise if the processor state  when  an  exception  is  raised  does  not  look  exactly  as  if  the instructions were executed sequentially in strict program order}. In other words imprecise exceptions can occur because when:

\begin{itemize}
\item The pipeline may have already completed instructions that are later in program order than the instruction causing the exception.
\item The pipeline may have not yet completed some instructions that are earlier in program order than the instruction causing the exception.
\end{itemize}

\textbf{Recall that any instruction may change the micro-architectural state, although finally not committing its actions onto ISA exposed resources.} Since the the pipeline may have not yet completed the execution of instructions preceding the offending one, hardware status can been already changed an this fact can be exploited by several attacks (like \textbf{Meltdown}).

\section{Tomasulo algorithm}

Tomasulo’s algorithm is a computer architecture hardware algorithm for dynamic scheduling of instructions that allows out-of-order execution and enables more efficient use of multiple execution units. Suppose two operation A and B such that A precedes B in program order, that algorithm permit to resolve three hazard:

\begin{description}
\item[RAW (Read After Write)] B reads a datum before A writes it.
\item[WAW (Write After Write)] B writes a datum before A writes the same datum.

\item[WAR (Write After Read)] B writes a datum before A reads the same datum.
\end{description}

\textbf{RAW hazards are avoided by executing an instruction only when its operands are available, while WAR and WAW hazards are eliminated by \textit{register renaming}}

\subsection{UMA}

When we have a \textbf{single main memory} that has a \textit{symmetric relationship to all processors and a uniform access time from any processor}, these multiprocessors are most often called \textit{symmetric shared-memory multiprocessors} (\textbf{SMPs}), and this style of architecture is sometimes called \textit{uniform memory access} (\textbf{UMA}): in fact, \textbf{all processors have a uniform latency from memory}. \textbf{The term shared memory refers to the fact that the address space is shared; that is, the same physical address on two processors refers to the same location in memory.} In this architecture all CPUs can have one or more level of cache. However this architecture is obliviously \textbf{not} scalable when the number of CPUs grows.

\subsection{NUMA}

When we have a distributed memory, a multiprocessor architecture is usually called \textit{distributed shared-memory} (\textbf{DSM}). When we use this kind of system, we have two benefits:
\begin{itemize}
\item A cost-effective way to scale the memory bandwidth if most of the accesses are to the local memory in the node.
\item Reduces the latency for accesses to the local memory by a CPU.
\end{itemize}

The key disadvantages for that architecture is that communicating data between processors becomes more complex, \textbf{and that it requires more effort in the software to take advantage of the increased memory bandwidth afforded by distributed memories.}

The DSM multiprocessors are also called \textbf{NUMAs} (\textit{non-uniform memory access}), \textbf{since the access time depends on the location of a data word in memory.} In fact, when a CPU wants to access to an item stored into his node, performing a \textit{local access} involving inner private/shared caches and controllers, access latency is very low. However when a CPU wants to access to an item stored on another node, performing a \textit{remote accesses} involving remote controllers and caches, latency can be very high respect to previous case.

\section{The problem of cache coherence}

Unfortunately, \textbf{the view of memory held by different processors is through their individual caches}, which, without any additional precautions, could end up seeing different values of a same shared data (\textbf{cache coherence} problem).

By definition, \textbf{coherence defines what values can be returned by a read} (a \textbf{cache coherence protocols} defines how to maintain coherence) while \textbf{consistency determines when a written value will be returned by a read} (a \textbf{memory consistency protocol} defines when written value must be seen by a reader).

A memory system is coherent if:
\begin{enumerate}
\item A read from location $X$, previously written by a processor, returns the last written value if no other processor carried out writes on $X$ in the meanwhile. \textbf{This property preserve program order that is the causal consistency along program order.}

\item A read by a processor to location $X$ that follows a write by another processor to $X$ returns the written value if the read and write are sufficiently separated in time and no other writes to $X$ occur between the two accesses. \textbf{This property assure that a processor couldn't continuously
read an old data value (Avoidance of staleness).}

\item \textbf{Writes to the same location are serialized}; that is, two writes to the same location by any two processors are seen in the same order by all processors.
\end{enumerate}

The choice and the design of a coherence protocol depends on many factors including: overhead, latency, cache policies, interconnection topology and so on.
\textbf{However the Key to implementing a cache coherence protocol is tracking the state of any copy of a data block}. There are two classes of protocol which define when update aforementioned copies:
\begin{description}
\item[Update protocol] When we use this type of protocol, also called \textit{write update} or \textit{write broadcast}, when a core writes to a block, it updates all other copies (\textbf{it consumes considerably more bandwidth}).

\item[Invalidate protocol] When we use this type of protocol, a processor has \textbf{exclusive access} to a data item before it writes that item; moreover that CPU invalidates other copies on a write that is no other readable or writeable copies of an item exist when the write occurs. \textbf{It is the most common protocol, but suffer of some latency.}

\end{description}

\section{Snooping protocol}

The key to implementing an invalidate protocol is the use of the bus, or another broadcast medium, called \textit{network} to perform invalidates and to issue "transactions" on the state of cache blocks.

To perform any operation, the processor simply \textbf{acquires} bus access and broadcasts the address to be invalidated on the bus. All processors continuously \textbf{snoop} on the bus, watching the addresses. The processors check whether the address on the bus is in their cache. If so, the corresponding data in the cache are invalidated. \textbf{A state transition cannot occur unless the broadcast medium is acquired by the source controller and are carried out atomically with a distribute fashions thanks to \textit{serialization} over the broadcast medium}.

When we perform a read, we also need to locate a data item when a cache miss occurs. In a \textbf{write-through cache}, it is easy to find the recent value of a data item, \textit{since all written data are always sent to the memory, from which the most recent value of a data item can always be fetched} (using write through simplifies the implementation of cache coherence). For a \textbf{write-back cache}, the problem of finding the most recent data value is
harder, since the most recent value of a data item can be in a cache rather than in memory (the CPU must get data from another cache)


\newpage
\section{Slide \textit{'kernel-level-memory-management'}}

\subsection{Page Descriptor}

In Linux, \textbf{state information of a page frame is kept in a page descriptor} of type \texttt{struct page} (or struct \texttt{mem\_map\_t}), and all page descriptors, which are 32 byte long, are stored in an array called \texttt{mem\_map} (the space required by it is slightly less than 1\% of the whole RAM). These data structures are defined into \texttt{include/linux/mm.h}. 

The \texttt{virt\_to\_page(addr)} macro yields the address of the page descriptor associated with the linear address \texttt{addr}. 

\texttt{struct page} has many fields but the most important are:
\begin{description}

\item[\texttt{atomic\_t \_count}] It represent a usage reference counter for the page. If it is set to -1, the corresponding page frame is free and can be assigned to any process or to the kernel itself. If it is set to a value greater than or equal to 0, the page frame is assigned to one or more processes or is used to store some kernel data structures. The \texttt{page\_count()} function returns the value of the \texttt{\_count} field increased by one, that is, the number of users of the page. This field is managed via atomic updates, such as with \texttt{LOCK} directives.
\item[\texttt{struct list\_head lru}] Contains pointers to the least recently used doubly linked list of pages. 
\item[\texttt{unsigned long flags}] Array of flags used to describe the status of current page frame (but also encodes the zone number to which the page frame belongs). There are up to 32 flags and Linux kernel defines many macros to manipulate them. Some flags are:
\begin{description}
\item[\texttt{PG\_locked}] The page is locked; for instance, it is involved in a disk I/O operation.
\item[\texttt{PG\_dirty}] The page has been modified.
\item[\texttt{PG\_reserved}] The page frame is reserved for kernel code or is unusable.
\end{description}

\end{description}

\subsection{Free list}

Linux uses \textbf{free list} to manage memory allocation. \textbf{It operates by connecting unallocated regions of memory together in a linked list, using the first word of each unallocated region as a pointer to the next.} 

Free lists make the allocation and deallocation operations very simple. \textbf{To free a region, one would just link it to the free list. To allocate a region, one would simply remove a single region from the end of the free list and use it}. 

\subsection{NUMA}

Is extremely important to remember that Linux 2.6 supports the \textit{Non-Uniform Memory Access} (\textbf{NUMA}) model, \textbf{in which the access times for different memory locations from a given CPU may vary} and, according to that architecture, physical memory is partitioned in several \textbf{nodes}. The time needed by a given CPU to access pages within a single node is the same. However, this time might not be the same for two different CPUs. 

\subsection{NUMA Node Descriptor}

\textbf{Be careful that Linux splits physical memory inside each node into several zones. We have 3 free lists of frames, depending on the frame positioning within available zones (defined in \texttt{include/linux/mmzone.h}) which are:}

\begin{description}
\item[\texttt{ZONE\_DMA}] Contains page frames of memory below 16 MB, that is page frames that can be used by old ISA-based devices (\textit{Direct Memory Access} (DMA) processors).
\item[\texttt{ZONE\_NORNMAL}] Contains page frames of memory at and above 16 MB and below 896 MB (direct mapped by the kernel).
\item[\texttt{ZONE\_HIGHMEM}] Contains page frames of memory at and above 896 MB (only page cache and user).
\end{description}

To represent a NUMA node, Linux uses a descriptor of type \texttt{struct pg\_data\_t}. All node descriptors are stored in a singly linked list, whose first
element is pointed to by the \texttt{pgdat\_list} variable. Be careful to the fact that this data structure is used by Linux kernel even if the architecture is based on \textit{Uniform Memory Access} (\textbf{UMA}): in fact Linux makes use of a single node that includes all system physical memory. Thus, the \texttt{pgdat\_list} variable points to a list consisting of a single element (node 0) stored in the \texttt{contig\_page\_data} variable.

Remember that free lists information is kept within the \texttt{struct pg\_data\_t} data structure. In fact the most important fields of \texttt{struct pg\_data\_t} are:

\begin{description}
\item[\texttt{struct page *node\_mem\_map}] Array of page descriptors of the node
\item[\texttt{struct zone [] node\_zones}] Array of zone descriptors of the node
\end{description}

\subsection{Zone Descriptor}

Obliviously each memory zone has its own descriptor of type \texttt{struct zone} and many fields of this data structure are used for page frame reclaiming. However, most important fields are:

\begin{description}
\item[\texttt{struct page * zone\_mem\_map}] Pointer to first page descriptor of the zone.
\item[\texttt{spinlock\_t lock}] Spin lock protecting the descriptor.
\item[\texttt{struct free\_area [] free\_area}] Identifies the blocks of free page frames in the zone
\end{description}

In summary, Linux has links to the memory node and to the zone inside the node that includes the corresponding page frame of type \texttt{struct page}.

\subsection{Buddy allocator}

The technique adopted by Linux to solve the external fragmentation problem is based on the well-known \textbf{buddy system} algorithm. All free page frames are grouped into 11 lists of blocks that contain groups of 1, 2, 4, 8, 16, 32, 64, 128, 256, 512, and 1024 contiguous page frames, respectively. The largest request of 1024 page frames corresponds to a chunk of 4 MB of contiguous RAM. We use the term \texttt{order} \textbf{to indicate the logarithmic size of a block}.

Assume there is a request for a group of 256 contiguous page frames (i.e., one megabyte). The algorithm checks first to see whether a free block in the 256-page-frame list exists. If there is no such block, the algorithm looks for the next larger block — a free block in the 512-page-frame list. If such a block exists, the kernel allocates 256 of the 512 page frames to satisfy the request and inserts the remaining 256 page frames into the list of free 256-page-frame blocks. If there is no free 512-page block, the kernel then looks for the next larger block (i.e., a free 1024-page-frame block). If
such a block exists, it allocates 256 of the 1024 page frames to satisfy the request, inserts the first 512 of the remaining 768 page frames into the list of free 512-page-frame blocks, and inserts the last 256 page frames into the list of free 256-page-frame blocks. If the list of 1024-page-frame blocks is empty, the algorithm gives up and signals an error condition.

\textbf{Linux 2.6 uses a different buddy system for each zone}. Thus, in the x86 architecture, there are \textbf{3 buddy systems}: the first handles the page frames suitable for ISA DMA, the second handles the "normal" page frames, and the third handles the high memory page frames. Each buddy system relies on the following main data structures:
\begin{itemize}
\item \textbf{The \texttt{mem\_map} array where all page descriptors are stored}. Actually, each zone is concerned with a subset of the \texttt{mem\_map} elements. The first element in the subset and its number of elements are specified, respectively, by the \texttt{zone\_mem\_map} and \texttt{size} fields of the zone descriptor.

\item The array consisting of eleven elements of type \texttt{struct free\_area}, one element for each group size. As we said the array is stored in the \texttt{free\_area} field of the zone descriptor.
\end{itemize}

Let us consider the $k^{th}$ element of the \texttt{struct free\_area} array in the zone descriptor, which identifies all the free blocks of size $2^k$. In this data structure there is a pointer of type struct \texttt{list\_head}  which is is the head of a doubly linked circular list that collects the page descriptors associated with the free blocks of $2^k$ pages. Besides the head of the list, the $k^{th}$ element of the \texttt{struct free\_area} array includes also the field \texttt{nr\_free}, which specifies the number of free blocks of size $2^k$ pages, and a pointer to a bitmap that keeps fragmentation information.  

Recall that spin locks are used to manage \texttt{mem\_map} AND \texttt{struct free\_area} array.

\textbf{To achieve better performance a little number of page frames are kept in cache to quickly satisfy the allocation requests for single page frames}.

\subsection{API}

Page frames can be requested by using some different functions and macros (APIs) (they return \texttt{NULL} in case of failure, a linear address of the first allocated page in case of success) which prototype are stored into \texttt{\#include <linux/malloc.h>}. The most important are:

\begin{description}
\item[\texttt{get\_zeroed\_page(gfp\_mask)}] Function used to obtain a page frame filled with zeros.
\item[\texttt{\_\_get\_free\_page(gfp\_mask)}] Macro used to get a single page frame.
\item[\texttt{\_\_get\_free\_pages(gfp\_mask, order)}] Macro used to request $2^{order}$ contiguous page frames returning the linear address of the
first allocated page.
\item[\texttt{free\_page(addr)}] This macro releases the page frame having the linear address \texttt{addr}.

The parameter \texttt{gfp\_mask} is a group of flags that specify \textbf{how to look for free page frames} and they are extremely important when we require page frame allocation in different contexts including: 
\begin{description}
\item[Interrupt context] allocation is requested by an \textbf{interrupt handler} which uses above function with \texttt{GFP\_ATOMIC} flag (equivalent to \texttt{\_\_GFP\_HIGH}) \textbf{which means that the kernel is allowed to access the pool of reserved page frames: therefore the call cannot lead to sleep (that is no wait)}  An atomic request never blocks: if there are not enough free pages the allocation simply fails.

\item[Process context] allocation is caused by a system call using \texttt{GFP\_KERNEL} or \texttt{GFP\_USER} (both equivalent to \texttt{\_\_GFP\_WAIT | \_\_GFP\_IO | \_\_GFP\_FS}) according to which kernel is allowed to block the current process waiting for free page frames (\texttt{\_\_GFP\_WAIT}) and to perform I/O transfers on low memory pages in order to free page frames (\texttt{\_\_GFP\_IO}): therefore the call can lead to sleep.
\end{description}

\end{description}



\section{TLB operation}

Besides general-purpose hardware caches, x86 processors include a cache called \textit{Translation Lookaside Buffers} (\textbf{TLB}) to speed up linear address translation.

When a linear address is used for the first time, the corresponding physical address is computed through slow accesses to the Page Tables in RAM. The physical address is then stored in a TLB entry so that further references to the same linear address can be quickly translated.

In a multiprocessor system, \textbf{each CPU has its own TLB, called the \textbf{local TLB} of the CPU}. Contrary to the hardware cache, the corresponding entries of the TLB need \textbf{not} be synchronized, because processes running on the existing CPUs may associate the same linear address with different physical ones.

\textbf{When the \texttt{cr3} control register of a CPU is modified, the hardware automatically invalidates all entries of the local TLB, because a new set of page tables is in use (page table changes).} \textbf{However changes inside the current page table are not automatically reflected within the TLB.}

Fortunately, Linux offers several TLB flush methods that should be applied appropriately, depending on the type of page table change:
\begin{description}
\item[\texttt{flush\_tlb\_all}] This flushes the \textbf{entire TLB on all processors} running in the system, which makes it the most expensive TLB flush operation. It is used when we have made changes into the kernel page table entries. \textbf{After it completes, all modifications to the page tables will be visible globally to all processors.}

\item[\texttt{flush\_tlb\_mm(struct mm\_struct *mm)}] Flushes all TLB entries of the non-global pages owned by a given process that is all entries related to the userspace portion for the requested \texttt{mm} context. Is used when forking a new process.

\item[\texttt{flush\_tlb\_range}] Flushes the TLB entries corresponding to a linear address interval of a given process and is used when releasing a linear address interval of a process (when \texttt{mremap()} or \texttt{mprotect()} is used).

\item[\texttt{flush\_tlb\_page}] Flushes the TLB of a single Page Table entry of a given process and is used when handling a page fault.

\item[\texttt{flush\_tlb\_pgtables}] Flushes the TLB entries of a given contiguous subset of page tables of a given process and is called when a region is being unmapped and the page directory entries are being reclaimed 
\end{description}

Despite the rich set of TLB methods offered by the generic Linux kernel, every microprocessor usually offers a far more restricted set of TLB-invalidating assembly language instructions. \textbf{Intel microprocessors offers only two TLB-invalidating techniques: the automatic flush of all TLB entries when a value is loaded into the cr3 register and the \texttt{invlpg} assembly language instruction which invalidates a single TLB entry mapping a given linear address.}

The architecture-independent TLB-invalidating methods are extended quite simply to multiprocessor systems. \textbf{The function running on a CPU sends an Interprocessor Interrupt to the other CPUs that forces them to execute the proper TLB-invalidating function} (\textbf{expensive} operation (\textit{direct cost}) due to latency for cross-CPU coordination in case of global TLB flushes).

Remember that flush a TLB has the direct cost of the latency of the firmware level protocol for TLB entries invalidation (selective vs non-selective). Recall that flush TLB lead to \textbf{indirect cost} of refilling TLB entries and the latency experimented by MMU firmware upon misses in the translation process of virtual to physical addresses.

\subsubsection{When flush TLB?}

As a general rule,\textbf{ any process switch implies changing the set of active page tables and therefore local TLB entries relative to the old page tables must be flushed}; this is done automatically when the kernel writes the address of the new Page Global Directory into the \texttt{cr3} control register.

Besides \textbf{process switches}, there are other cases in which the kernel needs to flush some entries in a TLB. For instance, when the kernel assigns a page frame to a User Mode process and stores its physical address into a Page Table entry, it must flush any local TLB entry that refers to the corresponding linear address (virtual addresses accessible \textbf{locally} in time-sharing concurrency). On multiprocessor systems, the kernel also must flush the same TLB entry on the CPUs that are using the same set of page tables, if any (virtual addresses accessible \textbf{globally}  by every CPU/core in real-time-concurrency).

Kernel-page mapping has a \textit{global} nature, therefore when we use \texttt{vmalloc()} / \texttt{vfree()} on a specific CPU, all the other must observer mapping updates and TLB flush is necessary. 

\section{Part 1}

Since a data dependence can limit the amount of instruction-level parallelism
we can exploit, a major focus of this chapter is overcoming these limitations. A
dependence can be overcome in two different ways: maintaining the dependence
but avoiding a hazard, and eliminating a dependence by transforming the code.
Scheduling the code is the primary method used to avoid a hazard without alter-
ing a dependence, and such scheduling can be done both by the compiler and by
the hardware.












Top and Bottom Halves
One of the main problems with interrupt handling is how to perform lengthy tasks
within a handler. Often a substantial amount of work must be done in response to a
device interrupt, but interrupt handlers need to finish up quickly and not keep inter-
rupts blocked for long. These two needs (work and speed) conflict with each other,
leaving the driver writer in a bit of a bind.
Linux (along with many other systems) resolves this problem by splitting the inter-
rupt handler into two halves. The so-called top half is the routine that actually
responds to the interrupt—the one you register with request\_irq. The bottom half is a
routine that is scheduled by the top half to be executed later, at a safer time. The big
difference between the top-half handler and the bottom half is that all interrupts are
enabled during execution of the bottom half—that’s why it runs at a safer time. In
the typical scenario, the top half saves device data to a device-specific buffer, sched-
ules its bottom half, and exits: this operation is very fast. The bottom half then per-
forms whatever other work is required, such as awakening processes, starting up
another I/O operation, and so on. This setup permits the top half to service a new
interrupt while the bottom half is still working.


\textbf{ksoftirqd} is a per-cpu kernel thread which runs in background (as a deamon): is triggered to handle the software interrupts in process context.



 NR\_CPUS structures (the default value for this
macro is 32; it denotes the maximum number of CPUs in the system



\end{document}