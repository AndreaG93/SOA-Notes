\documentclass[10pt,a4paper]{article}

\usepackage[a4paper, total={8in, 10in}]{geometry}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage[colorlinks=true,linkcolor=blue,urlcolor=black,bookmarksopen=true]{hyperref}
\usepackage{bookmark}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{soul}
\usepackage{quoting}

\quotingsetup{font=small}

\lstset{
language=C,
basicstyle=\small\ttfamily,			
keywordstyle=\color{blue},
commentstyle=\color{gray},			
stringstyle=\color{black},			
numbers=left,						
numberstyle=\tiny,					
stepnumber=1,						
breaklines=true						
}

\begin{document}

\tableofcontents
\newpage

\section{Slide \textit{'hardware-insights'}}

\subsection{OOO Pipeline execution and imprecise exceptions}

When processor executes instructions in an order governed by the availability of input data and execution units, rather than by their original order in a program, we are adopting an \textbf{out-of-order execution paradigm}; \textbf{in other words different instructions can surpass each other depending on data or micro-controller availability.} We distinguish two events when we use this kind of paradigm: the \textbf{emission}, that is the action of injecting instructions into the pipeline; the \textbf{retire}, that is the action of committing instructions and making their side effects visible in terms of ISA exposed architectural resources

\textbf{Is important to recall that out-of-order completion must preserve exception behaviour in the sense that exactly those exceptions that would arise if the program were executed in strict program order actually do arise}. However, when we use OOO execution paradigm a processor may generate the so called \textbf{imprecise exceptions}. \textbf{An exception is imprecise if the processor state  when  an  exception  is  raised  does  not  look  exactly  as  if  the instructions were executed sequentially in strict program order}. In other words imprecise exceptions can occur because when:

\begin{itemize}
\item The pipeline may have already completed instructions that are later in program order than the instruction causing the exception.
\item The pipeline may have not yet completed some instructions that are earlier in program order than the instruction causing the exception.
\end{itemize}

\textbf{Recall that any instruction may change the micro-architectural state, although finally not committing its actions onto ISA exposed resources.} Since the the pipeline may have not yet completed the execution of instructions preceding the offending one, hardware status can been already changed an this fact can be exploited by several attacks (like \textbf{Meltdown}).

\subsection{Tomasulo algorithm}

\textbf{Tomasulo’s algorithm is a computer architecture hardware algorithm for dynamic scheduling of instructions that allows out-of-order execution} and enables more efficient use of multiple execution units. Suppose two operation A and B such that A precedes B in program order, that algorithm permit to resolve three hazard:

\begin{description}
\item[RAW (Read After Write)] B reads a datum before A writes it.
\item[WAW (Write After Write)] B writes a datum before A writes the same datum.
\item[WAR (Write After Read)] B writes a datum before A reads the same datum.
\end{description}

\textbf{RAW hazards are avoided by executing an instruction only when its operands are available, while WAR and WAW hazards are eliminated by \textit{register renaming}}


According to Tomasulo's scheme, register renaming is provided by special buffers called \textbf{reservation stations}, which have three fields:

\begin{description}
\item[OP] the operations to be executed.
\item[Q1, Q2] the reservation stations that will produce the input for OP.
\item[V1, V2] the values of the source operands.
\end{description}

In other words, \textbf{reservation station are used to hold the operands of instructions, fetching and buffering an operand as soon as it is available, eliminating the need to get the operand from a register}. Results are passed directly to functional units from the reservation stations where they are buffered, rather than going through the registers. This bypassing is done with the \textbf{common data bus} (\textbf{CDB}) that  allows  all  units  waiting for an operand to be loaded simultaneously (\textit{In pipelines with multiple execution units and issuing multiple instructions per clock, more than one result bus will be needed}).

If one or more of the operands is not yet available the common data bus will be monitored. When an operand becomes available, it is placed into any reservation station awaiting it. When all the operands are available, the operation can be executed at the corresponding functional unit. By delaying instruction execution until the operands are available, RAW hazards are avoided. 

\textbf{When successive writes to a register overlap in execution, only the last one is actually used to update the register.} As instructions are issued, the register specifiers for pending operands are renamed to the names of the reservation station, which provides register renaming. \textbf{Each functional unit has a single reservation station. The functional unit begins processing when it is free and when all source operands needed for an instruction are real.}

A \textbf{re-order buffer} (ROB) is used TO acquire all the newly produced instruction results and keeps them uncommitted up to the point where the instruction can be committed in-order. 



\subsection{Dynamic Branch Prediction}

The hardware support for reducing the performance losses of branches is able to predict how they will behave. The behaviour of branches can be predicted both \textbf{statically} (\textit{to predict a branch as taken always: not very accurate}) at compile time and \textbf{dynamically} by the hardware at execution time, make predictions on the basis of profile information collected from earlier runs.

The simplest dynamic branch-prediction scheme is a \textbf{branch-prediction buffer} or \textbf{branch history table}. 

It is a small memory indexed by the lower portion of the address of the branch instruction. The memory contains a bit that says whether the branch was recently taken or not. The (speculative) execution flow follows the direction related to the prediction by the status bit, thus following the recent behaviour since recent past is expected to be representative of near future.

However this simple 1-bit prediction scheme has a performance shortcoming: \textbf{Even if a branch is almost always taken, we will likely predict incorrectly twice, rather
than once, when it is not taken, since the mis-prediction causes the prediction bit to be flipped}. 

To remedy this weakness, 2-bit prediction schemes are often used. \textbf{In a 2-bit scheme, a prediction must miss twice before it is changed} (that is require 2 subsequent prediction errors for inverting the prediction). Benchmarks on 2-bit branch-prediction buffer with 4096 entries results in a prediction accuracy ranging from over 99\% to 82\%, or a mis-prediction rate of 1\% to 18\%.

However this predictor schemes use only the recent behaviour of a single branch to predict the future behaviour of that branch. \textbf{It may be possible to improve the prediction accuracy if we also look at the recent behaviour of other branches rather than just the branch we are trying to predict.}

Branch predictors that use the behaviour of other branches to make a prediction are called \textbf{correlating predictors} or \textbf{two-level predictors}. In the general case an $(m,n)$ predictor uses the behaviour of the last $m$ branches; that is there are $2^m$ branch predictors, each of which is an $n$-bit predictor for a single branch.  The global history of the most recent m branches can be recorded in m-bit shift register, where each bit records whether the branch was taken or not taken. The branch-prediction buffer can then be indexed using a concatenation of the low-order bits from the branch address with the m-bit global history.

A 2-bit predictor with no global history is simply a (0,2) predictor. 

\textbf{Tournament predictors makes prediction using multiple predictors, usually one based on global information and one based on local information, and combining them with a selector}. Existing tournament predictors use a 2-bit saturating counter per branch to choose among two different predictors based on which predictor (local, global, or even some mix) was most effective in recent predictions. The advantage of a tournament predictor is its ability to select the right predictor for a particular branch.

Indirect branches are for which the target is not know at instruction fetch time. A table of 2-bit predictors are used which is indexed using lower portion of the address of the branch instruction.

\textbf{Loop unrolling} is a software technique that allows reducing the frequency of branches when running loops, and the relative cost of branch control instructions. Unrolling simply replicates the loop body multiple times, adjusting the loop termination code, allowing from  different  iterations  to  be  scheduled together reducing the cost of branch.

\textbf{Unrolling improves the performance of loop by eliminating overhead instructions, although it increases code size substantially (consequently locality and cache efficiency may degrade significantly) and the use of a more registers (leading to more frequent memory interactions).}




\subsection{UMA}

When we have a \textbf{single main memory} that has a \textit{symmetric relationship to all processors and a uniform access time from any processor}, these multiprocessors are most often called \textit{symmetric shared-memory multiprocessors} (\textbf{SMPs}), and this style of architecture is sometimes called \textit{uniform memory access} (\textbf{UMA}): in fact, \textbf{all processors have a uniform latency from memory}. \textbf{The term shared memory refers to the fact that the address space is shared; that is, the same physical address on two processors refers to the same location in memory.} In this architecture all CPUs can have one or more level of cache. However this architecture is obliviously \textbf{not} scalable when the number of CPUs grows.

\subsection{NUMA}

When we have a distributed memory, a multiprocessor architecture is usually called \textit{distributed shared-memory} (\textbf{DSM}). When we use this kind of system, we have two benefits:
\begin{itemize}
\item A cost-effective way to scale the memory bandwidth if most of the accesses are to the local memory in the node.
\item Reduces the latency for accesses to the local memory by a CPU.
\end{itemize}

The key disadvantages for that architecture is that communicating data between processors becomes more complex, \textbf{and that it requires more effort in the software to take advantage of the increased memory bandwidth afforded by distributed memories.}

The DSM multiprocessors are also called \textbf{NUMAs} (\textit{non-uniform memory access}), \textbf{since the access time depends on the location of a data word in memory.} In fact, when a CPU wants to access to an item stored into his node, performing a \textit{local access} involving inner private/shared caches and controllers, access latency is very low. However when a CPU wants to access to an item stored on another node, performing a \textit{remote accesses} involving remote controllers and caches, latency can be very high respect to previous case.

\subsection{The problem of cache coherence}

Unfortunately, \textbf{the view of memory held by different processors is through their individual caches}, which, without any additional precautions, could end up seeing different values of a same shared data (\textbf{cache coherence} problem).

By definition, \textbf{coherence defines what values can be returned by a read} (a \textbf{cache coherence protocols} defines how to maintain coherence) while \textbf{consistency determines when a written value will be returned by a read} (a \textbf{memory consistency protocol} defines when written value must be seen by a reader).

A memory system is coherent if:
\begin{enumerate}
\item A read from location $X$, previously written by a processor, returns the last written value if no other processor carried out writes on $X$ in the meanwhile. \textbf{This property preserve program order that is the causal consistency along program order.}

\item A read by a processor to location $X$ that follows a write by another processor to $X$ returns the written value if the read and write are sufficiently separated in time and no other writes to $X$ occur between the two accesses. \textbf{This property assure that a processor couldn't continuously
read an old data value (Avoidance of staleness).}

\item \textbf{Writes to the same location are serialized}; that is, two writes to the same location by any two processors are seen in the same order by all processors.
\end{enumerate}

The choice and the design of a coherence protocol depends on many factors including: overhead, latency, cache policies, interconnection topology and so on.
\textbf{However the Key to implementing a cache coherence protocol is tracking the state of any copy of a data block}. There are two classes of protocol which define when update aforementioned copies:
\begin{description}
\item[Update protocol] When we use this type of protocol, also called \textit{write update} or \textit{write broadcast}, when a core writes to a block, it updates all other copies (\textbf{it consumes considerably more bandwidth}).

\item[Invalidate protocol] When we use this type of protocol, a processor has \textbf{exclusive access} to a data item before it writes that item; moreover that CPU invalidates other copies on a write that is no other readable or writeable copies of an item exist when the write occurs. \textbf{It is the most common protocol, but suffer of some latency.}

\end{description}

\subsection{Snooping protocol}

The key to implementing an invalidate protocol is the use of the bus, or another broadcast medium, called \textit{network} to perform invalidates and to issue "transactions" on the state of cache blocks.

To perform any operation, the processor simply \textbf{acquires} bus access and broadcasts the address to be invalidated on the bus. All processors continuously \textbf{snoop} on the bus, watching the addresses. The processors check whether the address on the bus is in their cache. If so, the corresponding data in the cache are invalidated. \textbf{A state transition cannot occur unless the broadcast medium is acquired by the source controller and are carried out atomically with a distribute fashions thanks to \textit{serialization} over the broadcast medium}.

When we perform a read, we also need to locate a data item when a cache miss occurs. In a \textbf{write-through cache}, it is easy to find the recent value of a data item, \textit{since all written data are always sent to the memory, from which the most recent value of a data item can always be fetched} (using write through simplifies the implementation of cache coherence). For a \textbf{write-back cache}, the problem of finding the most recent data value is
harder, since the most recent value of a data item can be in a cache rather than in memory (the CPU must get data from another cache)



\subsection{Countermeasures for Meltdown}

The \textbf{KASLR} (Kernel Address Space Randomization) is security technique capable to hinders some types of security attacks involving memory. 

In order to prevent an attacker from accessing to kernel data stored in kernel address space, \textbf{KASLR able us to running Linux kernel images by randomizing where the kernel code is placed at boot time}. This feature was merged into the Linux kernel starting from version 3.14. It can be disabled at boot time by specifying \texttt{nokaslr} as kernel's boot parameters. \textit{However exists a maximum shift we can apply on the logical kernel image (40 bit in Linux Kernel 4.12, enabled by default) therefore KASLR is weak against brute force attacks}.

The \textbf{Kernel page-table isolation} (\textbf{KPTI}), previously called \textbf{KAISER} (\textit{Kernel Address Isolation to have Side-channels Efficiently Removed}), is a feature, merged into Linux kennel starting from version 4.15, \textbf{that mitigates the Meltdown security vulnerability affecting x86 CPUs based on a better isolation of user space and kernel space memory}.

\textbf{The key idea is use two set of page table}:
\begin{itemize}
\item One set of page tables includes both kernel-space and user-space addresses same as before, but it is only used when the system is running in kernel mode.
\item The second set of page tables, usually called as \textit{shadows}, \textbf{contains a copy of all of the user-space mappings but leaves out the kernel side}. However a minimal set of kernel-space mappings that provides the information needed to handle system calls and interrupts are used. This set is intended to be used in user mode. Copying the page tables may sound inefficient, but the copying only happens at the top level of the page-table hierarchy.
\end{itemize}

\textbf{Whenever a process is running in user mode, the shadow page tables will be active}. \textit{The kernel's address space is therefore completely hidden from the process, defeating the known hardware-based attacks. Whenever the system needs to switch to kernel mode, in response to a system call, an exception, or an interrupt, for example, a switch to the other page tables will be made. The code that manages the return to user space must then make the shadow page tables active again.}

\subsection{Total Store Order (TSO)}

The \textbf{Total Store Order} (\textbf{TSO}) is a relaxed consistency models according to which the constraint $W->R$ is relaxed, that is, \textit{rather than waiting for the write to become visible, a write is simply place into an on-core write buffer called \textbf{store buffer}}.

At some time in the future, when it is ``\textit{more convenient}'', \textit{the cache hierarchy will pull the write from the store buffer and propagate it through the caches so that it becomes visible to other threads.}

\textbf{The store buffer allows us to hide the write latency that would usually be required to make write visible to all the other threads.}

In other words, this model requires that all writes appear to occur in a total order, \textbf{but allows a processor's reads to pass its own writes}. \textit{A read can just inspect the store buffer directly, see that it contains a write to the location it's reading, and use that value instead.}


\newpage
\section{Slide \textit{'kernel-programming-basics'}}

\subsection{Segmentation}

Intel microprocessors perform address translation in \textbf{three} different ways:
\begin{description}
\item[Real Mode] This mode exists mostly to \textbf{maintain processor compatibility with older models and to allow the operating system to bootstrap}. In this modality, a logical address is composed of a \texttt{seg} segment (hold by a 16 bit \textit{segment register}) and an \texttt{off} offset (hold by a 16 bit \textit{general register}) while the corresponding physical address is simply computed using \texttt{seg*16 + off}: \textit{as a result, no Global Descriptor Table, Local Descriptor Table, or paging table is needed by the CPU addressing circuit to translate a logical address into a physical one}. \textbf{Observer that in real mode no segment specific protection information are provided, therefore this modality is unsuitable for modern operating system}. Around 1MB ($2^20$ B) of memory is allowed
\item[Protected Mode] Similarly to real mode, a logical address consists of two parts: a \textbf{segment identifier} and an \textbf{offset} that specifies the relative address within the segment. However the target segment \textit{\textbf{identifier}} is a \textbf{13-bit field} present into a \textbf{16 bit field} called the \textbf{segment selector} keep by 16 bit segment register (\textbf{remaining 3 bit are used for protection purposes}) while a 32 bit general register holds the offset. The corresponding physical address is computed using the linear address of the first byte of the segment (which is hold by a special table) using following equation: \texttt{TABLE[segment].base + offset}. In this modality Up to 4 GB of memory is allowed.
\item[Long Mode] Identical to protected mode but the offset is hold by 64-bit general registers (\textit{although up to 48-bit are used in canonical form}). In this way is possible to address up to $2^{48}$ B (256 TB) of linear memory.
\end{description}

\textbf{From now we describe protected mode}. To make it easy to retrieve segment selectors quickly, the processor provides \textbf{segmentation registers} whose only purpose is to hold segment selectors; these registers are called:

\begin{center}
\begin{tabular}{l|l|p{13cm}} 

\toprule
Register Label & Register Name & Description \\
\midrule

\texttt{cs} & \textit{\textbf{C}ode \textbf{S}egment Register} & Points to a segment containing program instructions.

\\
\texttt{ss} & \textit{\textbf{S}tack \textbf{S}egment Register} & Points to a segment containing the current program stack.

\\
\texttt{ds} & \textit{\textbf{D}ata \textbf{S}egment Register} & Points to a segment containing global and static data.

\\
\texttt{es} & \textit{\textbf{E}xtra \textbf{S}egment Register} & General purpose register and may refer to arbitrary data segments.

\\
\texttt{fs} & (\textit{'F' comes after 'E'}) & General purpose register and may refer to arbitrary data segments.

\\
\texttt{gs} & (\textit{'G' comes after 'F'}) & General purpose register and may refer to arbitrary data segments.

\\
\bottomrule
\end{tabular}
\end{center}


To be more precise, segment selectors have three fields:
\begin{description}
\item[index (13-bit)] identifies the Segment Descriptor entry contained in the GDT or in the LDT (see below).
\item[Table Indicator (1-bit)] specifies whether the Segment Descriptor is included in the GDT (TI = 0) or in the LDT (TI = 1).
\item[Requestor Privilege Level (RPL) (2-bit)] specifies the \textbf{Current Privilege Leve}l of the CPU when the corresponding Segment Selector is loaded into the \texttt{cs} register. The value 0 denotes the highest privilege level, while the value 3 denotes the lowest one. Linux uses only levels 0 and 3, which are respectively called \textbf{Kernel Mode} and \textbf{User Mode.}

\end{description}

 the 13-bit field which identifies the Segment Descriptor entry contained in the GDT or in the LDT (see below), 1-bit field which is te Table Indicator that specifies whether the Segment Descriptor is included in the GDT (TI = 0) or in the LDT (TI = 1). Requestor Privilege Level: specifies the Current Privilege Level of the CPU when the corresponding Seg-
ment Selector is loaded into the cs register; it also may be used to selectively weaken the processor priv-
ilege level when accessing data segments (see Intel documentation for details).

\subsubsection{GDT and LDT}

Each segment is represented by an \textbf{8-byte Segment Descriptor} that describes the segment characteristics. Segment Descriptors are stored either in the \textbf{Global Descriptor Table} (\textbf{GDT}) or in the \textbf{Local Descriptor Table} (\textbf{LDT}) \textbf{which are kept in main memory}.

\textbf{The address (32 bit in protected mode, 64 bit in long mode) and size (always 16 bit) of the GDT in main memory are contained in the \texttt{gdtr} control register, while the address and size of the currently used LDT are contained in the \texttt{ldtr} control register.}

\textbf{GDT is used for the mapping of linear addresses \textit{at least} for kernel mode} (that is to manage kernel level segments) while \textbf{LDT is used for user mode} (each process is permitted to have its own LDT). \textbf{However GDT is the unique used segment table in most operating systems}.

\subsubsection{Segment descriptor}

A Segment Descriptor, that is an entry of the GDT/LDT, has \textbf{many} fields including:
\begin{description}
\item[Base] Contains the linear address of the first byte of the segment.
\item[G (Granularity flag)] if equal to 0, the segment size is expressed in bytes; otherwise, it is expressed in multiples of 4096 bytes.
\item[Limit] Holds the offset of the last memory cell in the segment, \textit{thus binding the segment length}. When G is set to 0, the size of a segment may vary between 1 byte and 1 MB; otherwise, it may vary between 4 KB and 4 GB.
\item[DPL (Descriptor Privilege Level)(2 bit)]: \textit{used to restrict accesses to the segment}. It represents the minimal CPU privilege level requested for accessing the segment. Therefore, a segment with its DPL set to 0 is accessible only when the CPL is 0 (Kernel Mode) while a segment with its DPL set to 3 is accessible with every CPL value.
\item[P (Segment-Present)] is equal to 0 if the segment is not stored currently in main memory. Linux \textbf{always} sets this flag to 1, because it never swaps out whole segments to disk.
\end{description}

Now be careful. Because a Segment Descriptor is 8 bytes long, \textbf{its relative address inside the GDT/LDT is obtained by multiplying the 13-bit index field of the Segment Selector by 8}. For instance, if the GDT is at \texttt{0x00020000} (the value stored in the gdtr register) and the index specified by the Segment Selector is 2, the address of the corresponding Segment Descriptor is \texttt{0x00020000 + (2 x 8)}, or 0x00020010 (\textbf{all this in protected mode!!})

The first entry of the GDT is always set to 0. This ensures that logical addresses with a \texttt{null} Segment Selector will be considered invalid, thus causing a processor exception. 

\textbf{There a sort of cache to speed up the access to segment descriptor}. In fact for each of the six segmentation registers \textbf{there are additional non-programmable register which are used to contains the 8-byte Segment Descriptor}.  Every time a Segment Selector is loaded in a segmentation register, the corresponding Segment Descriptor is loaded from memory into the matching nonprogrammable CPU register. From then on, translations of logical addresses referring to that segment can be performed without accessing the GDT.

\textbf{After to have computed the address of a Segment Descriptor, to obtain the linear address, we adds the offset of the logical address to the Base field of the Segment Descriptor.}

\subsubsection{Segmentation in Linux}

Linux uses segmentation in a very limited way. Linux prefers paging to segmentation for the following reasons:
\begin{enumerate}
\item Memory management is simpler when all processes use the same segment register values, that is, when they share the same set of linear addresses.
\item \textbf{One of the design objectives of Linux is portability to a wide range of architectures} (RISC architectures in particular have limited support for segmentation).
\end{enumerate}



 \textbf{All Linux processes running in User Mode use the same pair of segments to address instructions and data} which are called \textbf{user code segment} and \textbf{user data segment}, respectively. Similarly, \textbf{all Linux processes running in Kernel Mode use the same pair of segments to address instructions and data}: they are called \textbf{kernel code segment} and \textbf{kernel data segment}. Now there are some differences about values stored in the fields of their segmentation descriptors: for instance user data/code segments have a DPL equal to 3, while kernel data/code segments have a DPL equal to 0. All these segment descriptors have their base set to \texttt{0x00000000}. 

The corresponding Segment Selectors are defined by the macros \texttt{\_\_USER\_CS}, \texttt{\_\_USER\_DS}, \texttt{\_\_KERNEL\_CS}, and \texttt{\_\_KERNEL\_DS}, respectively. To address the kernel code segment, for instance, the kernel just loads the value yielded by the \texttt{\_\_KERNEL\_CS} macro into the \texttt{cs} segmentation register. It is sufficient to load \texttt{\_\_KERNEL\_CS} into cs whenever the CPU switches to Kernel Mode (CPL equal to 0).

\subsubsection{GDT}

\textbf{There is one GDT for every CPU in the system} that is we have some replicated GDT which differs by some entries (this is important for performance motivation and for transparency of data access separation (same segment = to access different linear addresses by more CPU)). All GDTs are stored in the \texttt{cpu\_gdt\_table} array.  \textbf{Each GDT includes 18 segment descriptors and 14 unused, or reserved entries including the null one}. The most important segment descriptors are:
\begin{itemize}
\item Four segment descriptors corresponding to user and kernel code and data segments (see previous section).
\item A \textbf{Task State Segment} (TSS), different for each processor in the system.
\item A segment including the default Local Descriptor Table (LDT).
\item Three \textbf{Thread-Local Storage} (TLS) segments: \textbf{this is a mechanism that allows multithreaded applications to make use of up to three segments containing data local to each thread}. The \texttt{set\_thread\_area()} and \texttt{get\_thread\_area()} system calls, respectively, create and release a TLS segment for the executing process.
\end{itemize}

\subsection{TSS}

The 80x86 architecture includes a specific segment type called the \textbf{Task State Segment} (\textbf{TSS}). Linux uses the tss\_struct structure to describe the format of the TSS. As already mentioned in
\textbf{The Task State Segments are sequentially stored into \texttt{init\_tss} array which stores one TSS for each CPU on the system}: in particular, the Base field of the TSS descriptor for the nth CPU points to the nth component of the \texttt{init\_tss} array. The DPL is set to 0, because processes in User Mode are not allowed to
access TSS segments.

\textbf{According to the original Intel design, TSS is normally used to store hardware contexts in case of process switch.} However, \textbf{Linux doesn't use hardware context switches because it uses a single TSS for each processor, instead of one for every process}. Linux use the TSS in only two cases:

\begin{itemize}
\item when the CPU switches from User Mode to Kernel Mode, \textbf{it fetches the address of the Kernel Mode stack from the TSS} 

Note that each process descriptor includes a field called \texttt{thread} of type \texttt{thread\_struct}, in
which the kernel saves the hardware context. This data structure includes fields for most of the CPU registers, except the general-purpose registers such as eax, ebx, etc., which are stored in the Kernel Mode stack.
\item When a User Mode process attempts to access an I/O port, the CPU may need to access an I/O Permission Bitmap stored in the TSS to verify whether the process is allowed to address the port.
\end{itemize}

80x86 architecture provides a segment register called the task register (\texttt{TR}) to hold a segment selector that points to a valid TSS segment descriptor which resides in the GDT. To initialize TR, 80x86 ISA provides the assembly instruction \texttt{ltr} which loads the source operand into the task register, where the source operand (a general-purpose register or a memory location) contains a segment selector that points to the TSS contained into GDT. After that, the CPU uses the segment selector into \texttt{tr} to locate the segment descriptor for the TSS in GDT.

\subsection{Control register}

A \textbf{control registers} are a set of registers which changes or controls the general behavior of a CPU like its addressing mode, paging control, and coprocessor control and so on. In modern x86 machines we have:
\begin{description}
\item[CR0] On x86-64 processors in long mode, it is 64 bits long register which have many control bits. The most important are:
\begin{itemize}
\item Bit 0 - PE Protected Mode Enable -If 1, system is in protected mode, else system is in real mode.
\item Bit 31 - PG Paging If 1, enable paging and use the CR3 register, else disable paging. 
\item Bit 16 - WP Write protect When set, the CPU can't write to read-only pages when privilege level is 0 
\end{itemize}
\item[CR1] Reserved.
\item[CR2] Contains a value called \textit{Page Fault Linear Address} (\textbf{PFLA}). \textbf{When a page fault occurs, the address the program attempted to access is stored in the CR2 register.}
\item[CR3] Used when virtual addressing is enabled, hence when the PG bit is set in CR0. CR3 holds a pointer to the \textit{page directory} for the current task
\end{description}

\subsection{Interrupts}

\begin{itemize}

\item An \textbf{interrupt} represents an \textbf{asynchronous event}, \textit{not related to the current CPU execution flaw}, that needs to be immediately managed by system. \textit{Interrupts can be hardware or software}. Under Linux, hardware interrupts are called \textbf{IRQ}'s (\textit{Interrupt Requests})

\item A \textbf{trap} (or \textbf{exceptions}) represents an \textbf{synchronous event} trap related to the current CPU execution flaw. 

\end{itemize}

\textbf{Each interrupt or exception is identified by a 8-bit unsigned number ranging from 0 to 255, which Intel calls \textit{vector}.}

When a CPU receives an interrupt, it starts executing the code at the address found in the corresponding gate of the IDT.

\texttt{SAVE\_ALL} saves all the CPU registers that may be used by the interrupt handler on the
stack, except for \texttt{eflags}, \texttt{cs}, \texttt{eip}, \texttt{ss}, and \texttt{esp}, which are already saved automatically by the control unit. The macro then loads the selector of the user data segment into \texttt{ds} and \texttt{es}.

\subsubsection{IDT}

Linux keeps a system table called \textbf{Interrupt Descriptor Table} (\textbf{IDT}) to \textbf{associate to each interrupt or exception vector with the address of the corresponding handler}. 

On each CPU, the \texttt{idtr} (\textit{interrupt descriptor table register}) CPU register allows the IDT to be located in memory. In fact, in protected mode it holds:
\begin{enumerate}
\item a 48 bit field (\textit{64 bit in long mode}) which specifies the IDT base linear address.
\item a 16 bit field which provides the IDT max length, that is the number of entries currently present in it. 
\end{enumerate}

There are two assembly instruction capable to manipulate the IDT:
\begin{description}
\item[\texttt{lidt} (Load IDT)] It is used to Load the values in the source operand into IDT. 
\item[\texttt{sidt} (Store IDT)] It is used to store the content the interrupt descriptor table register in the destination operand.
\end{description}

\textbf{Remember that the IDT must be initialized by Linux before enabling interrupts by using the \texttt{lidt} assembly language instruction.} IDT is initialized with an excep-
tion handler function for each recognized exception by \texttt{trap\_init()} function (\texttt{/usr/src/linux/kernel/traps.c}). For example the interrupt 0x80 is associated to \texttt{\_system\_call} entry point using \texttt{set\_system\_gate (0x80, \&system\_call)}.


\textbf{In protected mode, each IDT's entry is an 8-byte (64 bit) descriptor} of type struct \texttt{desc\_struct} (which is defined in \texttt{include/asm-i386/desc.h}). In long mode each entry in the IDT grows by 64-bits. The IDT contains some descriptor's type and the value of the \texttt{Type} field encoded in the bits 40–43 identifies the descriptor type:
\begin{description}
\item[Interrupt Gate Descriptor] \textbf{They are used to handle interrupts}. \textit{Includes the Segment Selector and the offset inside the segment of an interrupt
or exception handler}. While transferring control to the proper segment, the processor clears the \texttt{IF} (\textbf{Interrupt flag}) to disable further maskable interrupts (bit 40 set to 0, 41-43 to 1). 
\item[Trap Gate Descriptor] \textbf{They are used to handle exceptions}. Similar to an interrupt gate, except that while transferring control to the proper
segment, the processor does not modify the IF flag (bit 40 set to 1, 41-43 to 1).
\end{description}

A fully populated IDT is 2 KB (256 entries of 8 bytes each) in length. \textbf{All entry from 0 to 31 are reserved by Intel for processor generated exceptions} (general protection fault, page fault, etc.) while all the other entries are available for system programming purposes.

The following architecture-dependent functions (defined in \texttt{arch/i386/kernel/traps.c}) are used to insert on entry in the IDT (note that \texttt{n} indicates the target entry of the IDT, while \texttt{addr} indicates the address of the software module to be invoked for handling the trap or the interrupt):
\begin{description} 
\item[\texttt{set\_trap\_gate(n,addr)}] Inserts a trap gate in the nth IDT entry. The Segment Selector inside the gate is set to the kernel code’s Segment Selector. The Offset field is set to addr. \textbf{The DPL field is set to 0} (\textit{we cannot rely on the \texttt{int} assembly instruction unless we are already executing in kernel mode}).
\item[\texttt{set\_intr\_gate(n,addr)}] Inserts an interrupt gate in the nth IDT entry. The Segment Selector inside the gate is set to the kernel code's Segment Selector. The Offset field is set to addr. The DPL field is set to 0.
\item[\texttt{set\_system\_gate(n,addr)}] Inserts a trap gate in the nth IDT entry. The Segment Selector inside the gate is set to the kernel code's Segment Selector. The Offset field is set to addr. \textbf{The DPL field is set to 3}.
\end{description}









\subsection{System calls}

A \textbf{system call} represents a mechanism according to which an user mode process can requests service from the kernel of the operating system.

\textbf{When a User Mode process invokes a system call, the CPU switches to Kernel Mode and starts the execution of a kernel function.}

User mode processes can invoke a system call in two different ways:
\begin{itemize}

\item By executing the \texttt{int \$0x80} assembly language instruction. (for Windows system you can use the \texttt{int 0x2E} for Windows)
\item By executing the \texttt{sysenter} assembly language instruction, introduced in the Intel Pentium II microprocessors.

\end{itemize}

Similarly, the kernel can exit from a system call, switching the CPU back to User Mode, in two ways:
\begin{itemize}

\item By executing the \texttt{iret} assembly language instruction.
\item By executing the \texttt{sysexit} assembly language instruction, introduced in the Intel Pentium II microprocessors.

\end{itemize}

\subsubsection{Traditional way: \texttt{int \$0x80}}

The \textbf{vector 128} (in hexadecimal, \texttt{0x80}) is associated with the \textbf{unique} kernel entry point: in fact, \textbf{all the other gates aren't usable for on-demand access to the kernel} \textit{because they are reserved for the management of run-time errors and interrupts.}

\begin{quoting}
Recall that the \texttt{trap\_init()} function, invoked during kernel initialization, sets up the \textit{Interrupt Descriptor Table} entry corresponding to vector 128 as follows:
\begin{lstlisting}
set_system_gate(0x80, &system_call);
\end{lstlisting}
\end{quoting}

The call loads the following values into the gate descriptor fields:

\begin{center}
\begin{tabular}{l|p{13cm}} 

\toprule
Gate Field & Content \\
\midrule
\textbf{Segment Selector} & The \texttt{\_\_KERNEL\_CS} Segment Selector of the kernel code segment. \textit{Observe that one memory access to the GDT is required to retrieve it}.

\\
\textbf{Offset} & The pointer to the \texttt{system\_call()} system call handler.

\\
\textbf{Type} & Set to 15. Indicates that the exception is a Trap.

\\
\textbf{DPL (Descriptor Privilege Level)} & Set to 3. This allows processes in User Mode to invoke the exception handler

\\
 
\bottomrule
\end{tabular}
\end{center}



\newpage
\section{Slide \textit{'kernel-level-memory-management'}}




\subsection{Pagination}

The paging unit thinks of all RAM as partitioned into fixed-length \textit{page frames}. Each page frame contains a \textit{page}, that is, the length of a page frame coincides with that of a page. A page frame is a constituent of main memory, and hence it is a storage area. 
It is important to distinguish a page from a page frame; the former is just a block of data, which may be stored in any page frame or on disk.

The data structures that map linear to physical addresses are called \textbf{page tables}; \textit{ they are stored in main memory and must be properly initialized by the kernel before enabling the paging unit.}

\textbf{In 32-architectures, the linear address is 32 bit long} and is divided into three fields:
\begin{description}
\item[Directory] The most significant 10 bits.
\item[Table] The intermediate 10 bits.
\item[Offset] The least significant 12 bits.
\end{description}

\textbf{The physical address of the Page Directory in use is stored in a control register named \texttt{cr3}}. The translation of linear addresses is accomplished in this way:
\begin{enumerate}
\item The \textit{Directory field} within the linear address determines the entry in the Page Directory that points to the proper Page Table.
\item The \textit{Table field}, in turn, determines the entry in the Page Table that contains the physical address of the page frame containing the page.
\item The Offset field determines the relative position within the page frame. 
\end{enumerate}

Because Offset field is 12 bits long, each page consists of 4096 bytes of data; therefore paging unit of Intel processors handles 4 KB pages.

Both the Directory and the Table fields are 10 bits long, so Page Directories and Page Tables can include up to $1024$ entries. It follows that a Page Directory can address
up to $1024 \times 1024 \times 4096=2^{32}$ memory cells.


\textbf{Up to version 2.6.10, the Linux paging model consisted of three paging levels, but, starting with version 2.6.11, a four-level paging model has been adopted}.

The four types of page tables are called:
\begin{description}
\item[Page Global Directory] Its entry type is \texttt{pgd\_t}
\item[Page Upper Directory] Its entry type is \texttt{pud\_t}
\item[Page Middle Directory] Its entry type is \texttt{pmd\_t}
\item[Page Table] Its entry type is \texttt{pte\_t}
\end{description}


The Page Global Directory includes the addresses of several Page Upper Directories, which in turn include the addresses of several Page Middle Directories, which in
turn include the addresses of several Page Tables. Each Page Table entry points to a page frame. \textbf{Entry data types (defined into \texttt{include/asm-i386/page.h}) are 64-bit data types when PAE is enabled and 32-bit data types otherwise.}

Linux adopts some trick in order that paging model fits both 32-bit and 64-bit architectures maintaining the same code and structure:
\begin{description}
\item[32-bit architectures (PAE not enabled)] Since two paging levels are sufficient, Linux kernel set the number of entries of the Page Upper Directory and Page Middle Directory to 1.
\item[32-bit architectures (PAE enabled)] Three paging levels are used; in this case the Page Upper Directory is virtually eliminated setting its size to 1.
\end{description}

The following macros define the size of the page tables blocks (\texttt{include/asm-i386/pgtable-2level.h}):
\begin{itemize}
\item \texttt{\#define PTRS\_PER\_PGD    1024}
\item \texttt{\#define PTRS\_PER\_PMD    1}
\item \texttt{\#define PTRS\_PER\_PTE    1024}
\end{itemize}

\subsubsection{Kernel Page Initialization}

We now describe how the kernel initializes its own page tables. \textbf{Right after the kernel image is loaded into memory, the CPU is still running in real mode; thus, paging is not enabled.}

First of all, all normal code in \texttt{vmlinuz} is compiled with the base address at \texttt{PAGE\_OFFSET} + 1 MB; \textbf{therefore the kernel is actually loaded starting from the first megabyte of memory} (the first megabyte is typically used by BIOS to store system hardware configuration after \texttt{Power-On Self-Test} (POST) and for some BIOS routines used to interact with some devices; therefore, this memory area is ignored by Linux).

A provisional Page Global Directory is initialized statically during kernel compilation and it is contained into the \texttt{swapper\_pg\_dir} variable (now \texttt{init\_level4\_pgt} on x86-64/kernel3 or \texttt{init\_top\_pgt} on x86-64/kernel4)), placed using linker directives at 0x00101000, while the provisional Page Tables are initialized by the \texttt{startup\_32()} assembly language function defined in \texttt{arch/i386/kernel/head.S}. 

In the first phase, the kernel creates, invoking \texttt{alloc\_bootmem\_low\_pages(number of page)} (defined in \texttt{include/linux/bootmem.h}) a limited address space including the kernel's code and data segments, the initial Page Tables, and 128 KB for some dynamic data structures: all of this fit in the first 8 MB of RAM. The objective of this phase of paging is to allow these 8 MB of RAM to be easily addressed.

\textsc{The Kernel creates the desired mapping by filling all the \texttt{swapper\_pg\_dir} entries with zeroes, except for entries 0, 1, 0x300 (decimal 768), and 0x301 (decimal 769);   The address field of entries 0 and 0x300 is set to the physical address of pg0, while the address field of entries 1 and 0x301 is set to the physical address of the page frame following pg0.}




\subsubsection{Describing a Page Table Entry}

As mentioned, each entry is described by the structs \texttt{pte\_t}, \texttt{pmd\_t}, \texttt{pud\_t} and \texttt{pgd\_t}. Even though these are often just \texttt{unsigned integers}, they are defined as structs for type protection so that they will not be used inappropriately.

For type casting, several macros are provided in \texttt{asm/page.h}, which takes the above types and returns an unsigned integer. They are \texttt{pte\_val()}, \texttt{pmd\_val()}, \texttt{pud\_val()} e \texttt{pgd\_val()}. To cast an unsigned integer into the required type other more macros are provided \texttt{\_\_pte()}, \texttt{\_\_pmd()}, \texttt{\_\_pgd()} and \texttt{\_\_pud}.


The entries of Page Directories and Page Tables have the same structure. Each entry includes many filed. the most important are:

\begin{description}
\item[Field containing the 20 most significant bits of a page frame physical address] Field containing the 20 most significant bits of a page frame physical address. Because each page frame has a 4-KB capacity, its physical address must be a multiple of 4096, so the 12 least significant bits of the physical address are always equal to 0. If the field refers to a Page Directory, the page frame contains a Page Table; if it refers to a Page Table, the page frame contains a page of data.
\item[Present flag] If it is set, the referred-to page (or Page Table) is contained in main memory; if the flag is 0, the page is not contained in main memory.
\item[Access flag] Set each time the paging unit addresses the corresponding page frame. This flag may be used by the operating system when selecting pages to be swapped out. The paging unit never resets this flag; this must be done by the operating system.
\item[Dirty flag] Applies only to the Page Table entries. It is set each time a write operation is performed on the page frame. As with the Accessed flag, Dirty may be used by the operating system when selecting pages to be swapped out. The paging unit never resets this flag; this must be done by the operating system.
\item[Read/Write flag] Contains the access right (Read/Write or Read) of the page or of the Page Table. (Zero means read only access)
\item[User/Supervisor flag] Contains the privilege level required to access the page or Page Table (Zero means supervisor privilege)
\item[Global flag] Applies only to Page Table entries. This flag was introduced to prevent frequently used pages from being flushed from the TLB cache. It works
only if the Page Global Enable (PGE) flag of register \texttt{cr4} is set. Non-zero means that the corresponding TLB entry will not be flushed upon loading a new value into the page table pointer \texttt{cr3}.
\item[Page Size flag] Applies only to Page Directory entries. If it is set, the entry refers to 4 MB page frame.
\end{description}

\begin{center}
\begin{tabular}{lr} 
\toprule
 
\textbf{Present flag} & If it is set, the referred-to page (or Page Table) is contained in main memory; if the flag is 0, the page is not contained in main memory. \\ 




\bottomrule
\end{tabular}
\end{center}






There are \textbf{many} macro in \texttt{include/asm-i386/pgtable.h} to set or read above fields within the entries of PTE or PGD like \texttt{pte\_exec()} (Reads the User/Supervisor flag) or \texttt{pte\_mkread()} (Sets the User/Supervisor flag) (all macros are architecture dependent). There are also macros for masking and setting those above fields like \texttt{\#define \_PAGE\_PRESENT 0x001} or \texttt{\#define \_PAGE\_RW 0x002} ecc.

Recall that upon a TLB miss, firmware accesses the page table and then, after pick an entry, it first checked present flag: \textbf{if that flag is set to zero, a page fault occurs which gives rise to a trap.} After the execution of page fault handler, the instruction that gave rise to the trap can get finally re-executed but rememeber that it might rise additional traps (for instance, when it attempts to access a read only page in write mode (rising a segmentation fault))






\subsection{Page Descriptor}

In Linux, \textbf{state information of a page frame is kept in a page descriptor} of type \texttt{struct page} (or struct \texttt{mem\_map\_t}), and all page descriptors, which are 32 byte long, are stored in an array called \texttt{mem\_map} (the space required by it is slightly less than 1\% of the whole RAM). These data structures are defined into \texttt{include/linux/mm.h}. 

The \texttt{virt\_to\_page(addr)} macro yields the address of the page descriptor associated with the linear address \texttt{addr}. 

\texttt{struct page} has many fields but the most important are:
\begin{description}

\item[\texttt{atomic\_t \_count}] It represent a usage reference counter for the page. If it is set to -1, the corresponding page frame is free and can be assigned to any process or to the kernel itself. If it is set to a value greater than or equal to 0, the page frame is assigned to one or more processes or is used to store some kernel data structures. The \texttt{page\_count()} function returns the value of the \texttt{\_count} field increased by one, that is, the number of users of the page. This field is managed via atomic updates, such as with \texttt{LOCK} directives.
\item[\texttt{struct list\_head lru}] Contains pointers to the least recently used doubly linked list of pages. 
\item[\texttt{unsigned long flags}] Array of flags used to describe the status of current page frame (but also encodes the zone number to which the page frame belongs). There are up to 32 flags and Linux kernel defines many macros to manipulate them. Some flags are:
\begin{description}
\item[\texttt{PG\_locked}] The page is locked; for instance, it is involved in a disk I/O operation.
\item[\texttt{PG\_dirty}] The page has been modified.
\item[\texttt{PG\_reserved}] The page frame is reserved for kernel code or is unusable.
\end{description}

\end{description}

\subsection{Free list}

Linux uses \textbf{free list} to manage memory allocation. \textbf{It operates by connecting unallocated regions of memory together in a linked list, using the first word of each unallocated region as a pointer to the next.} 

Free lists make the allocation and deallocation operations very simple. \textbf{To free a region, one would just link it to the free list. To allocate a region, one would simply remove a single region from the end of the free list and use it}. 

\subsection{NUMA}

Is extremely important to remember that Linux 2.6 supports the \textit{Non-Uniform Memory Access} (\textbf{NUMA}) model, \textbf{in which the access times for different memory locations from a given CPU may vary} and, according to that architecture, physical memory is partitioned in several \textbf{nodes}. The time needed by a given CPU to access pages within a single node is the same. However, this time might not be the same for two different CPUs. 

\subsection{NUMA Node Descriptor}

\textbf{Be careful that Linux splits physical memory inside each node into several zones. We have 3 free lists of frames, depending on the frame positioning within available zones (defined in \texttt{include/linux/mmzone.h}) which are:}

\begin{description}
\item[\texttt{ZONE\_DMA}] Contains page frames of memory below 16 MB, that is page frames that can be used by old ISA-based devices (\textit{Direct Memory Access} (DMA) processors).
\item[\texttt{ZONE\_NORNMAL}] Contains page frames of memory at and above 16 MB and below 896 MB (direct mapped by the kernel).
\item[\texttt{ZONE\_HIGHMEM}] Contains page frames of memory at and above 896 MB (only page cache and user).
\end{description}

To represent a NUMA node, Linux uses a descriptor of type \texttt{struct pg\_data\_t}. All node descriptors are stored in a singly linked list, whose first
element is pointed to by the \texttt{pgdat\_list} variable. Be careful to the fact that this data structure is used by Linux kernel even if the architecture is based on \textit{Uniform Memory Access} (\textbf{UMA}): in fact Linux makes use of a single node that includes all system physical memory. Thus, the \texttt{pgdat\_list} variable points to a list consisting of a single element (node 0) stored in the \texttt{contig\_page\_data} variable.

Remember that free lists information is kept within the \texttt{struct pg\_data\_t} data structure. In fact the most important fields of \texttt{struct pg\_data\_t} are:

\begin{description}
\item[\texttt{struct page *node\_mem\_map}] Array of page descriptors of the node
\item[\texttt{struct zone [] node\_zones}] Array of zone descriptors of the node
\end{description}

\subsection{Zone Descriptor}

Obliviously each memory zone has its own descriptor of type \texttt{struct zone} and many fields of this data structure are used for page frame reclaiming. However, most important fields are:

\begin{description}
\item[\texttt{struct page * zone\_mem\_map}] Pointer to first page descriptor of the zone.
\item[\texttt{spinlock\_t lock}] Spin lock protecting the descriptor.
\item[\texttt{struct free\_area [] free\_area}] Identifies the blocks of free page frames in the zone
\end{description}

In summary, Linux has links to the memory node and to the zone inside the node that includes the corresponding page frame of type \texttt{struct page}.

\subsection{Buddy allocator}

The technique adopted by Linux to solve the external fragmentation problem is based on the well-known \textbf{buddy system} algorithm. All free page frames are grouped into 11 lists of blocks that contain groups of 1, 2, 4, 8, 16, 32, 64, 128, 256, 512, and 1024 contiguous page frames, respectively. The largest request of 1024 page frames corresponds to a chunk of 4 MB of contiguous RAM. We use the term \texttt{order} \textbf{to indicate the logarithmic size of a block}.

Assume there is a request for a group of 256 contiguous page frames (i.e., one megabyte). The algorithm checks first to see whether a free block in the 256-page-frame list exists. If there is no such block, the algorithm looks for the next larger block — a free block in the 512-page-frame list. If such a block exists, the kernel allocates 256 of the 512 page frames to satisfy the request and inserts the remaining 256 page frames into the list of free 256-page-frame blocks. If there is no free 512-page block, the kernel then looks for the next larger block (i.e., a free 1024-page-frame block). If
such a block exists, it allocates 256 of the 1024 page frames to satisfy the request, inserts the first 512 of the remaining 768 page frames into the list of free 512-page-frame blocks, and inserts the last 256 page frames into the list of free 256-page-frame blocks. If the list of 1024-page-frame blocks is empty, the algorithm gives up and signals an error condition.

\textbf{Linux 2.6 uses a different buddy system for each zone}. Thus, in the x86 architecture, there are \textbf{3 buddy systems}: the first handles the page frames suitable for ISA DMA, the second handles the "normal" page frames, and the third handles the high memory page frames. Each buddy system relies on the following main data structures:
\begin{itemize}
\item \textbf{The \texttt{mem\_map} array where all page descriptors are stored}. Actually, each zone is concerned with a subset of the \texttt{mem\_map} elements. The first element in the subset and its number of elements are specified, respectively, by the \texttt{zone\_mem\_map} and \texttt{size} fields of the zone descriptor.

\item The array consisting of eleven elements of type \texttt{struct free\_area}, one element for each group size. As we said the array is stored in the \texttt{free\_area} field of the zone descriptor.
\end{itemize}

Let us consider the $k^{th}$ element of the \texttt{struct free\_area} array in the zone descriptor, which identifies all the free blocks of size $2^k$. In this data structure there is a pointer of type struct \texttt{list\_head}  which is is the head of a doubly linked circular list that collects the page descriptors associated with the free blocks of $2^k$ pages. Besides the head of the list, the $k^{th}$ element of the \texttt{struct free\_area} array includes also the field \texttt{nr\_free}, which specifies the number of free blocks of size $2^k$ pages, and a pointer to a bitmap that keeps fragmentation information.  

Recall that spin locks are used to manage \texttt{mem\_map} AND \texttt{struct free\_area} array.

\textbf{To achieve better performance a little number of page frames are kept in cache to quickly satisfy the allocation requests for single page frames}.

\subsection{API}

Page frames can be requested by using some different functions and macros (APIs) (they return \texttt{NULL} in case of failure, a linear address of the first allocated page in case of success) which prototype are stored into \texttt{\#include <linux/malloc.h>}. The most important are:

\begin{description}
\item[\texttt{get\_zeroed\_page(gfp\_mask)}] Function used to obtain a page frame filled with zeros.
\item[\texttt{\_\_get\_free\_page(gfp\_mask)}] Macro used to get a single page frame.
\item[\texttt{\_\_get\_free\_pages(gfp\_mask, order)}] Macro used to request $2^{order}$ contiguous page frames returning the linear address of the
first allocated page.
\item[\texttt{free\_page(addr)}] This macro releases the page frame having the linear address \texttt{addr}.

The parameter \texttt{gfp\_mask} is a group of flags that specify \textbf{how to look for free page frames} and they are extremely important when we require page frame allocation in different contexts including: 
\begin{description}
\item[Interrupt context] allocation is requested by an \textbf{interrupt handler} which uses above function with \texttt{GFP\_ATOMIC} flag (equivalent to \texttt{\_\_GFP\_HIGH}) \textbf{which means that the kernel is allowed to access the pool of reserved page frames: therefore the call cannot lead to sleep (that is no wait)}  An atomic request never blocks: if there are not enough free pages the allocation simply fails.

\item[Process context] allocation is caused by a system call using \texttt{GFP\_KERNEL} or \texttt{GFP\_USER} (both equivalent to \texttt{\_\_GFP\_WAIT | \_\_GFP\_IO | \_\_GFP\_FS}) according to which kernel is allowed to block the current process waiting for free page frames (\texttt{\_\_GFP\_WAIT}) and to perform I/O transfers on low memory pages in order to free page frames (\texttt{\_\_GFP\_IO}): therefore the call can lead to sleep.
\end{description}

\end{description}



\subsection{TLB operation}

Besides general-purpose hardware caches, x86 processors include a cache called \textit{Translation Lookaside Buffers} (\textbf{TLB}) to speed up linear address translation.

When a linear address is used for the first time, the corresponding physical address is computed through slow accesses to the Page Tables in RAM. The physical address is then stored in a TLB entry so that further references to the same linear address can be quickly translated.

In a multiprocessor system, \textbf{each CPU has its own TLB, called the \textbf{local TLB} of the CPU}. Contrary to the hardware cache, the corresponding entries of the TLB need \textbf{not} be synchronized, because processes running on the existing CPUs may associate the same linear address with different physical ones.

\textbf{When the \texttt{cr3} control register of a CPU is modified, the hardware automatically invalidates all entries of the local TLB, because a new set of page tables is in use (page table changes).} \textbf{However changes inside the current page table are not automatically reflected within the TLB.}

Fortunately, Linux offers several TLB flush methods that should be applied appropriately, depending on the type of page table change:
\begin{description}
\item[\texttt{flush\_tlb\_all}] This flushes the \textbf{entire TLB on all processors} running in the system, which makes it the most expensive TLB flush operation. It is used when we have made changes into the kernel page table entries. \textbf{After it completes, all modifications to the page tables will be visible globally to all processors.}

\item[\texttt{flush\_tlb\_mm(struct mm\_struct *mm)}] Flushes all TLB entries of the non-global pages owned by a given process that is all entries related to the userspace portion for the requested \texttt{mm} context. Is used when forking a new process.

\item[\texttt{flush\_tlb\_range}] Flushes the TLB entries corresponding to a linear address interval of a given process and is used when releasing a linear address interval of a process (when \texttt{mremap()} or \texttt{mprotect()} is used).

\item[\texttt{flush\_tlb\_page}] Flushes the TLB of a single Page Table entry of a given process and is used when handling a page fault.

\item[\texttt{flush\_tlb\_pgtables}] Flushes the TLB entries of a given contiguous subset of page tables of a given process and is called when a region is being unmapped and the page directory entries are being reclaimed 
\end{description}

Despite the rich set of TLB methods offered by the generic Linux kernel, every microprocessor usually offers a far more restricted set of TLB-invalidating assembly language instructions. \textbf{Intel microprocessors offers only two TLB-invalidating techniques: the automatic flush of all TLB entries when a value is loaded into the cr3 register and the \texttt{invlpg} assembly language instruction which invalidates a single TLB entry mapping a given linear address.}

The architecture-independent TLB-invalidating methods are extended quite simply to multiprocessor systems. \textbf{The function running on a CPU sends an Interprocessor Interrupt to the other CPUs that forces them to execute the proper TLB-invalidating function} (\textbf{expensive} operation (\textit{direct cost}) due to latency for cross-CPU coordination in case of global TLB flushes).

Remember that flush a TLB has the direct cost of the latency of the firmware level protocol for TLB entries invalidation (selective vs non-selective). Recall that flush TLB lead to \textbf{indirect cost} of refilling TLB entries and the latency experimented by MMU firmware upon misses in the translation process of virtual to physical addresses.

\subsubsection{When flush TLB?}

As a general rule,\textbf{ any process switch implies changing the set of active page tables and therefore local TLB entries relative to the old page tables must be flushed}; this is done automatically when the kernel writes the address of the new Page Global Directory into the \texttt{cr3} control register.

Besides \textbf{process switches}, there are other cases in which the kernel needs to flush some entries in a TLB. For instance, when the kernel assigns a page frame to a User Mode process and stores its physical address into a Page Table entry, it must flush any local TLB entry that refers to the corresponding linear address (virtual addresses accessible \textbf{locally} in time-sharing concurrency). On multiprocessor systems, the kernel also must flush the same TLB entry on the CPUs that are using the same set of page tables, if any (virtual addresses accessible \textbf{globally}  by every CPU/core in real-time-concurrency).

Kernel-page mapping has a \textit{global} nature, therefore when we use \texttt{vmalloc()} / \texttt{vfree()} on a specific CPU, all the other must observer mapping updates and TLB flush is necessary. 

\newpage
\section{Slide \textit{'kernel-level-task-management'}}









\subsection{Interrupt handling}



IRQ's management typically occurs via a \textbf{two-level logic}:
\begin{description}
\item[Top Half] A routine that actually responds to the interrupt and do a minimal amount of work to schedule its bottom half (this operation is very fast).
\item[Bottom Half] A routine scheduled by top half which execute whatever other work is required to handle the interrupt (such as awakening processes, starting up another I/O operation, and so on)
\end{description}

For instance, when a network interface reports the arrival of a new packet, the top half routine just retrieves the data and pushes it up to the protocol layer; actual processing of the packet is performed in a bottom half.

The most important aspect of this setup it that it permits the \textit{top half to service a new interrupt while the bottom half is still working}; \textbf{in fact all interrupts are enabled during execution of the bottom half}. Generally the execution of top half code is handled according to a \textit{non-interruptible scheme} (\textbf{but isn't mandatory}). 

This scheme permit to \textbf{avoid to keep locked resources when an interrupt occurs} (we may incur the risk of delaying critical actions as a spin-lock release) \textbf{avoiding possible deadlocks} when a slow interrupt management is hit by the activation of another one that needs the same resources. \textbf{Moreover this scheme keep
kernel response time small which is a very important property for many time-critical applications that expect their interrupt requests to be serviced in a few milliseconds.}

\subsection{Softirqs, Tasklets and work queues}

Form Linux 2.6, two different mechanisms are used to implement top/bottom-half processing:
\begin{itemize}
\item The so-called \textit{deferrable functions}, which we will call as \textbf{softirqs} and \textbf{tasklets}: they are very fast, but all tasklet code must be atomic.
\item The \textbf{Workqueues}, which may have a higher latency but that are allowed to sleep.
\end{itemize}

\subsubsection{Softirqs}

\textbf{Softirqs are statically allocated}, that is they are defined at compile time. The main data structure used to represent softirqs is the \texttt{softirq\_vec} array, which includes \texttt{NR\_SOFTIRQS} (32 entries) elements of type \texttt{softirq\_action}. \textbf{Observer that the priority of a softirq is the index of the corresponding \texttt{softirq\_action} element inside the array}. Some of the softirqs used in Linux are:

\begin{description}
\item[\texttt{HI\_SOFTIRQ}] With priority equal to 0 (first element of array) and it handles high priority tasklets.
\item[\texttt{TIMER\_SOFTIRQ}] With priority equal to 1 and it is used for timer related interrupts.
\end{description}

Another crucial data structure for implementing the softirqs is a \textbf{per-CPU 32-bit mask describing the pending softirqs}; it is stored in the \texttt{\_\_softirq\_pending} field of the \texttt{irq\_cpustat\_t} data structure (which is one of the data structure used per each CPU in the system). To get and set the value of the bit mask, the kernel makes use of the \texttt{local\_softirq\_pending()}. This is way softirqs can run concurrently on several CPUs, even if they are of the same type.

During interrupt acceptance, top half routine set properly the bit mask in the \texttt{\_\_softirq\_pending} field and then exit. 

Checks for active (pending) softirqs should be perfomed periodically, but without inducing too much overhead. They are performed in a few points of the kernel code. 

For this purpose, Linux, \textbf{for each CPU}, uses the so called \texttt{ksoftirqd/n} kernel thread (where n is the logical number of the CPU) to manage softirqs array executing bottom halves asynchronously. Once awaken, that thread, running the \texttt{ksoftirqd()} function, checks softirq bit mask for pending softirqs inspecting the per-CPU field \texttt{\_\_softirq\_pending}. If there are no softirqs pending, the function puts the current thread in the \texttt{TASK\_INTERRUPTIBLE} state and invokes then the \texttt{cond\_resched()} function to perform a process switch; otherwise, the thread runs the softIRQ handler, running \texttt{do\_softirq()}.

Be careful that the top half routine can set the bit mask telling that a \texttt{ksoftirqd/x} awaken on a CPU-core x will not process the handler associated with a given softIRQ; in this way we can \textbf{create affinity between SoftIRQs and CPU-cores in order to exploit NUMA machines}. Is also possible to set bit mask  in order to build affinity on group of CPU for load balancing; \textbf{in other word is possible a multithread execution of bottom half tasks}.

\subsubsection{tasklet}

When we use softirqs not necessarily we queue bottom half task, so this setup can be even more responsive. However the queuing concept is still there for on demand usage, if required. 

Tasklets are built on top of two softirqs named \texttt{HI\_SOFTIRQ} and \texttt{TASKLET\_SOFTIRQ}. Several tasklets may be associated with the same softirq, each tasklet carrying its own function. There is no real difference between the two softirqs, except that \texttt{do\_softirq()} executes \texttt{HI\_SOFTIRQ}’s tasklets before \texttt{TASKLET\_SOFTIRQ}’s tasklets.

Tasklets and high-priority tasklets are stored in the \texttt{tasklet\_vec} and \texttt{tasklet\_hi\_vec}
arrays respectively and both of them include \texttt{NR\_CPUS} elements which are \textbf{list} of \textbf{tasklet descriptors} which are a data structure of type \texttt{tasklet\_struct}

Each \texttt{tasklet\_struct} has many fields including the \texttt{state} field which represents the status of current tasklet and can assume two value: \texttt{TASKLET\_STATE\_SCHED} (tasklet pending), \texttt{TASKLET\_STATE\_RUN} (tasklet is running). Using this field is possible to keep track of a specific bottom half task, related to the execution of a specific function internal to the kernel.

Linux offers many APIs to manage tasklets: for instance to allocate a new \texttt{tasklet\_struct} data structure and
initialize is need to invoke \texttt{tasklet\_init()}; this function receives as its parameters the address of the tasklet descriptor , the address of your tasklet function (\texttt{void (*func)}), and its optional integer argument (\texttt{unsigned long}) for data.

The tasklet may be selectively disabled by invoking either \texttt{tasklet\_disable\_nosync()} or \texttt{tasklet\_disable()}. Both functions increase the count field of the tasklet descriptor, but the latter function does not return until an already running instance of the tasklet function has terminated. To reenable the tasklet, use \texttt{tasklet\_enable()}. To activate the tasklet, you should invoke either the \texttt{tasklet\_schedule()} function or the \texttt{tasklet\_hi\_schedule()} function, according to the priority that you require for the tasklet. When a tasklet is enabled its descriptor is added at the beginning of the list pointed to by \texttt{tasklet\_vec[n]} or \texttt{tasklet\_hi\_vec[n]}, where n denotes the logical number of the local CPU; then \texttt{HI\_SOFTIRQ} and \texttt{TASKLET\_SOFTIRQ} softirq are enabled (\textbf{all these operation are executed with local interrupts disabled})

Remember that tasklets can be instantiated by exploiting also the following macros defined 
in include \texttt{include/linux/interrupt.h}: 
\begin{itemize}
\item \texttt{DECLARE\_TASKLET(tasklet, function, data)}
\item \texttt{DECLARE\_TASKLET\_DISABLED(tasklet, function, data)}
\end{itemize}

Finally to execute tasklet associated with the \texttt{HI\_SOFTIRQ} softirq we run \texttt{tasklet\_hi\_action()}, while for those associated with \texttt{TASKLET\_SOFTIRQ} we use \texttt{tasklet\_action()}. 

\textbf{Observer that if the tasklet has already been scheduled on a different CPU-core, it will not be moved to another CPU-core if it's still pending (generic softirqs can instead be processed by different CPU-cores)}

\textbf{Tasklets run in interrupt context (see below)}

\subsubsection{Work queue}

The work queues have been introduced in Linux 2.6 and replace a similar construct called "task queue" used in Linux 2.4. Also the work queues are used to allow kernel functions to be activated and later executed by special kernel threads called \textit{worker threads}. 

However there is one important difference with softirq and tasklet: \textbf{deferrable functions (that is softirqs and tasklet) run in interrupt context while functions in work queues run in process context}. \textit{Running in process context is the only way to execute functions that can block (for instance, functions that need to access some block of data on disk) because no process switch can take place in interrupt context.}

\textbf{Observer that interrupts are enabled while the work queues are being run (except if the same work to be done disables them)}

The main data structure associated with a work queue is a descriptor called \texttt{workqueue\_struct}, which contains \textbf{many} fields including an array of \texttt{NR\_CPUS} elements (the maximum number of CPUs in the system.) Each element is a descriptor of type \texttt{cpu\_workqueue\_struct}, which contains a \texttt{worklist} field which is the head of a doubly linked list collecting the pending functions of the work queue. Every pending function is represented by a \texttt{work\_struct} data structure.

The \texttt{create\_workqueue("foo")} function receives as its parameter a string of characters and returns the address of a \texttt{workqueue\_struct} descriptor. The function also creates n worker threads (where n is the number of CPUs effectively present in the system), named after the string passed to the function: \texttt{foo/0, foo/1}, and so on. The \texttt{create\_singlethread\_workqueue()} function is similar, but it creates just one worker thread, no matter what the number of CPUs in the system is. To destroy a work queue the kernel invokes the \texttt{destroy\_workqueue()} function, which receives as its parameter a pointer to a \texttt{workqueue\_struct} array.

Another very important API is \texttt{queue\_work()} which inserts a function (already packaged inside a \texttt{work\_struct} descriptor) in a work queue; it receives a pointer \texttt{wq} to the \texttt{workqueue\_struct} descriptor and a pointer work to the \texttt{work\_struct} descriptor.

The \texttt{queue\_delayed\_work()} function is nearly identical to \texttt{queue\_work()}, except that it
receives a third parameter representing a time delay in system ticks and it is used to ensure a minimum delay before the execution of the pending function.

\texttt{cancel\_delayed\_work()} cancels a previously scheduled work queue function.
The \texttt{flush\_workqueue()} function receives a \texttt{workqueue\_struct} descriptor address and blocks the calling process until all functions that are pending in the work queue terminate. 


\subsubsection{The predefined work queue} 

The kernel offers a predefined work queue called \textit{events}, which can be freely used by every kernel developer. The predefined work queue is nothing more than a standard work queue that may include functions of different kernel layers and I/O drivers.

To make use of the predefined work queue, the kernel offers some APIs including \texttt{schedule\_work(struct work\_struct *work)} and \texttt{schedule\_work\_on(int cpu, struct work\_struct *work)}.

\subsection{\texttt{container\_of}}

The macro \texttt{container\_of(ptr, type, member)} takes, as you can see, three arguments: a pointer to the member of a data structure, the name of the type of the data structure, and the name of the member the pointer refers to. The macro yields the address of the container structure which accommodates the specified member.

\subsection{Timers}

On the x86 architecture, the kernel must explicitly interact with several kinds of clock circuits which are used both to keep track of the current time of day and to make precise time measurements. \textbf{The timer circuits are programmed by the kernel, so that they issue interrupts at a fixed, predefined frequency; such periodic interrupts are crucial for implementing the software timers used by the kernel and the user programs}. 

\begin{description}
\item[Time Stamp Counter (TSC)] It is a counter accessible through the 64-bit \textit{Time Stamp Counter} (\textbf{TSC}) register, which can be read using \texttt{rdtsc} assembly language instruction. \textbf{It represents a counter that is increased at each clock signal.} It is used by Linux to determine the clock signal frequency while initializing the system; that task is accomplished using \texttt{calibrate\_tsc()}.
\item[High Precision Event Timer (HPET)] The HPET represents a very powerful chip which provides up to \textbf{eight 32-bit or 64-bit independent counters exploitable by kernel}. Each counter is driven by its own clock signal, whose frequency must be at least 10 MHz and, therefore, the counter is increased at least once in \textbf{100 nanoseconds}. Any counter is associated with at most 32 timers, each of which is composed by a \textit{comparator} and a \textit{match register}. \textbf{The comparator is a circuit that checks the value in the counter against the value in the match register, and raises a hardware interrupt if a match is found. Some of the timers can be enabled to generate a periodic interrupt.}
\item[LAPIC] The Local APIC Timer (LAPIC-T) represents another time-measuring device. This timer has a counter of \textbf{32 bits long} used to store the number of of ticks that must elapse before the interrupt is issued; therefore, the local timer can be programmed to issue interrupts at very low frequencies. \textbf{Observe that local APIC timer sends an interrupt only to its processor}. The APIC’s timer is based on the bus clock signal and can be can be programmed in such a way to decrease the timer counter every 1, 2, 4, 8, 16, 32, 64, or 128 bus clock signals.
\end{description}

\subsubsection{The timer interrupt handler}

As said these timer circuits issues special interrupts called \textbf{timer interrupt}, which notifies the kernel
that one more time interval has elapsed. Interrupts can both involve a specific CPU (CPU local timer interrupt signals timekeeping activities related to the local CPU, such as monitoring how long the current process has been running and updating the resource usage statistics) or signal activities not related to a specific CPU, such as handling of software timers and keeping the system time up-to-date.




\subsection{TCB}

In Linux, the process descriptor is represented by a \texttt{task\_struct} structures (defined in \texttt{include/linux/sched.h}) which have many fields including:

\begin{description}
\item[\texttt{volatile long state}] Process state.
\item[\texttt{struct mm\_struct *mm}] Pointer to memory are descriptor.
\item[\texttt{thread\_info}] Low-level information (CPU specific) for the process.
\item[\texttt{files\_struct}] Pointers to file descriptors.
\item[\texttt{signal\_struct}] Signals received.
\item[\texttt{pid\_t pid}]
\item[\texttt{volatile long need\_resched}]
\item[\texttt{long nice}]
\end{description}

Each \texttt{task\_struct} structure includes a tasks field of type \texttt{list\_head} (\textit{a doubly linked list defined by Linux}) whose \texttt{prev} and \texttt{next} fields point, respectively, to the previous and to the next \texttt{task\_struct} element.

The head of the process list is the \texttt{init\_task} descriptor (type \texttt{task\_struct}); it is the process descriptor of the so-called process 0 or \textit{swapper} or \textit{IDLE process}.

The \texttt{state} field consists of an array of flags, each of which describes a possible process state. In the current Linux version, these states are mutually exclusive, and hence exactly one flag of state always is set; the remaining flags are cleared. The possible states (there are some macro defined in \texttt{include/linux/sched.h}) are:
\begin{description}
\item[\texttt{TASK\_RUNNING}] The process is either executing on a CPU or waiting to be executed.
\item[\texttt{EXIT\_ZOMBIE}] Process execution is terminated, but the parent process has not yet issued wait() system call to return information about the dead process.
\item[\texttt{TASK\_INTERRUPTIBLE}] The process is suspended (sleeping) until some condition becomes true. Raising a hardware interrupt is an example of conditions that might wake up the process
\end{description}

When looking for a new process to run on a CPU, the kernel has to consider only the runnable processes (that is, the processes in the \texttt{TASK\_RUNNING} state).

\subsubsection{TCB allocation}

Processes are dynamic entities, whose lifetimes range from a few milliseconds to months, \textbf{therefore process descriptors are stored in dynamic memory rather than in the memory area permanently assigned to the kernel}. For each process, \textbf{Linux packs two different data structures in a single per-process memory area}:
\begin{itemize}
\item a small data structure linked to the process descriptor, namely the \texttt{thread\_info} structure.
\item Kernel Mode process stack and the length of this memory area is usually 8,192 bytes (8 KB) (\textbf{stored in two consecutive page frames with the first page frame aligned to a multiple of $2^13$}).
\end{itemize}

\textbf{The \texttt{thread\_info} structure resides at the beginning of the memory area, and the stack grows downward from the end}. The growth of the stack size may lead to buffer overflow risks, if the stack size is not rescaled and memory fragmentation, if the stack size is rescaled

The C language allows the \texttt{thread\_info} structure and the kernel stack of a process to be conveniently represented by means of the following union construct:


\begin{lstlisting}[frame=lines]
union thread_union {
    struct thread_info thread_info;
    unsigned long stack[2048]; /* 1024 for 4KB stacks */
};
\end{lstlisting}


\subsection{Preemption}

As a general definition a kernel is \textit{preemptive} \textbf{if a process switch may occur while the replaced process is executing a kernel function, that is, while it runs in Kernel Mode}. 

\textbf{Kernel pre-emption is disabled when the \texttt{preempt\_count} field in the \texttt{thread\_info} descriptor referenced by the \texttt{current\_thread\_info()} macro \textbf{is greater than zero}}. Linux provides several APIs to manage kernel pre-emption:
\begin{description}
\item[\texttt{preempt\_count()}] Return the \texttt{preempt\_count} field in the \texttt{thread\_info} descriptor.
\item[\texttt{preempt\_disable()}] Increases by one the value of the preemption counter.
\item[\texttt{preempt\_enable\_no\_resched()}] Decreases by one the value of the preemption counter.
\item[\texttt{preempt\_enable()}] Decreases by one the value of the preemption counter, and invokes \texttt{preempt\_schedule()} if the \texttt{TIF\_NEED\_RESCHED} flag in the \texttt{thread\_info} descriptor is set
\end{description}

But there are other two very important API and they are related to \textbf{per-CPU variables}. 

Remember that a \textbf{\textit{per-CPU variables} is an array of data structures, one element per each CPU in the system}. A CPU should not access the elements of the array corresponding to the other CPUs; on the other hand, it can freely read and modify its own element without fear of race conditions, because it is the only CPU entitled to do so. \textbf{While per-CPU variables provide protection against concurrent accesses from several CPUs}, they do \textbf{not provide protection against accesses from asynchronous functions} (interrupt handlers and deferrable functions like tasklet and softirqs). \textbf{Therefore per-CPU variables are prone to race conditions caused by kernel pre-emption, both in uniprocessor and multiprocessor systems}. \textbf{As a general rule, a kernel control path should access a per-CPU variable with kernel preemption disabled.}. We have some API:

\begin{description}
\item[\texttt{get\_cpu\_var(name)}] Disables kernel preemption, then selects the local CPU’s element of the per-CPU array name
\item[\texttt{put\_cpu\_var(name)}] Enables kernel preemption.
\end{description}

\subsubsection{Process schedule}

\textbf{Linux processes are preemptable} and therefore, when a process enters in the \texttt{TASK\_RUNNING} state, \textbf{the kernel checks whether its \textit{dynamic priority} is greater than the priority of the currently running process}. If it is, the execution of current process is interrupted and the scheduler is invoked to select another process to run. \textbf{A process also may be pre-empted when its time quantum expires}, that is when the \texttt{TIF\_NEED\_RESCHED} flag in the \texttt{thread\_info} structure of the current process is set, so the scheduler is invoked when the timer interrupt handler terminates.

\textbf{The scheduler always succeeds in finding a process to be executed}; in fact, there is always at least one runnable process, that is the process 0, also known as \textit{swapper} or \textit{idle process} (\textbf{every CPU of a multiprocessor system has its own swapper process with PID equal to 0}).

Every Linux process is always scheduled according to one of the following scheduling classes:

\begin{description}
\item[\texttt{SCHED\_FIFO}] A First-In, First-Out
\item[\texttt{SCHED\_RR}] A Round Robin.
\item[\texttt{SCHED\_NORMAL}] Time-shared.
\end{description}

\subsubsection{Process Priority}

Every conventional (\textit{non-real-time}) process has its own \textbf{static priority} which is used to determine the \textit{base time quantum} of a process, which is computed so that an higher static priority corresponds to a longer base time quantum.

The static priority is represented by a number ranging from 100 (highest priority) to 139 (lowest priority). A new process always inherits the static priority of its parent, although an user can change the static priority of the processes that he owns using \texttt{nice()} or \texttt{setpriority()} system call.

However every conventional process also has a \textbf{dynamic priority}, which is a value ranging from 100 (highest priority) to 139 (lowest priority). \textbf{The dynamic priority is the number actually looked up by the scheduler when selecting the new process to run: a thread that calls the schedule function can be pre-empted by one that has higher dynamic priority}.

\textit{It is related to static priority in term of reward or penalty depending on whether the thread is interactive or not.}

Generally a thread is considered interactive if its sleep time is very high; technically a process having highest static priority (100) is considered interactive when its average sleep time exceeds 200 ms. 

Obliviously all static and dynamic priority values of a process is stored into its TCB.

\subsubsection{Data structures used by the scheduler}

The \texttt{runqueue} data structure is the most important data structure of the Linux 2.6 scheduler. It links the process descriptors of all
runnable processes, that is of those in a \texttt{TASK\_RUNNING} state. \textbf{Each CPU in the system has its own \texttt{runqueue} and all \texttt{runqueue} structures are stored in the \texttt{runqueues} per-CPU variable}. \textit{However a CPU-core can access to the runqueue of another one for load balancing purposes}

The most important fields of the \texttt{runqueue} data structure are those related to the lists of runnable processes. They are:

\begin{center}
\begin{tabular}{l|l|p{13cm}} 

\toprule
Type & Field & Description \\
\midrule

\texttt{prio\_array\_t *} & \texttt{active} & Pointer to the lists of active processes. Technically it points to one of the two \texttt{prio\_array\_t} data structures in \texttt{arrays}

\\
\texttt{prio\_array\_t *} & \texttt{expired} & Pointer to the lists of expired processes. Technically it points to one of the two \texttt{prio\_array\_t} data structures in \texttt{arrays}


\\
\texttt{prio\_array\_t [2]} & \texttt{arrays} & The two sets of active and expired processes.


\\
\bottomrule
\end{tabular}
\end{center}

\textbf{Every runnable process in the system belongs to one, and just one, runqueue.} As long as a runnable process remains in the same runqueue, it can be executed only by the CPU owning that runqueue. \textbf{Remember that runnable processes may migrate from one runqueue to another.} \textit{As you see, there is no mix of runnable and non-runnable tasks on a runqueue.}

As you can see, the \texttt{arrays} field of the \texttt{runqueue} is an array consisting of two \texttt{prio\_array\_t} structures. \textbf{Each data structure represents a set of runnable processes, and includes 140 doubly linked list heads (one list for each possible process priority), a priority bitmap, and a counter of the processes included in the set}. \textit{Remember that 40 levels (say [100-139]) map to classical Unix time-sharing, while 100 levels (say [0-99]) map to Unix real-time scheduler extensions}.

\textit{Periodically, the role of the two data structures in arrays changes: the active processes suddenly become the expired processes, and the expired processes become the active ones. We switch the queues upon a new epoch.}

When a process creates a child, \textbf{the number of ticks left to the parent is split in two halves}: one for the parent and one for the child. \textit{This is done to prevent users from getting an unlimited amount of CPU time under certain condition.}



\newpage
\section{Slide \textit{'trap-interrupt-architecture'}}

\subsubsection{IPI}

An \textbf{Inter-processor Interrupt} (\textbf{IPI}) \textbf{allow a CPU to send interrupt signals to any other CPU in the system}. An interprocessor interrupt is delivered directly as a message on the bus that connects the \textbf{local APIC} (\textit{Advanced Programmable Interrupt Controller}) of all CPUs. \textbf{An IPI is a synchronous event from the sender CPU-core point of view while it is an asynchronous one from recipient CPU-core point of view.}
Classically, at least two priority levels are admitted:
\begin{description}
\item[High] Leads to immediate processing of the IPI at the recipient (\textit{a single IPI is accepted and stands out at any point in time})
\item[Low] Low priority generally leads to queue the requests and process them via sequentialization
\end{description}

Each local APIC of each CPU of the system has several 32-bit registers (which can be used for posting IPI requests in the system), an internal clock; a local timer device; and two additional IRQ lines, \texttt{LINT0} and \texttt{LINT1}, reserved for local APIC interrupts. All local APICs are connected to an external I/O APIC, giving rise to a multi-APIC system.

The I/O APIC consists of a set of 24 IRQ lines, a 24-entry Interrupt Redirection Table, programmable registers, and a message unit for sending and receiving APIC messages over the APIC bus.

The multi-APIC system allows CPUs to generate interprocessor interrupts. When a CPU wishes to send an interrupt to another CPU, it stores the interrupt vector and the identifier of the target's local APIC in the \textbf{Interrupt Command Register} (\textbf{ICR}) of its own local APIC. A message is then sent via the APIC bus to the target's local APIC, which therefore issues a corresponding interrupt to its own CPU. 

Interrupt requests coming from external hardware devices can be distributed among the available CPUs in two ways:
\begin{description}
\item[Static distribution] The interrupt is delivered to one specific CPU, to a subset of CPUs, or to all CPUs at once (broadcast mode).
\item[Dynamic distribution] The IRQ signal is delivered to the local APIC of the processor that is executing the process with the lowest priority. However interrupts are distributed in a round-robin fashion among CPUs with the same task priority, adopting a technique called \textit{arbitration}.
\end{description}



Linux makes use of three kinds of interprocessor interrupts:

\begin{description}
\item[\texttt{CALL\_FUNCTION\_VECTOR}] It is used to force all CPUs to run a function passed by the sender. The corresponding interrupt handler is named \texttt{call\_function\_interrupt()}. 
\item[\texttt{INVALIDATE\_TLB\_VECTOR}] It is used to force all CPUs to invalidate their TLB. The corresponding interrupt handler is named \texttt{invalidate\_interrupt()}. 
\item[\texttt{RESCHEDULE\_VECTOR}] When a CPU receives this type of interrupt, the corresponding handler, named \texttt{reschedule\_interrupt()}, limits itself to acknowledging the interrupt. Rescheduling is done automatically when returning from the interrupt.
\end{description}

Thanks to the following group of functions, issuing interprocessor interrupts (IPIs) becomes an easy task:

\begin{description}
\item[\texttt{send\_IPI\_all()}] Sends an IPI to all CPUs (including the sender).
\item[\texttt{send\_IPI\_allbutself()}] Sends an IPI to all CPUs except the sender.
\item[\texttt{send\_IPI\_self()}] Sends an IPI to the sender CPU.
\item[\texttt{send\_IPI\_mask()}] Sends an IPI to a group of CPUs specified by a bit mask.
\end{description}


\subsection{IST}

In long mode, each IDT's entry have a very important field (3 bit) called \textbf{IST} (\textit{Interrupt Stack Table}) which is completely absent on i386. \textbf{The IST is used to automatically switch to a new stack for events such trap or interrupts.} This mechanism unconditionally switches stacks when it is enabled and it can be enabled on an individual interrupt-vector basis using IST field in the IDT entry. This means that some interrupt vectors can use the legacy mechanism for stack-switch and others can use the IST mechanism.

\textbf{There can be up to 7 IST entries per CPU and they are located on the Task State Segment (TSS)}. The IST entries in the TSS point to dedicated stacks; each stack can be a different size. \textbf{When an interrupt occurs and the hardware loads such a descriptor, the hardware automatically sets the new stack pointer based on the IST value, then invokes the interrupt handler}.  

There are many stack provided by IST mechanism, one of them is the \texttt{DOUBLEFAULT\_STACK}, which is used for the \textbf{Double Fault Exception} (\#\texttt{DF}), invoked when handling one exception causes another exception. Using a separate stack allows the kernel to recover from it well enough in many cases.

\subsection{Exception handling}

Exception handlers have a standard structure consisting of three steps:
\begin{enumerate}
\item Save the contents of most registers (that is a CPU snapshot) in the Kernel Mode stack (this part is coded in assembly language);
\item Handle the exception by means of a high-level C function.
\item Exit from the handler by means of the \texttt{ret\_from\_exception()} function.
\end{enumerate}

When a CPU receives an interrupt, it starts executing the code at the address found in the corresponding entry of the IDT. Registers is the first task of the interrupt handler and a pointer to the \texttt{pt\_regs struct} (\texttt{include/asm-i386/ptrace.h}) is used to save register.

After the interrupt or exception is processed, \texttt{iret} assembly instruction is used to return program control from an exception or interrupt handler to a program or procedure that was interrupted by an exception. In Protected Mode, the action of the \texttt{iret} instruction depends on the settings of the \texttt{NT} (nested task) flag:
If the NT flag (EFLAGS register) is cleared, the \texttt{iret} instruction performs a far return from the interrupt procedure, without a task switch. Otherwise \texttt{iret} instruction performs a task switch (return) from a nested task

\newpage
\section{Slide '\textit{virtual-file-system}'}

The \textbf{Virtual File-system} (also known as \textbf{Virtual File-system Switch} or \textbf{VFS}) is a kernel \textbf{software layer that handles all system calls related to a standard Unix file-system}. 

Linux VFS is capable to manage several kind of file-systems. Any file-system object, like file or directories, \textbf{is represented in RAM via specific data structures which are managed by any overlying kernel layer in a File System independent manner} (\textit{FS independent part}). However, \textbf{each specific file-system implementation must translate its physical organization into the VFS’s common file model}: it is possible \textbf{using a pointer for each operation}; the pointer is made to point to the proper function for the particular file-system being accessed (\textit{FS dependent part}).

\textit{You can think of the common file model as object-oriented, where an object is a software construct that defines both a data structure and the methods that operate on it} (although Linux is not written using any object-oriented language.)

\subsection{Data Structure}

To be able to manage a file system type we need several suitable data structure, which includes both the \textbf{object attributes} and a pointer to a table of \textbf{object methods}.

VFS's data structure are:

\begin{center}
\begin{tabular}{l|p{13cm}} 
\toprule
 
\textbf{Superblock Object} & Stores information concerning a mounted file-system. For disk-based file-systems, \textbf{this object usually corresponds to a file-system control block stored on disk}.\\\\

\textbf{Inode Object} & Stores general all functions and informations about a specific file. For disk-based file-systems, this
object usually corresponds to a \textit{file control block} stored on disk. Each inode object is associated with an inode number, which uniquely identifies the file within the file-system. 

\textit{A filename is a casually assigned label that can be changed, but the inode is unique to the file and remains the same as long as the file exists.} 

\\\\

\textbf{File Object} & Stores information about the interaction between an open file and a process.
This information exists only in kernel memory during the period when a process has the file open.

\\\\

\textbf{Dentry Object} & Stores information about the linking of a directory entry (that is, a particular
name of the file) with the corresponding file.

VFS considers each directory a file that contains a list of files and other directories. Once a directory entry is read into memory, it is transformed by the VFS into a dentry object based on the dentry structure.

\textbf{The kernel creates a dentry object for every component of a pathname that a process looks up}.
\\\\


\bottomrule
\end{tabular}
\end{center}

\subsubsection{Superblock}

All superblock objects \textbf{are stored in a circular doubly linked list} where the first element of this list is represented by the \texttt{super\_blocks} variable. A super-block is represented by a \texttt{super\_block} struct whose fields are described in following table:

\begin{center}
\begin{tabular}{l|l|p{13cm}} 

\toprule
Type & Field & Description \\
\midrule

\texttt{struct file\_system\_type*} & \texttt{s\_type} & File-system type 

\\

\texttt{struct super\_operations*} & \texttt{s\_op} & The methods associated with a superblock are called \textit{superblock operations} and they are described by the \texttt{super\_operations struct}. Each specific file-system can define its own superblock operation and the \texttt{super\_operations} table is used to contain the address of them. They are invoked by VFS when it execute higher-level operations like deleting files or mounting disks like \texttt{read\_inode}, \texttt{write\_inode} etc.  

Generally all superblock operation implementations are provided by all possible filesystem types; however, the fields corresponding to unimplemented methods are set to NULL.

\\

\texttt{struct dentry*} & \texttt{s\_root} & Dentry object of the file-system's root directory 

\\

\texttt{struct list\_head} & \texttt{s\_dirty} & List of modified inodes 

\\
 
\bottomrule
\end{tabular}
\end{center}


\subsubsection{dentry}

Usually dentry objects have no corresponding image on disk, and hence no field is included in the dentry structure to specify that the object has been modified. Dentry objects are stored in a slab allocator cache whose descriptor is \texttt{dentry\_cache}; 

\begin{center}
\begin{tabular}{l|l|p{13cm}} 

\toprule
Type & Field & Description \\
\midrule

\texttt{struct inode *} & \texttt{d\_inode} & Inode associated with filename. 

\\


\texttt{struct dentry *} & \texttt{d\_parent} & Dentry object of parent directory. 

\\
\texttt{struct qstr} & \texttt{d\_name} & Filename. 

\\
\texttt{struct list\_head} & \texttt{d\_child} & For directories, pointers for the list of directory dentries in the same parent directory.

\\
\texttt{struct dentry\_operations*} & \texttt{d\_op} & The methods associated with a dentry object are called \textit{dentry operations}. Although some file-systems define their own dentry methods, the fields are usually NULL and the VFS replaces them with default functions (like \texttt{d\_delete(dentry)}, \texttt{d\_compare(dir, name1, name2)}). 

\\

\bottomrule
\end{tabular}
\end{center}


\subsubsection{Inode}

An inode object is represented by the \texttt{inode struct}:

\begin{center}
\begin{tabular}{l|l|p{13cm}} 

\toprule
Type & Field & Description \\
\midrule
\texttt{struct list\_head} & \texttt{i\_dentry} & The head of the list of dentry objects referencing this inode 

\\
\texttt{struct inode\_operations*} & \texttt{i\_op} & The methods associated with an inode object are also called \textit{inode operations} and they are stored by an \texttt{inode\_operations} structure, which is populated by all possible inodes and filesystem types. Available functions are \texttt{create}, \texttt{mkdir}, \texttt{symlink} etc.



\\
\texttt{struct file\_operations*} & \texttt{i\_fop} & Default file operations 

\\
\texttt{struct super\_block*} & \texttt{i\_sb} & Pointer to superblock object 

\\
 
\bottomrule
\end{tabular}
\end{center}



\subsection{File-systems}

In Linux, \textit{the code for a file-system actually may either be included in the kernel image or dynamically loaded as a module}. To handle all file-systems, VFS represents them as a \texttt{struct file\_system\_type} object, which is used to hold both \textbf{all file-system specific informations} and a \textbf{pointer to a function to be executed upon mounting the file system in order to read the super-block}.

\begin{center}
\begin{tabular}{l|l|p{13cm}} 

\toprule
Type & Field & Description \\
\midrule

\texttt{const char *} & \texttt{name} & File-system name\\\\

\texttt{struct super\_block * (*)( )} & \texttt{get\_sb} & Method for reading a super-block. (Be aware that in newer kernel versions this field is called \texttt{mount}) \textbf{This is a file-system-type-dependent function that allocates a new super-block object and initializes it} (if necessary, by reading a disk when we manage a disk-based device).

\textbf{Generally this function relies on a block-device driver of a device to instantiate the corresponding file system super-block in memory}. But be aware that if we use a RAM file systems, there is no need to read 
from a device, therefore this function is limited to in-memory instantiation of a fresh superblock.\\\\

\bottomrule
\end{tabular}
\end{center}

All file-system-type objects are inserted into a singly linked list according to which the \texttt{file\_systems} variable points to the first item. A spin lock called \texttt{file\_systems\_lock} protects the whole list against concurrent accesses.

During system initialization, the \texttt{register\_filesystem()} function is invoked for every filesystem specified at compile time, inserting corresponding \texttt{file\_system\_type} object into the aforementioned list.

The \texttt{register\_filesystem()} function is also invoked when a module implementing a file-system is loaded. In this case, the file-system may also be unregistered (by invoking the \texttt{unregister\_filesystem()} function) when the module is unloaded.

\subsubsection{Filesystem Mounting}

The directory on which a file-system is mounted is called the \textbf{mount point}. (\textit{remember that the root directory of a mounted file-system hides the content of the mount point directory of the parent file-system, as well as the whole sub-tree of the parent file-system below the mount point}.)

Is possible to mount the same file-system several times and, if it is mounted $n$ times, its root directory can be accessed through $n$ mount points, one per mount operation. \textbf{Although the same file-system can be accessed by using different mount points, there is only one super-block object for all of them, no matter of how many times it has been mounted.}

For each mount operation, the kernel must save in memory several information, like the mount point, the mount flags or the relationships between the file-system to be mounted and the other mounted file-systems. \textbf{Such informations are stored in a mounted file-system descriptor of type \texttt{struct vfsmount}}.

The \texttt{struct vfsmount} data structure, which from Linux kernel 4.8 is flagged with the \texttt{\_\_randomize\_layout} annotation, has several fields including:

\begin{center}
\begin{tabular}{l|l|p{13cm}} 

\toprule
Type & Field & Description \\
\midrule

\texttt{struct vfsmount *} & \texttt{mnt\_parent} & Points to the parent file-system on which this file-system is mounted.\\
\texttt{char *} & \texttt{mnt\_devname} & Device file-name.\\
\texttt{struct list\_head} & \texttt{mnt\_mounts} & Head of a list including all file-system descriptors mounted on directories of this filesystem. \\
\texttt{struct list\_head} & \texttt{mnt\_child} & Pointers for the \texttt{mnt\_mounts} list of mounted file-system descriptors. \\
\texttt{struct namespace *} & \texttt{mnt\_namespace} & Pointer to the name-space of the process that mounted the file-system. \\
\texttt{struct super\_block *} & \texttt{mnt\_sb} & Points to the super-block object of this file-system.\\
\texttt{int} & \texttt{mnt\_flags} & Flags that specify how some kinds of files in the mounted file-system are handled. For instance \texttt{MNT\_NOEXEC} flag is used to disallow program execution in the mounted file-system.\\

\bottomrule
\end{tabular}
\end{center}

\subsubsection{\texttt{rootfs}}

Linux makes use of a system's \textbf{root file-system} (\texttt{rootfs}) \textit{which is directly mounted by the kernel during the booting phase and that holds the system initialization scripts and the most essential system programs}. The \texttt{file\_system\_type} struct keeps meta-data for \texttt{rootfs} is allocated and initialized statically thorough \texttt{init\_rootfs()} function within \texttt{fs/ramfs/inode.c}.

\subsubsection{The VFS startup and \texttt{rootfs} mounting}

VFS is initialized by Linux through \texttt{vfs\_caches\_init()} function which is a crucial to mount \texttt{rootfs} and all other required file-systems. (\textit{However, in principles, the Linux kernel could be configured such in a way to support no file-system. In this case, any task to be executed needs to be coded within the kernel}).

Mounting process is performed by the following functions (invoked by \texttt{vfs\_caches\_init()}):

\begin{description}
\item[\texttt{init\_rootfs()}] function registers the special file-system type \textit{rootfs} by setting a \texttt{struct file\_system\_type} variable called \texttt{rootfs\_fs\_type}.
\item[\texttt{init\_mount\_tree()}] This function executes the following operations:

\begin{enumerate}

\item The \texttt{do\_kern\_mount()} function is invoked passing to it the string "rootfs" as filesystem type; \textbf{this function checks the file-system type flags to determine how the mount operation is to be done}. \texttt{do\_kern\_mount()} (\textbf{it is used to mount generic file-systems}) performs several operations like:
\begin{itemize}
\item execute the file-system-dependent function to allocate a new super-block and to initialize it (invoking \texttt{get\_sb} function).
\item Initializes the dentry object corresponding to the root directory of the filesystem, and increases the usage counter of the dentry object (\texttt{mnt\_root} field is used)
\item Initializes the name-space field with the name-space of the current process (that is \texttt{mnt->mnt\_namespace} field is set with the value in \texttt{current->namespace}.) (not done when initialize \texttt{rootfs})
\end{itemize}

Finally, \texttt{do\_kern\_mount()} function returns a the address of the mounted filesystem descriptor, 

\item Allocates and initialize a \texttt{namespace} object for the namespace of process 0, and inserts into it the mounted file-system descriptor previously allocated. During system initialization, the name-space field of every other process in the system is set to the address of the namespace object of process 0 (\textbf{By default, all processes share the same, initial namespace}) (it is also initialized the \texttt{namespace->count} usage counter).

\item Sets the root directory and the current working directory of process $0$ (the idle process) to the root filesystem using \texttt{set\_fs\_pwd()} and \texttt{set\_fs\_root()} function.

\end{enumerate}
\end{description}

\subsubsection{Namespaces}

\textbf{The list of mount points along directory tree is called name-space}. \textit{In Linux, every process might have its own tree of mounted file-systems, that is the so called name-space of the process}.

Usually most processes share the same name-space, which is the tree of mounted file-systems that is rooted at the system's root file-system and that is used by the \texttt{init} process. 

However, a process gets a new name-space if it is created by the \texttt{clone()} system call with the \texttt{CLONE\_NEWNS} flag set. 

If a parent process creates a child process without the \texttt{CLONE\_NEWNS} flag, its names-pace is inherited by its children.

\textbf{When a process mounts or unmounts a file-system, it only modifies its names-pace}. Therefore, the change is visible to all processes that share the same name-space, and only to them (except if the mount operation is tagged with SHARED).


\subsubsection{Structure randomization}

\textbf{Fields in a C structure are laid out by the compiler in order of their declaration}. One technique for attacking the kernel is to \textbf{overwrite specific fields of a structure with malicious values} and, when the order of fields in a structure is known, it is trivial to calculate the offsets where sensitive fields reside. 

A useful type of field for such exploitation is the function pointer, which an attacker can overwrite with a location containing malicious code that the kernel can be tricked into executing. 

\textbf{The \texttt{randstruct} plug-in randomly rearranges fields at compile time given a randomization seed}. When potential attackers do not know the layout of a structure, it becomes much harder for them to overwrite specific fields in those structures providing extra protection to the kernel. 

To get structure randomization working in the kernel, the structures marked for randomization need to be tagged with the \texttt{\_\_randomize\_layout} annotation. Usually, structures that only contain function pointers are a big target for attackers and reordering them is unlikely to cause problems elsewhere; therefore \textbf{data structures consisting entirely of function pointers are automatically randomized}. This behaviour can be overridden with the \texttt{\_\_no\_randomize\_layout} annotation. 

Naturally, \textbf{compiler support is necessary to get this feature to work}. Since Linux kernel 4.8, this plug-in is been used.


\subsection{Relationship with processes}

To represent the interactions between a process and a file-system, Linux use the \texttt{fs\_struct} data structure (defined in \texttt{include/fs\_struct.h}), and \textbf{each process descriptor has an \texttt{fs} field} that points to the process \texttt{fs\_struct} structure. 

Some \texttt{fs\_struct} fields are described below:

\begin{center}
\begin{tabular}{l|l|p{13cm}} 

\toprule
Type & Field & Description \\
\midrule

\texttt{atomic\_t} & \texttt{count} & Number of processes sharing this table

\\

\texttt{struct dentry *} & \texttt{root} & Dentry of the root directory

\\

\texttt{struct dentry *} & \texttt{pwd} & Dentry of the current working directory

\\

\texttt{struct vfsmount *} & \texttt{rootmnt} & Mounted filesystem object of the root directory

\\

\texttt{struct vfsmount *} & \texttt{pwdmnt} & Mounted filesystem object of the current working directory

\\
 
\bottomrule
\end{tabular}
\end{center}

In Linux 3.xx/4.7, the \texttt{fs\_struct} data structure is slightly different: the \texttt{root} and \texttt{pwd} fields are now of \texttt{struct path} type. From kernel 4.8, this data structure is flagged as \texttt{\_\_randomize\_layout}.

The \texttt{files\_struct} data structure represents the \textbf{file descriptor table}, whose address is contained in the \texttt{files} field of the process descriptor, used to specify which files are currently opened by the process (\textit{is also said that this table builds a relation between an I/O channel (a numerical ID code, used as a key) and an I/O object}). Some fields are:

\begin{center}
\begin{tabular}{l|l|p{13cm}} 

\toprule
Type & Field & Description \\
\midrule

\texttt{atomic\_t} & \texttt{count} & Number of processes sharing this table

\\

\texttt{struct file **} & \texttt{fd} & Pointer to array of \texttt{struct file} object pointers. The size of the array is stored in the \texttt{max\_fds} field. 

For every file with an entry in the \texttt{fd} array, the array index is the \textit{file descriptor}. Usually, the first element (index 0) of the array is associated with the \textbf{standard input} of the process, the second with the \textbf{standard output}, and the third with the \textbf{standard error}.

A process cannot use more than \texttt{NR\_OPEN} (usually $1048576$) file descriptors.

\\

\texttt{fd\_set *} & \texttt{open\_fds} & Pointer to open file descriptors.

\\
 
\bottomrule
\end{tabular}
\end{center}

We said that the file descriptor table contains a pointer to an array of \texttt{struct file} objects.
A \texttt{file} object (\textit{the session data of the file descriptor table}) describes how a process interacts with a file it has opened and it is created when the file is opened. \textit{Notice that file objects have no corresponding image on disk}. (in newer version of kernel, \texttt{file} data structure is flagged with \texttt{\_\_randomize\_layout} annotation) \texttt{file} fields are:

\begin{center}
\begin{tabular}{l|l|p{13cm}} 

\toprule
Type & Field & Description \\
\midrule


\texttt{struct dentry *} & \texttt{f\_dentry} & dentry object associated with the file.

\\
\texttt{struct file\_operations *} & \texttt{f\_op} & Pointer to file operation table.

\\
\texttt{atomic\_t} & \texttt{f\_count} & File object’s reference counter which counts the number of
processes that are using the file object. 

\\
\texttt{mode\_t} & \texttt{f\_mode} & Process access mode.

\\
\texttt{loff\_t} & \texttt{f\_pos} & Current file offset (file pointer).

\\ 
\bottomrule
\end{tabular}
\end{center}

\subsection{Device Files}

Unix-like operating systems are based on the notion of a file. This is true also for I/O devices which are treated as special files called \textbf{device files}; \textbf{thus, the same system calls used to interact with regular files on disk can be used to directly interact with I/O devices}. 

Device files can be of two types: 

\begin{description}
\item[Block Device] In this devices type, the data can be addressed randomly, and the time needed to transfer a data block is small and roughly the same.
\item[Char Device] The data of a character device either cannot be addressed randomly, or they can be addressed randomly, but the time required to access a random datum largely depends on its position inside the device.
\end{description} 

A device file is usually a real file which inode \textbf{includes an identifier of the hardware device corresponding to the character or block device file}. This identifier consists of a pair of numbers:

\begin{description}

\item[Major number] It is used to identify the device type. Traditionally, all device files that have the same major number and the same type share the same set of file operations, because they are handled by the same device driver (\textit{the operations for managing the device are retrieved via the device driver tables})
\item[Minor number] It is used to identify a specific device among a group of devices that share the same major number. For instance, a group of disks managed by the same disk controller have the same major number and different minor numbers.
\end{description}

In x86 machines, device numbers are represented as bit masks stored into \texttt{kdev\_t i\_rdev} field of \texttt{struct inode}. In particular, major number corresponds to the least significant byte within the mask, while the minor correspond to second least significant byte. The macro \texttt{MKDEV(ma,mi)} (defined in \texttt{include/linux/kdev\_t.h}), can be used to setup a correct bit mask by starting from the two numbers.

The \texttt{mknod()} system call is used to create device files. It receives the name of the device file, its type (\texttt{S\_IFREG}, \texttt{S\_IFCHR}, \texttt{S\_IFBLK}, \texttt{S\_IFIFO}), and the major and minor numbers as its parameters. Device files are usually included in the \texttt{/dev} directory.

Usually, a device file is associated with a hardware device (such as a hard disk) or with some physical or logical portion of a hardware device.

\subsubsection{Device Drivers}

\textbf{A device driver is the set of kernel routines that makes a hardware device respond to the programming interface defined by the canonical set of VFS functions that control a device} (that is \texttt{open}, \texttt{write}, \texttt{read} and so on...) 

\textit{The actual implementation of all these functions is delegated to the device driver. Because each device has a different I/O controller, and thus different commands and different state information, most I/O devices have their own drivers}.

Each system call issued on a device file is translated by the kernel into an invocation of a suitable function of a corresponding device driver. \textbf{To achieve this, a device driver must register itself}. In other words, registering a device driver means allocating a new \texttt{device\_driver} descriptor.

A \textbf{character device driver} is described by a \texttt{cdev} structure, whose main fields are:

\begin{center}
\begin{tabular}{l|l|p{13cm}} 

\toprule
Type & Field & Description \\
\midrule


\texttt{dev\_t} & \texttt{dev} & Initial major and minor numbers assigned to the device driver.

\\ 
\texttt{struct file\_operations *} & \texttt{ops} & Pointer to the file operations table of the device driver.

\\
\texttt{struct list\_head} & \texttt{list} & Head of the list of inodes relative to device files for this character device

\\
\bottomrule
\end{tabular}
\end{center}

To keep track of which character device numbers are currently assigned, the kernel uses a hash table \texttt{chrdevs}, which contains intervals of device numbers. \textbf{Two intervals may share the same major number, but they cannot overlap, thus their minor numbers should be all different}.

The table includes up to 255 entries every of which points to the first element of a collision list ordered by increasing major and minor numbers.

For assigning a range of device numbers to a character device driver, is possible to use both the \texttt{register\_chrdev\_region()} function, which assigns an arbitrary range of device numbers (\textit{it receives as its parameters the requested major number major, the name of the device driver name, and a pointer to a table of file operations specific to the character device files in the interval}), and the \texttt{register\_chrdev()} function which assigns a fixed interval of device numbers including a single major number and minor numbers from 0 to 255 (\textit{it receives as its parameters the requested major number major, the name of the device driver name, and a pointer to a table of file operations specific to the character device files in the interval.})






\subsection{\texttt{procfs}}

The \texttt{proc} file-system is a \textbf{pseudo-file-system} provided by the Linux kernel which is \textbf{designed to allow User Mode applications to access kernel internal data structures}, exporting information about various kernel subsystems, hardware devices, and associated device drivers \textbf{via common I/O operations on virtual files}. It is automatically mounted at \texttt{/proc}. 

\texttt{/proc} includes non-process-related system informations like:

\begin{description}
\item[\texttt{cpuinfo}] contains information about the processor at boot time.
\item[\texttt{meminfo}] contains information about the memory usage.
\item[\texttt{version}] contains the kernel version information.
\item[\texttt{kcore}] contains the entire RAM contents as seen by the kernel
\end{description}

\subsection{\texttt{sysfs}}

sysfs is a \textbf{pseudo file-system} similar to \texttt{proc}. \texttt{sysfs} is compiled into the kernel by default depending on the configuration option \texttt{CONFIG\_SYSFS} (visible only if \texttt{CONFIG\_EMBEDDED} is set).

A goal of the sysfs filesystem is to expose the hierarchical relationships among the components of the device driver model. 
\begin{itemize}
\item Relationships between components of the device driver models are expressed in the sysfs filesystem as symbolic links between directories and files.
\item The main role of regular files in the sysfs filesystem is to represent attributes of driv-
ers and devices. 
\item The related top-level directories of this filesystem are used to represent several kernel object like block, device, bus and so on.

\end{itemize}

\section{Trivia}

\begin{itemize}
\item \texttt{\_\_init}, \texttt{\_\_initdata}, \texttt{\_\_initconst}	are used to mark some functions or initialized data	(doesn't apply to uninitialized data) as `\textit{initialization}' functions. \textbf{The kernel can take this as hint that the function is used only during the initialization phase and free up used memory resources after.} They are defined into \texttt{include/linux/init.h}.

\item The \texttt{EFLAGS} register is the status register in Intel x86 microprocessors that contains the current state of the processor. This register is 32 bits wide. One of the most important bit 9 which is \texttt{IF} (\textit{Interrupt enable flag}) 

\item The \texttt{EIP} register contains the address of the next instruction to be executed if no branching is done. 
\end{itemize}

\end{document}